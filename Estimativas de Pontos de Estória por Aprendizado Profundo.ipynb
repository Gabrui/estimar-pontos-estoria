{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimativa de pontos de estória usando aprendizado profundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilização de aprendizado profundo por redes neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "#tokenizar = spacy.tokenizer.Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão do conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treino, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_csv_texto_tokenizado(caminho_arquivo, tokenizador):\n",
    "    tabela_dados = pd.read_csv(caminho_arquivo)\n",
    "    titulos = tabela_dados.values[:, 1].astype('str')\n",
    "    separadores = np.full(len(titulos), '. ')\n",
    "    descricao = tabela_dados.values[:, 2].astype('str')\n",
    "    juntos = [a + b + c for a, b, c in zip(titulos, separadores, descricao)]\n",
    "    tokenizados = [[w.text.strip().lower() for w in tokenizador(frase) if not w.text.isspace()] for frase in juntos]\n",
    "    return tokenizados\n",
    "\n",
    "def gerar_dicionario(lista_tokens, vocabulario_maximo=100000):\n",
    "    contagem = defaultdict(int)\n",
    "    for tokens in lista_tokens:\n",
    "        for token in tokens:\n",
    "            contagem[token] += 1\n",
    "    ordenadas = sorted(contagem.items(), key=lambda x: x[1], reverse=True)\n",
    "    IDs = {}\n",
    "    for i in range(min(vocabulario_maximo, len(ordenadas))): # EOF = 0, ID_desconhecido = 1\n",
    "        IDs[token] = i + 2\n",
    "    return IDs\n",
    "\n",
    "def converte_tokens(lista_tokens, dic, fracao_valida = 0.1):\n",
    "    lista_ids = [[(dic.get(tk) or 0) for tk in tks] for tks in lista_tokens]\n",
    "    quant_valida = int(fracao_valida * len(lista_ids))\n",
    "    return lista_ids[:-quant_valida], lista_ids[-quant_valida:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ler_csv_texto_tokenizado('dados/pretreino/moodle.csv', nlp.tokenizer)\n",
    "dic = gerar_dicionario(a)\n",
    "treino, validacao = converte_tokens(a, dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretreinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
