{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimativa de pontos de estória usando aprendizado profundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilização de aprendizado profundo por redes neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import keras\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "#tokenizar = spacy.tokenizer.Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão do conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treino, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_csv_texto_tokenizado(caminho_arquivo, tokenizador):\n",
    "    tabela_dados = pd.read_csv(caminho_arquivo)\n",
    "    titulos = tabela_dados.values[:, 1].astype('str')\n",
    "    separadores = np.full(len(titulos), '. ')\n",
    "    descricao = tabela_dados.values[:, 2].astype('str')\n",
    "    juntos = [a + b + c for a, b, c in zip(titulos, separadores, descricao)]\n",
    "    tokenizados = [[w.text.strip().lower() for w in tokenizador(frase) if not w.text.isspace()] for frase in juntos]\n",
    "    return tokenizados\n",
    "\n",
    "def gerar_dicionario(lista_tokens, vocabulario_maximo=100000):\n",
    "    contagem = defaultdict(int)\n",
    "    for tokens in lista_tokens:\n",
    "        for token in tokens:\n",
    "            contagem[token] += 1\n",
    "    ordenadas = sorted(contagem.items(), key=lambda x: x[1], reverse=True)\n",
    "    IDs = {}\n",
    "    for i in range(min(vocabulario_maximo, len(ordenadas))): # EOF = 0, ID_desconhecido = 1\n",
    "        IDs[token] = i + 2\n",
    "    return IDs\n",
    "\n",
    "def converte_tokens(lista_tokens, dic, fracao_valida = 0.1):\n",
    "    lista_ids = [[(dic.get(tk) or 0) for tk in tks] for tks in lista_tokens]\n",
    "    quant_valida = int(fracao_valida * len(lista_ids))\n",
    "    return lista_ids[:-quant_valida], lista_ids[-quant_valida:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ler_csv_texto_tokenizado('dados/pretreino/moodle.csv', nlp.tokenizer)\n",
    "dic = gerar_dicionario(a)\n",
    "treino, validacao = converte_tokens(a, dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretreinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimativaConstrastivaRuidosa(keras.engine.topology.Layer):\n",
    "    def __init__(self, init='glorot_uniform', comprimento=10,\n",
    "                 dimensao_entrada=None, vocabulario=None, ruidos = 25, distribuicao_ruidos=[0.5, 0.5], **kwargs):\n",
    "        self.init = init\n",
    "        self.comprimento = comprimento\n",
    "        self.vocabulario = vocabulario\n",
    "        self.ruidos = ruidos\n",
    "        self.distribuicao_ruidos = theano.shared(numpy.array(distribuicao_ruidos).astype(config.floatX)) #IMP\n",
    "        self.gerador_aleatorio = RS.RandomStreams(seed=SEED) #IMP\n",
    "        kwargs['input_shape'] = (self.dimensao_entrada, )\n",
    "        super(EstimativaConstrastivaRuidosa, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, dims_entrada):\n",
    "        self.W = self.add_weight(name='{}_W'.format(self.name), shape=(self.vocabulario, self.dimensao_entrada), initializer=self.init, trainable=True)\n",
    "        self.b = self.add_weight(name='{}_b'.format(self.name), shape=(self.vocabulario, )              , initializer=self.init, trainable=True)\n",
    "        super(EstimativaConstrastivaRuidosa, self).build(dims_entrada)\n",
    "\n",
    "    def compute_output_shape(self, dims_entrada):\n",
    "        return (None, self.comprimento, self.ruidos + 1)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        contexto = inputs[0] #shape: amostras * passos * dim\n",
    "        proxima_palavra = inputs[1] #shape: amostras * passos\n",
    "\n",
    "        amostras, passos = proxima_palavra.shape\n",
    "        dim_saida = self.ruidos + 1\n",
    "\n",
    "        noise_w = self.gerador_aleatorio.choice(size=(amostras, passos, self.ruidos), a=self.distribuicao_ruidos.shape[0], p=self.distribuicao_ruidos)\n",
    "        proxima_palavra = proxima_palavra.flatten().reshape([amostras, passos, 1])\n",
    "        proxima_palavra = tensor.concatenate([proxima_palavra, noise_w], axis=-1) #IMP shape: amostras * passos * dim_saida\n",
    "\n",
    "        W_ = self.W[proxima_palavra.flatten()].flatten().reshape([amostras, passos, dim_saida, self.dimensao_entrada])\n",
    "        b_ = self.b[proxima_palavra.flatten()].reshape([amostras, passos, dim_saida])\n",
    "\n",
    "        s_theta = (contexto[:, :, None, :] * W_).sum(axis=-1) + b_ # dims: amostras * passos * dim_saida\n",
    "        noiseP = self.distribuicao_ruidos[proxima_palavra.flatten()].reshape([amostras, passos, dim_saida])\n",
    "        noise_score = K.log(self.ruidos * noiseP) #log(k * distribuicao_ruidos(w))\n",
    "\n",
    "        return activations.sigmoid(s_theta - noise_score) # dims: amostras, passos, dim_saida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_pretreinamento(dicionario):\n",
    "    quant_vocab = len(dicionario)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
