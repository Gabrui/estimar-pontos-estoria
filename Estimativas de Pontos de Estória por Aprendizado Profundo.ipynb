{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimativa de pontos de estória usando aprendizado profundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilização de aprendizado profundo por redes neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "#os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import keras\n",
    "import theano\n",
    "from keras.layers import Input, Embedding, LSTM\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "#nlp = spacy.load('pt', disable=['parser', 'tagger', 'ner'])\n",
    "nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "#tokenizar = spacy.tokenizer.Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão do conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treino, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_csv_texto_tokenizado(caminho_arquivo, tokenizador):\n",
    "    tabela_dados = pd.read_csv(caminho_arquivo)\n",
    "    titulos = tabela_dados.values[:, 1].astype('str')\n",
    "    separadores = np.full(len(titulos), '. ')\n",
    "    descricao = tabela_dados.values[:, 2].astype('str')\n",
    "    valores = tabela_dados.values[:, 3].astype('float32')\n",
    "    juntos = [a + b + c for a, b, c in zip(titulos, separadores, descricao)]\n",
    "    tokenizados = [[w.text.strip().lower() for w in tokenizador(frase) if not w.text.isspace()] for frase in juntos]\n",
    "    return tokenizados, valores\n",
    "\n",
    "def gerar_dicionario(lista_tokens, vocabulario_maximo=500):\n",
    "    contagem = defaultdict(int)\n",
    "    for tokens in lista_tokens:\n",
    "        for token in tokens:\n",
    "            contagem[token] += 1\n",
    "    ordenadas = sorted(contagem.items(), key=lambda x: x[1], reverse=True)\n",
    "    IDs = {'EOF_VALUE': 0, 'UNK_VALUE': 1}\n",
    "    for i in range(min(vocabulario_maximo, len(ordenadas))): # EOF = 0, ID_desconhecido = 1\n",
    "        IDs[ordenadas[i][0]] = i + 2\n",
    "    return IDs\n",
    "\n",
    "def converte_tokens_pretreino(lista_tokens, dic): #, fracao_valida = 0.2):\n",
    "    dados = [np.array([(dic.get(tk) or 1) for tk in tks], dtype=np.int16) for tks in lista_tokens]\n",
    "    #quant_valida = int(fracao_valida * len(lista_tokens))\n",
    "    return dados #[:-quant_valida], dados[-quant_valida:]\n",
    "\n",
    "def converte_tokens_treino(lista_tokens, dic, fracao_valida = 0.2, fracao_teste = 0.2, tamanho_frase=256):\n",
    "    dados = np.zeros((len(lista_tokens), tamanho_frase), dtype='int16')\n",
    "    for i, tks in enumerate(lista_tokens):\n",
    "        compri = min(tamanho_frase, len(tks))\n",
    "        dados[i, :compri] = [(dic.get(tks[i]) or 1) for i in range(compri)]\n",
    "    quant_valida = int(fracao_valida * len(lista_tokens))\n",
    "    quant_teste = int(fracao_teste * len(lista_tokens))\n",
    "    return dados[:-quant_valida-quant_teste], dados[-quant_valida-quant_teste:-quant_teste], dados[-quant_teste:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario:  18268 \n",
      "Treino:  2452\n"
     ]
    }
   ],
   "source": [
    "#a = ler_csv_texto_tokenizado('dados/pretreino/ccasj.csv', nlp.tokenizer)\n",
    "a, valores = ler_csv_texto_tokenizado('dados/pretreino/lsscorp_pretreino.csv', nlp.tokenizer)\n",
    "dic = gerar_dicionario(a)\n",
    "#treino, validacao = converte_tokens_pretreino(a, dic)\n",
    "treino = converte_tokens_pretreino(a, dic)\n",
    "print('Vocabulario: ', len(dic), '\\nTreino: ', len(treino))#, '\\n Validação: ', len(validacao))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  2801 \n",
      "Validation:  933 \n",
      "Test:  933\n"
     ]
    }
   ],
   "source": [
    "train_text, train_points = ler_csv_texto_tokenizado('dados/treino/datamanagement.csv', nlp.tokenizer)\n",
    "x_train, x_val, x_test = converte_tokens_treino(train_text, dic)\n",
    "y_train = train_points[:x_train.shape[0]].reshape((-1, 1))\n",
    "y_val = train_points[x_train.shape[0]:x_train.shape[0]+x_val.shape[0]].reshape((-1, 1))\n",
    "y_test = train_points[x_train.shape[0] + x_val.shape[0]:].reshape((-1, 1))\n",
    "print('Train: ', x_train.shape[0], '\\nValidation: ', x_val.shape[0], '\\nTest: ', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.19507"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.median(y_train) - y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open('dados/treino/datamanagement.pkl.gz', 'rb')\n",
    "\n",
    "train_t, train_d, train_y, \\\n",
    "valid_t, valid_d, valid_y, \\\n",
    "test_t, test_d, test_y = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(935,)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., 18.,  2., 10.,  4.,  1.,  1.,  2.,  4.,  4.,  5.,  2.,  2.,\n",
       "        2.,  6., 10.,  2., 21., 21.,  6.,  1.,  1.,  2.,  3., 20.,  6.,\n",
       "        6., 14.,  4., 10.,  6.,  6.,  5.,  6.,  2.,  4.,  8., 10., 10.,\n",
       "       10.,  8., 12.,  8.,  2.,  6.,  6.,  4., 12., 18.,  4.,  6.,  3.,\n",
       "        4.,  1.,  2.,  1.,  1.,  1.,  1.,  4.,  4.,  4.,  2.,  8., 10.,\n",
       "        5.,  6.,  6., 10.,  5.,  4.,  5.,  6.,  8.,  4.,  1.,  2., 10.,\n",
       "        4.,  3.,  2.,  1., 21.,  1., 21.,  5.,  3.,  3.,  6.,  2.,  2.,\n",
       "        6.,  1.,  1.,  8.,  2., 21., 10., 21., 21., 15.,  2., 21.,  8.,\n",
       "       21., 16., 12., 12., 20., 10., 10., 15.,  4.,  4.,  4.,  2.,  1.,\n",
       "        1.,  1.,  1.,  1.,  3.,  1.,  2.,  2.,  4.,  2.,  6.,  6.,  2.,\n",
       "        4.,  2.,  3.,  3.,  3.,  1.,  3., 10.,  2.,  1.,  1.,  3.,  4.,\n",
       "        5.,  4.,  5.,  5.,  1.,  1.,  2.,  4., 10.,  5.,  2.,  3.,  9.,\n",
       "        6.,  1.,  3.,  7.,  4., 12.,  8., 10.,  7., 13., 12.,  2.,  2.,\n",
       "        4.,  2.,  4.,  2.,  2.,  3.,  2.,  2.,  3.,  2.,  1.,  2.,  2.,\n",
       "        1.,  3.,  2.,  2.,  4.,  8., 21., 21.,  3.,  2.,  1.,  4.,  1.,\n",
       "        7.,  2.,  4.,  1., 21., 21.,  2.,  4.,  2.,  1.,  1.,  4.,  2.,\n",
       "        1.,  1., 10.,  1.,  5.,  6.,  3.,  1.,  1.,  2.,  6.,  4.,  6.,\n",
       "        4.,  1.,  1.,  3.,  5.,  1.,  2.,  6.,  2.,  2.,  1.,  1.,  4.,\n",
       "        2., 10.,  4.,  2.,  2., 10.,  5., 10., 10., 10.,  3.,  5.,  1.,\n",
       "        3.,  5.,  2.,  1., 20.,  2., 15., 10., 21.,  8.,  3.,  1., 21.,\n",
       "        2.,  3.,  3.,  8., 21., 15.,  1.,  4.,  2., 12.,  2.,  1.,  1.,\n",
       "        1.,  1.,  6.,  3.,  2.,  4.,  4.,  1.,  1.,  2.,  8.,  1.,  1.,\n",
       "        4.,  2.,  3.,  1.,  2.,  2.,  1.,  1.,  2.,  2.,  1., 15.,  6.,\n",
       "       10.,  6.,  2.,  4., 14.,  3.,  2.,  1.,  4.,  2., 10.,  1.,  5.,\n",
       "        1.,  3.,  2.,  1.,  4., 15.,  1.,  4.,  2., 21.,  2.,  2.,  8.,\n",
       "        3.,  3.,  5.,  3.,  4.,  2.,  6.,  8.,  1.,  8.,  2., 14.,  2.,\n",
       "        1.,  1., 21.,  5., 10.,  5.,  1.,  4.,  1.,  4.,  1.,  6.,  3.,\n",
       "       10.,  1.,  1.,  4.,  6.,  2., 21.,  3., 10.,  5.,  8., 10.,  5.,\n",
       "        6., 19.,  5.,  6.,  1.,  2., 21., 21.,  4.,  9.,  4., 10., 12.,\n",
       "        1.,  2.,  2.,  2.,  2.,  3.,  2.,  1.,  3.,  1., 15., 15.,  1.,\n",
       "        1.,  1.,  4.,  4.,  4.,  2.,  3.,  3.,  1.,  2.,  2.,  4.,  4.,\n",
       "        1.,  1.,  5.,  6.,  6.,  4.,  2.,  2.,  2., 21., 21., 21., 21.,\n",
       "       21., 21., 21., 21., 21., 21.,  5.,  1.,  1.,  4.,  1., 21.,  2.,\n",
       "        2.,  1.,  1.,  1., 10.,  2.,  1.,  4.,  1.,  1.,  1.,  6.,  6.,\n",
       "        2., 10.,  3.,  4.,  1.,  2.,  1.,  8., 16., 10.,  1.,  1.,  1.,\n",
       "        4.,  4.,  2.,  1.,  6.,  3.,  8., 21.,  4.,  1.,  1.,  1.,  1.,\n",
       "        8.,  6.,  6.,  3., 10.,  6.,  8.,  8., 16.,  4., 12.,  1.,  8.,\n",
       "        8.,  1., 15.,  1.,  4.,  1., 10.,  1.,  2., 10.,  4.,  4.,  5.,\n",
       "        1., 20.,  2.,  2., 10.,  2.,  1.,  2., 21.,  1.,  1., 10.,  3.,\n",
       "        1.,  1.,  4.,  2.,  2.,  3.,  6.,  1.,  4.,  1.,  4.,  2.,  3.,\n",
       "        1.,  1.,  1.,  3.,  5.,  2.,  1., 12.,  8.,  8.,  8.,  8., 21.,\n",
       "       10.,  5., 10.,  3.,  4.,  5., 10.,  4., 21.,  4.,  6.,  4.,  1.,\n",
       "        2.,  1.,  1.,  4., 21.,  4., 10.,  2.,  4.,  4., 10.,  6.,  1.,\n",
       "        2.,  5.,  6.,  2.,  4.,  6.,  3.,  6.,  1., 21.,  3.,  2.,  5.,\n",
       "        2.,  1.,  1.,  1.,  1.,  8.,  6.,  3.,  1.,  1., 10., 10., 10.,\n",
       "        8.,  8.,  1.,  1.,  1.,  4.,  1.,  4.,  6., 12.,  4.,  3.,  1.,\n",
       "        8., 10.,  4.,  2.,  1.,  2., 21., 20., 20., 21., 21.,  2., 21.,\n",
       "        3.,  7., 16.,  2.,  1.,  4.,  8.,  1.,  6.,  6.,  2., 21.,  8.,\n",
       "       21.,  1., 21., 21., 21.,  2.,  2., 21.,  2.,  2., 10.,  5., 10.,\n",
       "       10.,  3.,  2.,  2.,  3., 21., 10., 20., 21.,  2.,  8.,  3., 21.,\n",
       "       21.,  6., 12., 21.,  4.,  4.,  6.,  1., 10., 21.,  3., 10.,  3.,\n",
       "        1.,  1.,  4.,  4.,  4., 20.,  5., 20., 10.,  5.,  5.,  1.,  2.,\n",
       "        2.,  2.,  8.,  1.,  3.,  3.,  3.,  3.,  2.,  2.,  2., 21.,  2.,\n",
       "        2., 21.,  5., 10., 21.,  6.,  3., 21.,  7., 21.,  1.,  3.,  9.,\n",
       "        1.,  3.,  3., 11., 21., 10., 15.,  6.,  1., 21.,  4., 10.,  6.,\n",
       "        6.,  8.,  8.,  6.,  6.,  6.,  6.,  1.,  6.,  6.,  7.,  2.,  2.,\n",
       "       10.,  6.,  6.,  2.,  8.,  2., 21., 21.,  6.,  2., 10.,  4.,  1.,\n",
       "        2.,  1.,  3.,  2.,  2.,  4.,  2.,  6.,  2.,  4.,  3.,  4.,  5.,\n",
       "        3.,  4.,  1., 21.,  1.,  5.,  1.,  1.,  4.,  1., 12.,  3., 21.,\n",
       "        7., 21., 21., 21.,  8., 20., 21., 21., 21., 21., 21., 20., 21.,\n",
       "       21., 20., 12.,  3.,  1.,  2.,  1.,  1.,  4.,  2.,  2.,  9.,  8.,\n",
       "        8.,  4.,  1.,  2.,  2.,  8.,  2.,  3.,  5.,  2.,  1.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2., 10.,  1., 10., 10.,  2.,  8.,  2.,\n",
       "        6.,  2., 19.,  4.,  6.,  2.,  8.,  2.,  6.,  4.,  4.,  4.,  4.,\n",
       "        8.,  8.,  6.,  6.,  4., 10., 12.,  2.,  4.,  2.,  2.,  2.,  5.,\n",
       "        1.,  1.,  2.,  1.,  3.,  3., 21.,  3.,  6.,  4.,  6.,  8.,  1.,\n",
       "        5.,  4.,  2.,  1.,  4.,  2., 10.,  6.,  3.,  5.,  1.,  6.,  1.,\n",
       "        1.,  2.,  2.,  2.,  2.,  1.,  1.,  1., 10.,  1.,  8.,  1.,  1.,\n",
       "        8.,  6.,  8.,  1.,  2.,  4.,  8.,  4.,  1.,  2.,  1.,  1.,  4.,\n",
       "        1.,  1.,  2.,  6.,  3.,  2.,  1.,  2., 15.,  2.,  3.,  6.,  2.,\n",
       "        4., 10.,  2.,  4.,  2.,  1.,  1.,  1., 20.,  2.,  2.,  2.,  2.,\n",
       "        1.,  6., 21.,  2., 10.,  1., 10., 10.], dtype=float32)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70.],\n",
       "       [ 38.],\n",
       "       [100.],\n",
       "       [ 60.],\n",
       "       [ 40.],\n",
       "       [ 60.],\n",
       "       [ 25.],\n",
       "       [ 80.],\n",
       "       [ 25.],\n",
       "       [ 38.],\n",
       "       [ 24.],\n",
       "       [ 29.],\n",
       "       [ 50.],\n",
       "       [ 24.],\n",
       "       [ 30.],\n",
       "       [ 50.],\n",
       "       [ 24.],\n",
       "       [ 33.],\n",
       "       [ 73.],\n",
       "       [ 30.],\n",
       "       [ 30.],\n",
       "       [ 65.],\n",
       "       [ 65.],\n",
       "       [ 26.],\n",
       "       [ 65.],\n",
       "       [ 65.],\n",
       "       [100.],\n",
       "       [ 26.],\n",
       "       [ 26.],\n",
       "       [ 26.],\n",
       "       [100.],\n",
       "       [ 60.],\n",
       "       [ 45.],\n",
       "       [ 49.],\n",
       "       [ 64.],\n",
       "       [ 28.],\n",
       "       [100.],\n",
       "       [ 50.],\n",
       "       [ 25.],\n",
       "       [ 25.],\n",
       "       [ 50.],\n",
       "       [ 25.],\n",
       "       [ 32.],\n",
       "       [ 23.],\n",
       "       [ 52.],\n",
       "       [ 43.],\n",
       "       [ 38.],\n",
       "       [ 45.],\n",
       "       [ 30.],\n",
       "       [100.],\n",
       "       [100.],\n",
       "       [ 40.],\n",
       "       [ 54.],\n",
       "       [ 23.],\n",
       "       [ 24.],\n",
       "       [100.],\n",
       "       [ 26.],\n",
       "       [ 22.],\n",
       "       [ 22.],\n",
       "       [ 25.],\n",
       "       [ 40.],\n",
       "       [100.],\n",
       "       [ 27.],\n",
       "       [ 40.],\n",
       "       [ 54.],\n",
       "       [ 68.],\n",
       "       [ 23.],\n",
       "       [100.],\n",
       "       [ 95.],\n",
       "       [ 36.],\n",
       "       [ 42.],\n",
       "       [ 28.],\n",
       "       [ 36.],\n",
       "       [ 40.],\n",
       "       [ 36.],\n",
       "       [ 42.]], dtype=float32)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[:-2][(valid_y[2:] - y_val[:-2].flatten()) != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representação Vetorial do FB FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def carrega_vetor(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "vetores = carrega_vetor('word_embeddings/cc.pt.300.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretreinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeradorDados(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, funcoes, tamanho_batch=128, quant_batch=100, janela=7, fracao_positiva=0.5):\n",
    "        self.funcoes = funcoes\n",
    "        self.tamanho_batch = tamanho_batch\n",
    "        self.quant_batch = quant_batch\n",
    "        self.janela = janela\n",
    "        self.fracao_positiva = fracao_positiva\n",
    "        self.quant_funcoes = len(funcoes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.quant_batch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        alvo = np.zeros((self.tamanho_batch, 1), dtype='int16')\n",
    "        contexto = np.zeros((self.tamanho_batch, 1), dtype='int16')\n",
    "        verdade = np.zeros((self.tamanho_batch, 1))\n",
    "        for i in range(self.tamanho_batch):\n",
    "            positivo = random.random() < self.fracao_positiva\n",
    "            j = random.randint(0, self.quant_funcoes - 1)\n",
    "            k = random.randint(0, self.funcoes[j].shape[0] - 1)\n",
    "            if positivo:\n",
    "                l = j\n",
    "                m = random.randint(max(0, k-self.janela), min(self.funcoes[j].shape[0] - 1, k+self.janela))\n",
    "            else:\n",
    "                l = random.randint(0, self.quant_funcoes - 1)\n",
    "                m = random.randint(0, self.funcoes[l].shape[0] - 1)\n",
    "            alvo[i] = self.funcoes[j][k]\n",
    "            contexto[i] = self.funcoes[l][m]\n",
    "            verdade[i] = positivo\n",
    "        return [alvo, contexto], verdade\n",
    "\n",
    "def gerar_modelo_keras(vocabulario, dimensao_vetorial=50):\n",
    "    entrada_contexto = keras.layers.Input(shape=(1,), dtype='int16', name='palavra_contexto')\n",
    "    entrada_alvo = keras.layers.Input(shape=(1,), dtype='int16', name='palavra_alvo')\n",
    "    vetorizacao = keras.layers.Embedding(vocabulario, dimensao_vetorial, name='vetorizacao')\n",
    "    contexto = keras.layers.Flatten()(vetorizacao(entrada_contexto))\n",
    "    alvo = keras.layers.Flatten()(vetorizacao(entrada_alvo))\n",
    "    produto_interno = keras.layers.dot([alvo, contexto], axes=-1)\n",
    "    estimativa = keras.layers.Dense(1, activation='sigmoid')(produto_interno)\n",
    "    return keras.models.Model(inputs=[entrada_alvo, entrada_contexto], outputs=estimativa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "palavra_alvo (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "palavra_contexto (InputLayer)   (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vetorizacao (Embedding)         (None, 1, 50)        913400      palavra_contexto[0][0]           \n",
      "                                                                 palavra_alvo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 50)           0           vetorizacao[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 50)           0           vetorizacao[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_9 (Dot)                     (None, 1)            0           flatten_6[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dot_9[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 913,402\n",
      "Trainable params: 913,402\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gerador_treino = GeradorDados(treino, 128, 128)\n",
    "gerador_valida = GeradorDados(validacao, 128, 8)\n",
    "mod_emb = gerar_modelo_keras(len(dic))\n",
    "mod_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_emb.compile(loss='binary_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "127/128 [============================>.] - ETA: 0s - loss: 0.5801Epoch 1/50\n",
      "Epoch 1/50\n",
      "128/128 [==============================] - 11s 84ms/step - loss: 0.5800 - val_loss: 0.6410\n",
      "Epoch 2/50\n",
      "128/128 [==============================] - 10s 82ms/step - loss: 0.5803 - val_loss: 0.6326\n",
      "Epoch 3/50\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.5804 - val_loss: 0.6074\n",
      "Epoch 4/50\n",
      "128/128 [==============================] - 11s 87ms/step - loss: 0.5828 - val_loss: 0.6273\n",
      "Epoch 5/50\n",
      "128/128 [==============================] - 12s 91ms/step - loss: 0.5775 - val_loss: 0.6016\n",
      "Epoch 6/50\n",
      "128/128 [==============================] - 11s 85ms/step - loss: 0.5793 - val_loss: 0.6209\n",
      "Epoch 7/50\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.5811 - val_loss: 0.6173\n",
      "Epoch 8/50\n",
      "128/128 [==============================] - 11s 85ms/step - loss: 0.5779 - val_loss: 0.6246\n",
      "Epoch 9/50\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5801 - val_loss: 0.6260\n",
      "Epoch 10/50\n",
      "128/128 [==============================] - 11s 87ms/step - loss: 0.5791 - val_loss: 0.6187\n",
      "Epoch 11/50\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 0.5830 - val_loss: 0.6237\n",
      "Epoch 12/50\n",
      "128/128 [==============================] - 10s 76ms/step - loss: 0.5853 - val_loss: 0.6259\n",
      "Epoch 13/50\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 0.5768 - val_loss: 0.6292\n",
      "Epoch 14/50\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5862 - val_loss: 0.6259\n",
      "Epoch 15/50\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 0.5818 - val_loss: 0.6165\n",
      "Epoch 16/50\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 0.5781 - val_loss: 0.6126\n",
      "Epoch 17/50\n",
      "128/128 [==============================] - 11s 86ms/step - loss: 0.5792 - val_loss: 0.5957\n",
      "Epoch 18/50\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.5878 - val_loss: 0.6244\n",
      "Epoch 19/50\n",
      "128/128 [==============================] - 11s 85ms/step - loss: 0.5783 - val_loss: 0.6420\n",
      "Epoch 20/50\n",
      "128/128 [==============================] - 12s 93ms/step - loss: 0.5811 - val_loss: 0.6136\n",
      "Epoch 21/50\n",
      "128/128 [==============================] - 11s 85ms/step - loss: 0.5762 - val_loss: 0.6175\n",
      "Epoch 22/50\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5770 - val_loss: 0.6114\n",
      "Epoch 23/50\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 0.5877 - val_loss: 0.6338\n",
      "Epoch 24/50\n",
      "128/128 [==============================] - 10s 76ms/step - loss: 0.5788 - val_loss: 0.6336\n",
      "Epoch 25/50\n",
      "128/128 [==============================] - 11s 84ms/step - loss: 0.5807 - val_loss: 0.6116\n",
      "Epoch 26/50\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.5847 - val_loss: 0.6192\n",
      "Epoch 27/50\n",
      "128/128 [==============================] - 11s 87ms/step - loss: 0.5848 - val_loss: 0.6188\n",
      "Epoch 28/50\n",
      "128/128 [==============================] - 11s 87ms/step - loss: 0.5735 - val_loss: 0.6259\n",
      "Epoch 29/50\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 0.5788 - val_loss: 0.6132\n",
      "Epoch 30/50\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.5829 - val_loss: 0.6390\n",
      "Epoch 31/50\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 0.5808 - val_loss: 0.6429\n",
      "Epoch 32/50\n",
      "128/128 [==============================] - 11s 85ms/step - loss: 0.5753 - val_loss: 0.6280\n",
      "Epoch 33/50\n",
      "128/128 [==============================] - 10s 77ms/step - loss: 0.5747 - val_loss: 0.6038\n",
      "Epoch 34/50\n",
      "128/128 [==============================] - 11s 85ms/step - loss: 0.5778 - val_loss: 0.6128\n",
      "Epoch 35/50\n",
      "128/128 [==============================] - 11s 86ms/step - loss: 0.5731 - val_loss: 0.6301\n",
      "Epoch 36/50\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5784 - val_loss: 0.6349\n",
      "Epoch 37/50\n",
      "128/128 [==============================] - 11s 82ms/step - loss: 0.5765 - val_loss: 0.6222\n",
      "Epoch 38/50\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5822 - val_loss: 0.6061\n",
      "Epoch 39/50\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 0.5765 - val_loss: 0.6228\n",
      "Epoch 40/50\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.5803 - val_loss: 0.6254\n",
      "Epoch 41/50\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 0.5797 - val_loss: 0.5974\n",
      "Epoch 42/50\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5782 - val_loss: 0.6203\n",
      "Epoch 43/50\n",
      "128/128 [==============================] - 10s 77ms/step - loss: 0.5703 - val_loss: 0.6243\n",
      "Epoch 44/50\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.5776 - val_loss: 0.6267\n",
      "Epoch 45/50\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.5802 - val_loss: 0.6298\n",
      "Epoch 46/50\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.5812 - val_loss: 0.6208\n",
      "Epoch 47/50\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.5777 - val_loss: 0.6170\n",
      "Epoch 48/50\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 0.5818 - val_loss: 0.6127\n",
      "Epoch 49/50\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5795 - val_loss: 0.6269\n",
      "Epoch 50/50\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 0.5772 - val_loss: 0.6045\n"
     ]
    }
   ],
   "source": [
    "csv_logger = keras.callbacks.CSVLogger('logs/pretreino.log')\n",
    "parar_mlp = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=500, min_delta=0)\n",
    "salvar_mlp = keras.callbacks.ModelCheckpoint('logs/pretreino.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "historico3 = mod_emb.fit_generator(generator=gerador_treino, steps_per_epoch=128, epochs=50, use_multiprocessing=True, shuffle=False, workers=32, max_queue_size=64, validation_data=gerador_valida, validation_steps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(historico3.history['loss'])\n",
    "plt.plot(historico3.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = mod_emb.get_layer('vetorizacao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18268, 50)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_train.get_layer('emb_layer').set_weights(mod_emb.get_layer('vetorizacao').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_model(vocab_size, story_max_len = 256, emb_dim=50, num_lstm=64, num_relu1=32, num_relu2=16, num_relu3=8):\n",
    "    input_layer = keras.layers.Input(shape=(story_max_len,), name='input_layer')\n",
    "    X = keras.layers.Embedding(vocab_size, emb_dim, mask_zero=True, name='emb_layer')(input_layer)\n",
    "    X = keras.layers.LSTM(num_lstm)(X)\n",
    "    X = keras.layers.Dropout(0.4)(X)\n",
    "    X = keras.layers.Dense(num_relu1, activation='relu')(X)\n",
    "    X = keras.layers.Dropout(0.2)(X)\n",
    "    X = keras.layers.Dense(num_relu2, activation='relu')(X)\n",
    "    X = keras.layers.Dense(num_relu2, activation='relu')(X)\n",
    "    output_layer = keras.layers.Dense(1, activation='relu')(X)\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "def initialize_train_model(model, emb_weights):\n",
    "    model.compile(loss='mae',optimizer='adam')\n",
    "    model.get_layer('emb_layer').set_weights(emb_weights)\n",
    "\n",
    "mod_train = create_train_model(len(dic))\n",
    "initialize_train_model(mod_train, mod_emb.get_layer('vetorizacao').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2801 samples, validate on 933 samples\n",
      "Epoch 1/10\n",
      "2801/2801 [==============================] - 22s 8ms/step - loss: 9.0253 - val_loss: 6.2359\n",
      "Epoch 2/10\n",
      "2801/2801 [==============================] - 23s 8ms/step - loss: 8.5050 - val_loss: 5.9250\n",
      "Epoch 3/10\n",
      "2801/2801 [==============================] - 22s 8ms/step - loss: 8.0853 - val_loss: 6.0607\n",
      "Epoch 4/10\n",
      "2801/2801 [==============================] - 22s 8ms/step - loss: 7.4871 - val_loss: 6.1678\n",
      "Epoch 5/10\n",
      "2801/2801 [==============================] - 22s 8ms/step - loss: 7.2221 - val_loss: 6.0357\n",
      "Epoch 6/10\n",
      "2801/2801 [==============================] - 21s 8ms/step - loss: 6.3468 - val_loss: 6.0630\n",
      "Epoch 7/10\n",
      "2801/2801 [==============================] - 21s 8ms/step - loss: 5.5445 - val_loss: 6.1631\n",
      "Epoch 8/10\n",
      "2801/2801 [==============================] - 21s 8ms/step - loss: 5.1171 - val_loss: 6.0894\n",
      "Epoch 9/10\n",
      "2801/2801 [==============================] - 21s 8ms/step - loss: 4.7606 - val_loss: 6.6100\n",
      "Epoch 10/10\n",
      "2801/2801 [==============================] - 21s 8ms/step - loss: 4.3676 - val_loss: 6.2091\n"
     ]
    }
   ],
   "source": [
    "save_best = keras.callbacks.ModelCheckpoint('models/train.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "hist_train = mod_train.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val), callbacks=[save_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "933/933 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "212.65664538539875"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_train.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_train = keras.models.load_model('models/train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "predito = mod_train.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.2479343 ],\n",
       "       [5.5068016 ],\n",
       "       [6.1625166 ],\n",
       "       [6.0079937 ],\n",
       "       [5.7648916 ],\n",
       "       [0.33027086],\n",
       "       [1.412546  ],\n",
       "       [3.8277018 ],\n",
       "       [5.860844  ],\n",
       "       [4.619356  ],\n",
       "       [6.0824385 ],\n",
       "       [6.007251  ],\n",
       "       [3.3315358 ],\n",
       "       [5.6457977 ],\n",
       "       [3.1196175 ],\n",
       "       [2.5677874 ],\n",
       "       [1.056709  ],\n",
       "       [5.921824  ],\n",
       "       [5.9003954 ],\n",
       "       [5.9565983 ],\n",
       "       [2.8244567 ],\n",
       "       [2.5625262 ],\n",
       "       [2.2353    ],\n",
       "       [3.7661562 ],\n",
       "       [5.9715652 ],\n",
       "       [5.7421    ],\n",
       "       [5.5029097 ],\n",
       "       [6.340906  ],\n",
       "       [6.1059475 ],\n",
       "       [6.0940714 ],\n",
       "       [5.609194  ],\n",
       "       [3.1144717 ],\n",
       "       [5.9863887 ],\n",
       "       [2.4334645 ],\n",
       "       [4.2730184 ],\n",
       "       [6.1046696 ],\n",
       "       [5.728243  ],\n",
       "       [4.7947245 ],\n",
       "       [5.8513436 ],\n",
       "       [6.0013294 ],\n",
       "       [6.437192  ],\n",
       "       [5.918774  ],\n",
       "       [4.5211277 ],\n",
       "       [5.430067  ],\n",
       "       [5.854681  ],\n",
       "       [6.3503003 ],\n",
       "       [6.3236227 ],\n",
       "       [5.9643526 ],\n",
       "       [3.1315036 ],\n",
       "       [5.9991508 ],\n",
       "       [5.5255327 ],\n",
       "       [6.311565  ],\n",
       "       [5.7988224 ],\n",
       "       [1.9953356 ],\n",
       "       [5.998955  ],\n",
       "       [0.75521576],\n",
       "       [1.6014465 ],\n",
       "       [1.6231123 ],\n",
       "       [5.9823813 ],\n",
       "       [4.565504  ],\n",
       "       [4.7650213 ],\n",
       "       [2.3907578 ],\n",
       "       [4.8929806 ],\n",
       "       [6.126437  ],\n",
       "       [2.0065584 ],\n",
       "       [3.434156  ],\n",
       "       [5.99321   ],\n",
       "       [5.8979235 ],\n",
       "       [5.3603015 ],\n",
       "       [5.8580656 ],\n",
       "       [5.849188  ],\n",
       "       [2.4104035 ],\n",
       "       [1.4832802 ],\n",
       "       [5.8828115 ],\n",
       "       [2.6941848 ],\n",
       "       [2.3443465 ],\n",
       "       [3.052349  ],\n",
       "       [2.374909  ],\n",
       "       [2.785952  ],\n",
       "       [1.8680998 ],\n",
       "       [3.3412697 ],\n",
       "       [3.8002434 ],\n",
       "       [5.7926235 ],\n",
       "       [1.4502115 ],\n",
       "       [4.8613305 ],\n",
       "       [2.2439232 ],\n",
       "       [3.661233  ],\n",
       "       [5.881628  ],\n",
       "       [5.2896433 ],\n",
       "       [2.7627249 ],\n",
       "       [1.7993865 ],\n",
       "       [5.161257  ],\n",
       "       [5.898763  ],\n",
       "       [5.0450644 ],\n",
       "       [6.166722  ],\n",
       "       [1.0879323 ],\n",
       "       [6.1206045 ],\n",
       "       [6.08455   ],\n",
       "       [6.1090674 ],\n",
       "       [4.9204216 ],\n",
       "       [5.106602  ],\n",
       "       [2.0931845 ],\n",
       "       [6.0895762 ],\n",
       "       [5.5749884 ],\n",
       "       [3.6316476 ],\n",
       "       [6.001997  ],\n",
       "       [3.3532116 ],\n",
       "       [5.605846  ],\n",
       "       [5.620922  ],\n",
       "       [2.5689716 ],\n",
       "       [0.82902646],\n",
       "       [3.7202768 ],\n",
       "       [2.9062643 ],\n",
       "       [2.3789215 ],\n",
       "       [5.1098094 ],\n",
       "       [0.28042302],\n",
       "       [1.4052013 ],\n",
       "       [5.729705  ],\n",
       "       [5.830862  ],\n",
       "       [3.0423498 ],\n",
       "       [5.6818895 ],\n",
       "       [1.3714359 ],\n",
       "       [1.4214063 ],\n",
       "       [2.9107974 ],\n",
       "       [2.9316897 ],\n",
       "       [5.130701  ],\n",
       "       [5.5019884 ],\n",
       "       [4.795742  ],\n",
       "       [5.627058  ],\n",
       "       [2.8305087 ],\n",
       "       [5.8762765 ],\n",
       "       [6.215688  ],\n",
       "       [2.2665868 ],\n",
       "       [5.333146  ],\n",
       "       [3.7662816 ],\n",
       "       [0.25184268],\n",
       "       [0.3167443 ],\n",
       "       [5.991843  ],\n",
       "       [5.8016458 ],\n",
       "       [2.6316595 ],\n",
       "       [2.5645175 ],\n",
       "       [2.0911188 ],\n",
       "       [0.302832  ],\n",
       "       [3.111258  ],\n",
       "       [2.0773125 ],\n",
       "       [5.7714057 ],\n",
       "       [2.7300706 ],\n",
       "       [5.974368  ],\n",
       "       [5.839945  ],\n",
       "       [1.6633908 ],\n",
       "       [5.828469  ],\n",
       "       [5.830439  ],\n",
       "       [5.712565  ],\n",
       "       [2.275555  ],\n",
       "       [3.9263942 ],\n",
       "       [6.306959  ],\n",
       "       [3.9928467 ],\n",
       "       [1.9751674 ],\n",
       "       [5.8727894 ],\n",
       "       [4.2138395 ],\n",
       "       [1.6736742 ],\n",
       "       [6.506938  ],\n",
       "       [2.7696273 ],\n",
       "       [6.237972  ],\n",
       "       [6.1525164 ],\n",
       "       [6.186124  ],\n",
       "       [5.9488297 ],\n",
       "       [5.6115246 ],\n",
       "       [5.4743814 ],\n",
       "       [2.2217383 ],\n",
       "       [4.20112   ],\n",
       "       [4.882249  ],\n",
       "       [6.160265  ],\n",
       "       [6.1893063 ],\n",
       "       [5.8570986 ],\n",
       "       [5.6624017 ],\n",
       "       [2.7857237 ],\n",
       "       [6.0045357 ],\n",
       "       [6.0114903 ],\n",
       "       [1.9501411 ],\n",
       "       [2.0616417 ],\n",
       "       [5.203827  ],\n",
       "       [2.427171  ],\n",
       "       [3.2212954 ],\n",
       "       [5.9500837 ],\n",
       "       [0.93396664],\n",
       "       [4.1209745 ],\n",
       "       [2.0173044 ],\n",
       "       [5.426343  ],\n",
       "       [6.0004644 ],\n",
       "       [1.0846518 ],\n",
       "       [3.6765866 ],\n",
       "       [2.0840335 ],\n",
       "       [2.0593665 ],\n",
       "       [2.2741585 ],\n",
       "       [3.170275  ],\n",
       "       [1.0934163 ],\n",
       "       [3.1804    ],\n",
       "       [1.8856634 ],\n",
       "       [6.0213175 ],\n",
       "       [5.9098454 ],\n",
       "       [3.474094  ],\n",
       "       [5.501005  ],\n",
       "       [5.8005505 ],\n",
       "       [1.8458213 ],\n",
       "       [3.9968617 ],\n",
       "       [3.6465404 ],\n",
       "       [2.7189574 ],\n",
       "       [2.053736  ],\n",
       "       [2.031155  ],\n",
       "       [6.0132427 ],\n",
       "       [0.34406585],\n",
       "       [3.9713385 ],\n",
       "       [0.3294288 ],\n",
       "       [2.217677  ],\n",
       "       [2.4467094 ],\n",
       "       [3.9654005 ],\n",
       "       [2.0169983 ],\n",
       "       [2.384333  ],\n",
       "       [1.8748976 ],\n",
       "       [6.0176525 ],\n",
       "       [2.217524  ],\n",
       "       [2.0346189 ],\n",
       "       [2.3146389 ],\n",
       "       [2.1782045 ],\n",
       "       [6.044241  ],\n",
       "       [1.6704204 ],\n",
       "       [2.0190358 ],\n",
       "       [4.358419  ],\n",
       "       [5.1961536 ],\n",
       "       [0.36050695],\n",
       "       [2.2206671 ],\n",
       "       [1.6504629 ],\n",
       "       [0.44633695],\n",
       "       [2.1609614 ],\n",
       "       [6.253771  ],\n",
       "       [0.6014663 ],\n",
       "       [2.1828494 ],\n",
       "       [2.210861  ],\n",
       "       [3.562056  ],\n",
       "       [6.0212593 ],\n",
       "       [5.0839443 ],\n",
       "       [5.9484253 ],\n",
       "       [6.061628  ],\n",
       "       [3.3029563 ],\n",
       "       [5.272865  ],\n",
       "       [2.3607457 ],\n",
       "       [5.587288  ],\n",
       "       [6.1555567 ],\n",
       "       [2.3855157 ],\n",
       "       [2.0741568 ],\n",
       "       [5.8738346 ],\n",
       "       [2.1075773 ],\n",
       "       [6.011101  ],\n",
       "       [4.9388905 ],\n",
       "       [6.159425  ],\n",
       "       [6.048937  ],\n",
       "       [2.9637206 ],\n",
       "       [2.8238182 ],\n",
       "       [4.4836287 ],\n",
       "       [2.2749176 ],\n",
       "       [3.0679262 ],\n",
       "       [5.7281523 ],\n",
       "       [6.0765476 ],\n",
       "       [5.799899  ],\n",
       "       [6.1078176 ],\n",
       "       [3.502714  ],\n",
       "       [5.721534  ],\n",
       "       [0.638625  ],\n",
       "       [5.135536  ],\n",
       "       [5.9412384 ],\n",
       "       [5.7629538 ],\n",
       "       [4.8155513 ],\n",
       "       [3.4252818 ],\n",
       "       [5.6794167 ],\n",
       "       [2.531623  ],\n",
       "       [1.5267421 ],\n",
       "       [4.9221115 ],\n",
       "       [6.0359344 ],\n",
       "       [5.6187205 ],\n",
       "       [3.3430543 ],\n",
       "       [2.8066263 ],\n",
       "       [2.1464586 ],\n",
       "       [0.7062929 ],\n",
       "       [1.6880256 ],\n",
       "       [1.6886443 ],\n",
       "       [5.980035  ],\n",
       "       [2.6073198 ],\n",
       "       [1.8442158 ],\n",
       "       [2.5492425 ],\n",
       "       [5.3343606 ],\n",
       "       [2.8026886 ],\n",
       "       [1.9400142 ],\n",
       "       [2.7404542 ],\n",
       "       [4.8789635 ],\n",
       "       [5.264205  ],\n",
       "       [3.2370048 ],\n",
       "       [4.4295125 ],\n",
       "       [2.77328   ],\n",
       "       [6.0807705 ],\n",
       "       [0.48573557],\n",
       "       [3.357879  ],\n",
       "       [3.087305  ],\n",
       "       [6.036927  ],\n",
       "       [5.8919115 ],\n",
       "       [2.5550768 ],\n",
       "       [1.0013038 ],\n",
       "       [6.386156  ],\n",
       "       [3.146062  ],\n",
       "       [2.8811831 ],\n",
       "       [0.29141864],\n",
       "       [5.4375963 ],\n",
       "       [2.463848  ],\n",
       "       [5.956933  ],\n",
       "       [5.6808534 ],\n",
       "       [4.139584  ],\n",
       "       [6.1222477 ],\n",
       "       [3.131905  ],\n",
       "       [3.599817  ],\n",
       "       [6.053379  ],\n",
       "       [5.699995  ],\n",
       "       [5.9284325 ],\n",
       "       [5.151578  ],\n",
       "       [4.746393  ],\n",
       "       [2.502355  ],\n",
       "       [1.6178182 ],\n",
       "       [6.124937  ],\n",
       "       [5.9862204 ],\n",
       "       [1.8874644 ],\n",
       "       [6.1169977 ],\n",
       "       [2.964714  ],\n",
       "       [5.802847  ],\n",
       "       [4.177169  ],\n",
       "       [1.1472667 ],\n",
       "       [5.9601583 ],\n",
       "       [1.7061199 ],\n",
       "       [5.8240657 ],\n",
       "       [2.9201384 ],\n",
       "       [2.081324  ],\n",
       "       [2.282897  ],\n",
       "       [6.031099  ],\n",
       "       [5.8348565 ],\n",
       "       [6.104224  ],\n",
       "       [1.7375423 ],\n",
       "       [2.4862123 ],\n",
       "       [1.9613945 ],\n",
       "       [1.9016999 ],\n",
       "       [3.7450294 ],\n",
       "       [1.7596841 ],\n",
       "       [5.795856  ],\n",
       "       [4.438711  ],\n",
       "       [5.6244698 ],\n",
       "       [0.4851852 ],\n",
       "       [4.221674  ],\n",
       "       [5.6586323 ],\n",
       "       [5.965428  ],\n",
       "       [0.691952  ],\n",
       "       [6.0542426 ],\n",
       "       [5.805187  ],\n",
       "       [6.224044  ],\n",
       "       [6.0638356 ],\n",
       "       [4.7914076 ],\n",
       "       [4.506461  ],\n",
       "       [5.0442266 ],\n",
       "       [5.7342    ],\n",
       "       [5.000465  ],\n",
       "       [4.072975  ],\n",
       "       [0.4008288 ],\n",
       "       [0.92395806],\n",
       "       [2.9106898 ],\n",
       "       [5.504626  ],\n",
       "       [2.6746035 ],\n",
       "       [3.6614957 ],\n",
       "       [3.6188364 ],\n",
       "       [5.3638277 ],\n",
       "       [5.9247475 ],\n",
       "       [4.217458  ],\n",
       "       [6.1197214 ],\n",
       "       [6.1502943 ],\n",
       "       [6.1035004 ],\n",
       "       [6.1035004 ],\n",
       "       [5.1748323 ],\n",
       "       [5.4856825 ],\n",
       "       [4.9727025 ],\n",
       "       [4.7633753 ],\n",
       "       [4.1687922 ],\n",
       "       [1.5134581 ],\n",
       "       [5.9478426 ],\n",
       "       [6.137722  ],\n",
       "       [2.1361666 ],\n",
       "       [1.0516578 ],\n",
       "       [5.2475414 ],\n",
       "       [3.1045566 ],\n",
       "       [1.6058899 ],\n",
       "       [3.2365036 ],\n",
       "       [4.4569006 ],\n",
       "       [5.7930098 ],\n",
       "       [2.3593097 ],\n",
       "       [4.519259  ],\n",
       "       [2.7734196 ],\n",
       "       [3.443481  ],\n",
       "       [5.4100776 ],\n",
       "       [2.8258314 ],\n",
       "       [4.7907963 ],\n",
       "       [0.7446958 ],\n",
       "       [3.735341  ],\n",
       "       [5.5858297 ],\n",
       "       [5.8679533 ],\n",
       "       [2.7284307 ],\n",
       "       [0.3435376 ],\n",
       "       [5.107196  ],\n",
       "       [3.942852  ],\n",
       "       [6.1107764 ],\n",
       "       [6.1107764 ],\n",
       "       [6.0609694 ],\n",
       "       [6.080213  ],\n",
       "       [6.1202736 ],\n",
       "       [6.0543065 ],\n",
       "       [6.0520487 ],\n",
       "       [6.0538607 ],\n",
       "       [6.0520487 ],\n",
       "       [6.0529776 ],\n",
       "       [5.081813  ],\n",
       "       [3.8133054 ],\n",
       "       [2.3698974 ],\n",
       "       [5.341091  ],\n",
       "       [2.0509899 ],\n",
       "       [5.869067  ],\n",
       "       [6.0441628 ],\n",
       "       [3.6106007 ],\n",
       "       [2.203556  ],\n",
       "       [4.101783  ],\n",
       "       [2.7116303 ],\n",
       "       [1.8792619 ],\n",
       "       [2.140895  ],\n",
       "       [2.7126172 ],\n",
       "       [5.963823  ],\n",
       "       [3.5838387 ],\n",
       "       [1.7960902 ],\n",
       "       [2.068794  ],\n",
       "       [6.0234547 ],\n",
       "       [3.192635  ],\n",
       "       [5.0378046 ],\n",
       "       [2.2177715 ],\n",
       "       [1.9732566 ],\n",
       "       [3.659319  ],\n",
       "       [1.7871104 ],\n",
       "       [3.2001927 ],\n",
       "       [3.6620145 ],\n",
       "       [5.8408794 ],\n",
       "       [4.7389717 ],\n",
       "       [5.7598143 ],\n",
       "       [5.3913083 ],\n",
       "       [3.1729095 ],\n",
       "       [0.8720795 ],\n",
       "       [3.1884022 ],\n",
       "       [2.0156326 ],\n",
       "       [0.57599586],\n",
       "       [5.066248  ],\n",
       "       [3.1507597 ],\n",
       "       [5.837264  ],\n",
       "       [5.0187073 ],\n",
       "       [5.386757  ],\n",
       "       [2.148015  ],\n",
       "       [5.176579  ],\n",
       "       [0.4202697 ],\n",
       "       [0.24634206],\n",
       "       [4.731864  ],\n",
       "       [5.0825596 ],\n",
       "       [3.7529953 ],\n",
       "       [2.576521  ],\n",
       "       [0.37555498],\n",
       "       [5.400303  ],\n",
       "       [3.1566803 ],\n",
       "       [3.5407114 ],\n",
       "       [5.89187   ],\n",
       "       [1.9834286 ],\n",
       "       [5.946071  ],\n",
       "       [5.56474   ],\n",
       "       [3.2327085 ],\n",
       "       [2.7085857 ],\n",
       "       [5.574671  ],\n",
       "       [0.33632085],\n",
       "       [0.26960883],\n",
       "       [1.7930573 ],\n",
       "       [4.3248253 ],\n",
       "       [4.537708  ],\n",
       "       [2.1488252 ],\n",
       "       [0.52818763],\n",
       "       [5.97101   ],\n",
       "       [2.9198318 ],\n",
       "       [3.2648525 ],\n",
       "       [1.9411882 ],\n",
       "       [0.9731176 ],\n",
       "       [4.576253  ],\n",
       "       [5.9930944 ],\n",
       "       [2.1255522 ],\n",
       "       [6.0161085 ],\n",
       "       [3.0408134 ],\n",
       "       [4.1803093 ],\n",
       "       [1.9519244 ],\n",
       "       [6.090682  ],\n",
       "       [5.9292006 ],\n",
       "       [2.7445953 ],\n",
       "       [3.0671759 ],\n",
       "       [5.8962603 ],\n",
       "       [5.797995  ],\n",
       "       [1.3717121 ],\n",
       "       [3.7011719 ],\n",
       "       [5.3502827 ],\n",
       "       [1.6399592 ],\n",
       "       [2.1261592 ],\n",
       "       [0.24318713],\n",
       "       [4.382386  ],\n",
       "       [2.097705  ],\n",
       "       [2.2053113 ],\n",
       "       [2.2231693 ],\n",
       "       [0.7997415 ],\n",
       "       [2.3360171 ],\n",
       "       [4.275635  ],\n",
       "       [1.9438955 ],\n",
       "       [2.114839  ],\n",
       "       [2.5609212 ],\n",
       "       [1.0598408 ],\n",
       "       [1.2396518 ],\n",
       "       [4.399201  ],\n",
       "       [6.0508137 ],\n",
       "       [2.22727   ],\n",
       "       [5.9402723 ],\n",
       "       [5.0128603 ],\n",
       "       [2.0521502 ],\n",
       "       [5.9694543 ],\n",
       "       [5.9973345 ],\n",
       "       [5.955681  ],\n",
       "       [3.2960832 ],\n",
       "       [3.8891268 ],\n",
       "       [4.115244  ],\n",
       "       [3.0048728 ],\n",
       "       [5.591978  ],\n",
       "       [5.3089914 ],\n",
       "       [2.304748  ],\n",
       "       [5.849193  ],\n",
       "       [5.971884  ],\n",
       "       [5.6976056 ],\n",
       "       [5.721238  ],\n",
       "       [2.550695  ],\n",
       "       [3.9186635 ],\n",
       "       [0.31035385],\n",
       "       [2.1718445 ],\n",
       "       [0.3477796 ],\n",
       "       [5.700708  ],\n",
       "       [5.2682247 ],\n",
       "       [5.9164195 ],\n",
       "       [4.153313  ],\n",
       "       [2.3719182 ],\n",
       "       [2.1192832 ],\n",
       "       [4.9556828 ],\n",
       "       [6.062579  ],\n",
       "       [1.2789035 ],\n",
       "       [5.186946  ],\n",
       "       [6.1675854 ],\n",
       "       [6.1401224 ],\n",
       "       [6.172933  ],\n",
       "       [6.211149  ],\n",
       "       [6.115954  ],\n",
       "       [5.950014  ],\n",
       "       [4.2859206 ],\n",
       "       [2.6342392 ],\n",
       "       [5.949539  ],\n",
       "       [2.6105132 ],\n",
       "       [3.18799   ],\n",
       "       [5.973813  ],\n",
       "       [5.529251  ],\n",
       "       [4.2611947 ],\n",
       "       [2.8945713 ],\n",
       "       [1.6926059 ],\n",
       "       [0.41025016],\n",
       "       [4.679902  ],\n",
       "       [3.3073618 ],\n",
       "       [2.7517457 ],\n",
       "       [0.39374056],\n",
       "       [0.28507084],\n",
       "       [5.913517  ],\n",
       "       [5.99949   ],\n",
       "       [3.3603446 ],\n",
       "       [3.6509829 ],\n",
       "       [6.034851  ],\n",
       "       [2.8504565 ],\n",
       "       [0.30650952],\n",
       "       [2.2123957 ],\n",
       "       [4.5910993 ],\n",
       "       [1.5633804 ],\n",
       "       [6.141757  ],\n",
       "       [5.076797  ],\n",
       "       [6.339596  ],\n",
       "       [3.0932243 ],\n",
       "       [2.2265801 ],\n",
       "       [1.9480289 ],\n",
       "       [4.209603  ],\n",
       "       [5.717105  ],\n",
       "       [5.8949623 ],\n",
       "       [5.0074663 ],\n",
       "       [1.3252105 ],\n",
       "       [0.70969504],\n",
       "       [1.2752768 ],\n",
       "       [2.1385522 ],\n",
       "       [5.4364853 ],\n",
       "       [6.1066127 ],\n",
       "       [5.8855004 ],\n",
       "       [2.1693275 ],\n",
       "       [6.007144  ],\n",
       "       [5.5727777 ],\n",
       "       [5.7852764 ],\n",
       "       [5.9212704 ],\n",
       "       [3.8306282 ],\n",
       "       [0.278123  ],\n",
       "       [4.4834557 ],\n",
       "       [0.5205284 ],\n",
       "       [2.9119735 ],\n",
       "       [5.958748  ],\n",
       "       [5.8575583 ],\n",
       "       [2.0419314 ],\n",
       "       [5.7015715 ],\n",
       "       [5.4029417 ],\n",
       "       [6.0442667 ],\n",
       "       [3.063848  ],\n",
       "       [5.85189   ],\n",
       "       [2.31223   ],\n",
       "       [4.3691955 ],\n",
       "       [2.3201847 ],\n",
       "       [1.923611  ],\n",
       "       [6.0294576 ],\n",
       "       [1.1981243 ],\n",
       "       [2.0939696 ],\n",
       "       [2.4288118 ],\n",
       "       [5.4343686 ],\n",
       "       [3.4371421 ],\n",
       "       [5.812252  ],\n",
       "       [1.7938033 ],\n",
       "       [1.808839  ],\n",
       "       [2.7197204 ],\n",
       "       [5.360181  ],\n",
       "       [5.6220527 ],\n",
       "       [5.615676  ],\n",
       "       [4.087284  ],\n",
       "       [6.1465845 ],\n",
       "       [2.0340154 ],\n",
       "       [5.8401694 ],\n",
       "       [2.8153443 ],\n",
       "       [5.9082947 ],\n",
       "       [5.921939  ],\n",
       "       [5.7905555 ],\n",
       "       [5.293377  ],\n",
       "       [6.16389   ],\n",
       "       [1.9305562 ],\n",
       "       [2.898279  ],\n",
       "       [1.8608726 ],\n",
       "       [2.1922865 ],\n",
       "       [4.0937843 ],\n",
       "       [6.004403  ],\n",
       "       [2.5284567 ],\n",
       "       [3.0695539 ],\n",
       "       [1.4924015 ],\n",
       "       [2.8837414 ],\n",
       "       [0.38736615],\n",
       "       [5.863152  ],\n",
       "       [1.611267  ],\n",
       "       [6.077002  ],\n",
       "       [5.9531813 ],\n",
       "       [1.6419805 ],\n",
       "       [6.0330753 ],\n",
       "       [5.8541255 ],\n",
       "       [5.900958  ],\n",
       "       [2.2277267 ],\n",
       "       [0.3941947 ],\n",
       "       [2.8724694 ],\n",
       "       [2.5947511 ],\n",
       "       [6.0683794 ],\n",
       "       [2.2002463 ],\n",
       "       [3.9838643 ],\n",
       "       [6.0607667 ],\n",
       "       [6.0811763 ],\n",
       "       [6.0921874 ],\n",
       "       [6.0871387 ],\n",
       "       [6.087823  ],\n",
       "       [6.0848484 ],\n",
       "       [6.0921483 ],\n",
       "       [5.393326  ],\n",
       "       [6.0757184 ],\n",
       "       [5.8174067 ],\n",
       "       [4.1486683 ],\n",
       "       [4.993886  ],\n",
       "       [5.8539596 ],\n",
       "       [5.743759  ],\n",
       "       [4.1445904 ],\n",
       "       [5.090193  ],\n",
       "       [5.730918  ],\n",
       "       [6.044284  ],\n",
       "       [4.2646294 ],\n",
       "       [5.090193  ],\n",
       "       [5.02386   ],\n",
       "       [5.1805286 ],\n",
       "       [1.5480338 ],\n",
       "       [5.0901923 ],\n",
       "       [5.2409906 ],\n",
       "       [5.7488995 ],\n",
       "       [6.0659328 ],\n",
       "       [5.862459  ],\n",
       "       [3.0628815 ],\n",
       "       [4.001151  ],\n",
       "       [2.483664  ],\n",
       "       [6.094555  ],\n",
       "       [3.155933  ],\n",
       "       [3.402859  ],\n",
       "       [5.4454975 ],\n",
       "       [5.087225  ],\n",
       "       [5.4353433 ],\n",
       "       [5.051176  ],\n",
       "       [6.305087  ],\n",
       "       [6.12504   ],\n",
       "       [5.383964  ],\n",
       "       [5.0263276 ],\n",
       "       [2.7230272 ],\n",
       "       [5.4454975 ],\n",
       "       [5.087225  ],\n",
       "       [6.149261  ],\n",
       "       [5.445497  ],\n",
       "       [5.087225  ],\n",
       "       [3.0285301 ],\n",
       "       [5.5717983 ],\n",
       "       [5.2377696 ],\n",
       "       [3.3392172 ],\n",
       "       [6.3503933 ],\n",
       "       [6.0844407 ],\n",
       "       [6.1549783 ],\n",
       "       [5.519889  ],\n",
       "       [5.9804897 ],\n",
       "       [6.0538454 ],\n",
       "       [2.029079  ],\n",
       "       [5.8084846 ],\n",
       "       [5.8094654 ],\n",
       "       [5.804102  ],\n",
       "       [5.8045487 ],\n",
       "       [6.049035  ],\n",
       "       [4.340416  ],\n",
       "       [3.8448215 ],\n",
       "       [5.033511  ],\n",
       "       [1.711294  ],\n",
       "       [2.6808305 ],\n",
       "       [3.5920463 ],\n",
       "       [6.0225906 ],\n",
       "       [3.7662625 ],\n",
       "       [3.3847523 ],\n",
       "       [2.5752172 ],\n",
       "       [1.5078367 ],\n",
       "       [3.058985  ],\n",
       "       [0.60329014],\n",
       "       [6.180865  ],\n",
       "       [2.6926684 ],\n",
       "       [5.78653   ],\n",
       "       [0.8616776 ],\n",
       "       [2.761826  ],\n",
       "       [2.5540164 ],\n",
       "       [1.7512958 ],\n",
       "       [4.0977397 ],\n",
       "       [2.5420492 ],\n",
       "       [5.8783956 ],\n",
       "       [5.3465195 ],\n",
       "       [5.8751965 ],\n",
       "       [5.9914823 ],\n",
       "       [6.021627  ],\n",
       "       [6.082797  ],\n",
       "       [5.887623  ],\n",
       "       [4.8220353 ],\n",
       "       [6.077808  ],\n",
       "       [5.894732  ],\n",
       "       [5.545491  ],\n",
       "       [4.81546   ],\n",
       "       [3.5495791 ],\n",
       "       [4.4259877 ],\n",
       "       [5.948662  ],\n",
       "       [2.8494637 ],\n",
       "       [4.8656707 ],\n",
       "       [5.877556  ],\n",
       "       [1.6999522 ],\n",
       "       [3.5923333 ],\n",
       "       [2.5790462 ],\n",
       "       [1.1782209 ],\n",
       "       [2.193535  ],\n",
       "       [4.1941223 ],\n",
       "       [6.2255607 ],\n",
       "       [5.53604   ],\n",
       "       [5.676868  ],\n",
       "       [6.0951037 ],\n",
       "       [4.707567  ],\n",
       "       [3.6863246 ],\n",
       "       [3.3108363 ],\n",
       "       [1.6206675 ],\n",
       "       [5.606786  ],\n",
       "       [3.1350539 ],\n",
       "       [1.1200924 ],\n",
       "       [1.6155933 ],\n",
       "       [2.0112054 ],\n",
       "       [3.7817268 ],\n",
       "       [4.9029093 ],\n",
       "       [4.9029093 ],\n",
       "       [4.902909  ],\n",
       "       [4.5662556 ],\n",
       "       [4.9029093 ],\n",
       "       [4.9029093 ],\n",
       "       [4.9029093 ],\n",
       "       [4.9029093 ],\n",
       "       [6.0588856 ],\n",
       "       [0.39238462],\n",
       "       [2.0342944 ],\n",
       "       [3.1644974 ],\n",
       "       [5.98522   ],\n",
       "       [2.2531211 ],\n",
       "       [2.3985188 ],\n",
       "       [5.79885   ],\n",
       "       [1.7698418 ],\n",
       "       [4.5538654 ],\n",
       "       [5.9452996 ],\n",
       "       [6.0302343 ],\n",
       "       [5.9859314 ],\n",
       "       [5.987988  ],\n",
       "       [4.5076113 ],\n",
       "       [5.304461  ],\n",
       "       [3.4699185 ],\n",
       "       [5.952699  ],\n",
       "       [2.739983  ],\n",
       "       [6.0397778 ],\n",
       "       [3.0891287 ],\n",
       "       [5.9204082 ],\n",
       "       [4.280511  ],\n",
       "       [6.1694584 ],\n",
       "       [4.851136  ],\n",
       "       [5.477144  ],\n",
       "       [6.0655847 ],\n",
       "       [2.8742118 ],\n",
       "       [5.4470406 ],\n",
       "       [2.6196845 ],\n",
       "       [4.5835867 ],\n",
       "       [5.502866  ],\n",
       "       [6.232828  ],\n",
       "       [2.5465221 ],\n",
       "       [2.169023  ],\n",
       "       [2.3487196 ],\n",
       "       [2.0109766 ],\n",
       "       [6.0977426 ],\n",
       "       [4.39121   ],\n",
       "       [5.7308097 ],\n",
       "       [5.6448226 ],\n",
       "       [3.999245  ],\n",
       "       [5.936075  ],\n",
       "       [4.7206717 ],\n",
       "       [5.798144  ],\n",
       "       [2.9344478 ],\n",
       "       [5.2610445 ],\n",
       "       [1.5066503 ],\n",
       "       [1.19489   ],\n",
       "       [1.924574  ],\n",
       "       [5.816991  ],\n",
       "       [2.859244  ],\n",
       "       [3.2085433 ],\n",
       "       [3.116165  ],\n",
       "       [4.6049595 ],\n",
       "       [5.9489317 ],\n",
       "       [2.9075947 ],\n",
       "       [4.8505845 ],\n",
       "       [2.4256067 ],\n",
       "       [4.0664415 ],\n",
       "       [5.0306015 ],\n",
       "       [2.2665737 ],\n",
       "       [2.6304479 ],\n",
       "       [5.927042  ],\n",
       "       [3.937287  ],\n",
       "       [1.1496238 ],\n",
       "       [4.6755123 ],\n",
       "       [2.3942828 ],\n",
       "       [0.26718777],\n",
       "       [2.3160744 ],\n",
       "       [3.5474987 ],\n",
       "       [2.7274296 ],\n",
       "       [3.1417394 ],\n",
       "       [3.2157645 ],\n",
       "       [4.390129  ],\n",
       "       [1.1207237 ],\n",
       "       [2.1038375 ],\n",
       "       [5.5552835 ],\n",
       "       [4.404747  ],\n",
       "       [3.8765645 ],\n",
       "       [1.9382364 ],\n",
       "       [2.2198594 ],\n",
       "       [5.724516  ],\n",
       "       [5.9259834 ],\n",
       "       [3.4862518 ],\n",
       "       [4.689637  ],\n",
       "       [2.6031709 ],\n",
       "       [5.073988  ],\n",
       "       [1.7204236 ],\n",
       "       [0.6821032 ],\n",
       "       [0.40168953],\n",
       "       [1.9855684 ],\n",
       "       [2.573043  ],\n",
       "       [4.1927366 ],\n",
       "       [5.719111  ],\n",
       "       [6.1905355 ],\n",
       "       [4.0788746 ],\n",
       "       [5.923992  ],\n",
       "       [5.5628266 ],\n",
       "       [6.0911245 ],\n",
       "       [6.0226917 ],\n",
       "       [5.8061647 ],\n",
       "       [0.59387386],\n",
       "       [2.913507  ],\n",
       "       [2.024464  ],\n",
       "       [2.9822936 ],\n",
       "       [3.428795  ],\n",
       "       [2.0664048 ],\n",
       "       [0.9353452 ],\n",
       "       [6.1056886 ],\n",
       "       [5.9681435 ],\n",
       "       [4.1017942 ],\n",
       "       [1.4795094 ],\n",
       "       [5.979818  ],\n",
       "       [2.4237437 ],\n",
       "       [3.010521  ],\n",
       "       [2.8543563 ],\n",
       "       [1.3462577 ],\n",
       "       [1.1146556 ],\n",
       "       [5.8962307 ],\n",
       "       [5.9201283 ]], dtype=float32)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.9249587"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(predito-y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(train_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.668086"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.mean(y_train)-y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-fdc61914d117>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist_train' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist_train.history['loss'])\n",
    "plt.plot(hist_train.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimativaConstrastivaRuidosa(keras.layers.Layer):\n",
    "    def __init__(self, init='glorot_uniform', comprimento=10,\n",
    "                 dimensao_entrada=None, vocabulario=None, ruidos = 25, distribuicao_ruidos=[0.5, 0.5], semente=19, **kwargs):\n",
    "        self.init = init\n",
    "        self.comprimento = comprimento\n",
    "        self.vocabulario = vocabulario\n",
    "        self.ruidos = ruidos\n",
    "        self.distribuicao_ruidos = theano.shared(np.array(distribuicao_ruidos).astype(theano.config.floatX)) #IMP\n",
    "        self.gerador_aleatorio = theano.tensor.shared_randomstreams.RandomStreams(seed=semente) #IMP\n",
    "        self.dimensao_entrada = dimensao_entrada\n",
    "        kwargs['input_shape'] = (self.dimensao_entrada, )\n",
    "        super(EstimativaConstrastivaRuidosa, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, dims_entrada):\n",
    "        self.W = self.add_weight(name='{}_W'.format(self.name), shape=(self.vocabulario, self.dimensao_entrada), initializer=self.init, trainable=True)\n",
    "        self.b = self.add_weight(name='{}_b'.format(self.name), shape=(self.vocabulario, )                     , initializer=self.init, trainable=True)\n",
    "        super(EstimativaConstrastivaRuidosa, self).build(dims_entrada)\n",
    "\n",
    "    def compute_output_shape(self, dims_entrada):\n",
    "        return (None, self.comprimento, self.ruidos + 1)\n",
    "    \n",
    "    def compute_mask(self, input, mask=None):\n",
    "        return mask[0]\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        contexto = inputs[0] #shape: amostras * passos * dim\n",
    "        proxima_palavra = inputs[1] #shape: amostras * passos\n",
    "\n",
    "        amostras, passos = proxima_palavra.shape\n",
    "        dim_saida = self.ruidos + 1\n",
    "\n",
    "        noise_w = self.gerador_aleatorio.choice(size=(amostras, passos, self.ruidos), a=self.distribuicao_ruidos.shape[0], p=self.distribuicao_ruidos)\n",
    "        proxima_palavra = proxima_palavra.flatten().reshape([amostras, passos, 1])\n",
    "        proxima_palavra = theano.tensor.concatenate([proxima_palavra, noise_w], axis=-1) #IMP shape: amostras * passos * dim_saida\n",
    "\n",
    "        W_ = self.W[proxima_palavra.flatten()].flatten().reshape([amostras, passos, dim_saida, self.dimensao_entrada])\n",
    "        b_ = self.b[proxima_palavra.flatten()].reshape([amostras, passos, dim_saida])\n",
    "\n",
    "        s_theta = (contexto[:, :, None, :] * W_).sum(axis=-1) + b_ # dims: amostras * passos * dim_saida\n",
    "        noiseP = self.distribuicao_ruidos[proxima_palavra.flatten()].reshape([amostras, passos, dim_saida])\n",
    "        noise_score = keras.backend.log(self.ruidos * noiseP) #log(k * distribuicao_ruidos(w))\n",
    "\n",
    "        return keras.activations.sigmoid(s_theta - noise_score) # dims: amostras, passos, dim_saida\n",
    "\n",
    "def custo_estimativa_contrastiva(real, estimado):\n",
    "    custo = K.binary_crossentropy(estimado, real[:, :, 1:])\n",
    "    custo = custo.sum(axis=-1)\n",
    "    custo *= real[:, :, 0]\n",
    "    return K.sum(custo) / K.sum(real[:, :, 0])\n",
    "\n",
    "def separar_xy_mascara(sequencias, vocabulario=5000, comprimento_maximo=100):\n",
    "    novas_sequencias = [[palavra if palavra < vocabulario else 0 for p in s] for s in sequencias]\n",
    "\n",
    "    comprimentos = [min(comprimento_maximo, len(s)-1) for s in sequencias]\n",
    "    comprimento_maximo = max(comprimentos)\n",
    "    amostras = numpy.count_nonzero(comprimentos)\n",
    "\n",
    "    x = numpy.zeros((amostras, comprimento_maximo)).astype('int32')\n",
    "    y = numpy.zeros((amostras, comprimento_maximo)).astype('int32')\n",
    "    mascara = numpy.zeros((amostras, comprimento_maximo)).astype('int32')\n",
    "\n",
    "    idx = 0\n",
    "    for i, s in enumerate(sequencias):\n",
    "        l = comprimentos[i]\n",
    "        if l < 1: continue\n",
    "        mask[idx, :l] = 1\n",
    "        x[idx, :l] = s[:l]\n",
    "        y[idx, :l] = s[1 : l+1]\n",
    "        x[idx] += mask[idx]\n",
    "        y[idx] += mask[idx]\n",
    "        idx += 1\n",
    "\n",
    "    return x, y, mascara\n",
    "\n",
    "def calcular_distancias(sequencias, indice_maximo):\n",
    "    Pn = np.zeros((maxword,))\n",
    "    for s in sequencias:\n",
    "        for w in s:\n",
    "            if w >= indice_maximo: Pn[0] += 1\n",
    "            else: Pn[w] += 1\n",
    "    Pn = 1.0 * Pn / sum(Pn)\n",
    "    return Pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_pretreinamento(vocabulario, dim_vetorial, comprimento, ruidos, distribuicao_ruidos):\n",
    "    entrada_atual = Input(shape=(comprimento,), dtype='int32', name='entrada_atual')\n",
    "    proxima_entrada = Input(shape=(comprimento,), dtype='int32', name='proxima_entrada')\n",
    "    vetorizado = Embedding(output_dim=dim_vetorial, input_dim=vocabulario, input_length=comprimento, mask_zero=True)(entrada_atual)\n",
    "    contexto_recorrente = LSTM(dim_vetorial, input_shape=(None, dim_vetorial), return_sequences=True)(vetorizado)\n",
    "    estimativa = EstimativaConstrastivaRuidosa(dimensao_entrada=dim_vetorial, comprimento=comprimento, vocabulario=vocabulario,\n",
    "                ruidos=ruidos, distribuicao_ruidos=distribuicao_ruidos)([contexto_recorrente, proxima_entrada])\n",
    "    return Model(inputs=[entrada_atual, proxima_entrada], outputs=estimativa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruidos = 100\n",
    "comprimento_maximo = 100\n",
    "dim_vetorial = 200\n",
    "vocabulario_maximo = 5000\n",
    "\n",
    "x_treino, y_treino, mascara_treino = separar_xy_mascara(treino, vocabulario_maximo, comprimento_maximo)\n",
    "\n",
    "vocabulario_maximo += 1\n",
    "amostras, comprimento = x_treino.shape\n",
    "Pn = calcular_distancias(treino, vocabulario_maximo)\n",
    "masc_treino = numpy.zeros((amostras, comprimento, ruidos + 2), dtype='int64')\n",
    "masc_treino[:, :, 0] = mascara_treino\n",
    "masc_treino[:, :, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelo_pretreino = modelo_pretreinamento(vocabulario_maximo, dim_vetorial, comprimento, ruidos, Pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_pretreino.compile(optimizer='adam', loss=custo_estimativa_contrastiva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  320/10970 [..............................] - ETA: 6:16 - loss: 16.7152"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-fab677c9a1cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistorico_pretreino\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_treino\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_treino\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasc_treino\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0mallow_gc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_gc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_gc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0m\u001b[1;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m    963\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "historico_pretreino = model.fit([x_treino, y_treino], masc_treino, batch_size=64, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "from theano import config\n",
    "import theano.tensor as tensor\n",
    "import theano.tensor.shared_randomstreams as RS\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.callbacks import *\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "class NCEContext(Layer):\n",
    "    def __init__(self, init='glorot_uniform', activation='linear',\n",
    "                 weights=None, input_dim=None, context_dim=None,\n",
    "                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=False, **kwargs):\n",
    "        self.init = initializers.get(init)\n",
    "        self.activation = activations.get(activation)\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.initial_weights = weights\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.context_dim = context_dim\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_dim,)\n",
    "\n",
    "        super(NCEContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape): #input shape: nsamples * n_context * dim\n",
    "        self.C = self.init((self.context_dim, self.input_dim),\n",
    "                           name='{}_C'.format(self.name))\n",
    "        self.trainable_weights = [self.C]\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], self.input_dim)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        #x shape: nsamples * n_context * dim\n",
    "        #out shape: nsamples * dim\n",
    "        out = self.C[None, :, :] * x\n",
    "        out = out.sum(axis=-2)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'init': self.init.__name__,\n",
    "                  'activation': self.activation.__name__,\n",
    "                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n",
    "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
    "                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
    "                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n",
    "                  'bias': self.bias,\n",
    "                  'input_dim': self.input_dim,\n",
    "                  'context_dim': self.context_dim}\n",
    "        base_config = super(NCEContext, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class NCE(Layer):\n",
    "    def __init__(self, init='glorot_uniform', activation='linear',\n",
    "                 input_dim=None, vocab_size=None, n_noise = 25, Pn=[0.5, 0.5],\n",
    "                 weights=None,\n",
    "                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.init = init #initializers.get(init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.input_dim = input_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_noise = n_noise\n",
    "        self.Pn = theano.shared(numpy.array(Pn).astype(config.floatX))\n",
    "        self.rng = RS.RandomStreams(seed=SEED)\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.initial_weights = weights\n",
    "\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_dim,)\n",
    "\n",
    "        super(NCE, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name='{}_W'.format(self.name),\n",
    "            shape=(self.vocab_size, self.input_dim),\n",
    "            initializer=self.init,\n",
    "        )\n",
    "        #self.init((self.vocab_size, self.input_dim), name='{}_W'.format(self.name))\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(\n",
    "                name='{}_b'.format(self.name),\n",
    "                shape=(self.vocab_size,),\n",
    "                initializer=self.init,\n",
    "            )\n",
    "            #self.init((self.vocab_size,), name='{}_b'.format(self.name))\n",
    "            self.trainable_weights = [self.W, self.b]\n",
    "        else:\n",
    "            self.trainable_weights = [self.W]\n",
    "\n",
    "        self.regularizers = []\n",
    "        if self.W_regularizer:\n",
    "            self.W_regularizer.set_param(self.W)\n",
    "            self.regularizers.append(self.W_regularizer)\n",
    "\n",
    "        if self.bias and self.b_regularizer:\n",
    "            self.b_regularizer.set_param(self.b)\n",
    "            self.regularizers.append(self.b_regularizer)\n",
    "\n",
    "        if self.activity_regularizer:\n",
    "            self.activity_regularizer.set_layer(self)\n",
    "            self.regularizers.append(self.activity_regularizer)\n",
    "\n",
    "        self.constraints = {}\n",
    "        if self.W_constraint:\n",
    "            self.constraints[self.W] = self.W_constraint\n",
    "\n",
    "        if self.bias and self.b_constraint:\n",
    "            self.constraints[self.b] = self.b_constraint\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        super(NCE, self).build(input_shape)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (None, self.n_noise + 1)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        context = inputs[0] #shape: n_samples * dim\n",
    "        next_w = inputs[1] #shape: n_samles * 1\n",
    "\n",
    "        n_samples = next_w.shape[0]\n",
    "        n_next = self.n_noise + 1\n",
    "\n",
    "        #generate n_noise samples from noise distribution Pn.\n",
    "        noise_w = self.rng.choice(size=(n_samples, self.n_noise), a=self.Pn.shape[0], p=self.Pn)\n",
    "        next_w = tensor.concatenate([next_w, noise_w], axis=-1)\n",
    "\n",
    "        W_ = self.W[next_w.flatten()].flatten().reshape([n_samples, n_next, self.input_dim])\n",
    "        b_ = self.b[next_w.flatten()].reshape([n_samples, n_next])\n",
    "\n",
    "        # compute s_theta(w): scores of words under the model\n",
    "        s_theta = (context[:, None, :] * W_).sum(axis=-1) + b_\n",
    "        # compute the scores of words under the noise distribution: log(k * Pn(w))\n",
    "        noiseP = self.Pn[next_w.flatten()].reshape([n_samples, n_next])\n",
    "        noise_score = K.log(self.n_noise * noiseP)\n",
    "\n",
    "        # the difference in the scores of words under the model and the noise distribution\n",
    "        # shape: n_samples * n_next\n",
    "        out = s_theta - noise_score\n",
    "\n",
    "        return activations.sigmoid(out)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'init': self.init,\n",
    "                  'activation': self.activation.__name__,\n",
    "                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n",
    "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
    "                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
    "                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n",
    "                  'bias': self.bias,\n",
    "                  'input_dim': self.input_dim,\n",
    "                  'vocab_size': self.vocab_size,\n",
    "                  'n_noise': self.n_noise\n",
    "                  }\n",
    "        base_config = super(NCE, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class NCE_seq(NCE):\n",
    "    def __init__(self, input_len=10, **kwargs):\n",
    "        self.input_len = input_len\n",
    "        super(NCE_seq, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, mask=None):\n",
    "        return mask[0]\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (None, self.input_len, self.n_noise + 1)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        context = inputs[0] #shape: n_samples * n_steps * dim\n",
    "        next_w = inputs[1] #shape: n_samles * n_steps\n",
    "\n",
    "        n_samples, n_steps = next_w.shape\n",
    "        n_next = self.n_noise + 1\n",
    "\n",
    "        #generate n_noise samples from noise distribution Pn.\n",
    "        noise_w = self.rng.choice(size=(n_samples, n_steps, self.n_noise), a=self.Pn.shape[0], p=self.Pn)\n",
    "        next_w = next_w.flatten().reshape([n_samples, n_steps, 1])\n",
    "        next_w = tensor.concatenate([next_w, noise_w], axis=-1) # shape: n_samples * n_steps * n_next\n",
    "\n",
    "        W_ = self.W[next_w.flatten()].flatten().reshape([n_samples, n_steps, n_next, self.input_dim])\n",
    "        b_ = self.b[next_w.flatten()].reshape([n_samples, n_steps, n_next])\n",
    "\n",
    "        # compute s_theta(w): scores of words under the model\n",
    "        # s_theta shape: n_samples * n_steps * n_next\n",
    "        s_theta = (context[:, :, None, :] * W_).sum(axis=-1) + b_\n",
    "        # compute the scores of words under the noise distribution: log(k * Pn(w))\n",
    "        noiseP = self.Pn[next_w.flatten()].reshape([n_samples, n_steps, n_next])\n",
    "        noise_score = K.log(self.n_noise * noiseP)\n",
    "\n",
    "        # the difference in the scores of words under the model and the noise distribution\n",
    "        # output shape: n_samples, n_steps, n_next\n",
    "        out = s_theta - noise_score\n",
    "\n",
    "        return activations.sigmoid(out)\n",
    "\n",
    "class NCETest(NCE):\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (None, 1)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        context = inputs[0] # shape: n_samples * dim\n",
    "        next_w = inputs[1] # shape: n_samples * 1\n",
    "        n_samples = next_w.shape[0]\n",
    "\n",
    "        out = K.dot(context, K.transpose(self.W)) + self.b\n",
    "        out = activations.softmax(out)\n",
    "        next_w = next_w.flatten()\n",
    "        return out[tensor.arange(n_samples), next_w]\n",
    "\n",
    "class NCETest_seq(NCETest):\n",
    "    def __init__(self, input_len=10, **kwargs):\n",
    "        self.input_len = input_len\n",
    "        super(NCETest_seq, self).__init__(**kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (None, self.input_len)\n",
    "\n",
    "    def compute_mask(self, input, mask=None):\n",
    "        return mask[0]\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        context = inputs[0] # shape: n_samples * n_steps * dim\n",
    "        next_w = inputs[1] # shape: n_samples * n_steps\n",
    "        n_samples, n_steps = next_w.shape\n",
    "        vocab_size = self.W.shape[0]\n",
    "\n",
    "        out = K.dot(context, K.transpose(self.W)) + self.b\n",
    "        out = activations.softmax(out)\n",
    "        out = out.flatten().reshape([n_samples*n_steps, vocab_size])\n",
    "        next_w = next_w.flatten()\n",
    "\n",
    "        prob = out[tensor.arange(n_samples*n_steps), next_w]\n",
    "        prob = prob.reshape([n_samples, n_steps])\n",
    "        return prob\n",
    "\n",
    "class NCETestCallback(Callback):\n",
    "    def __init__(self, data, testModel, fResult, fParams, patient=3):\n",
    "        self.testModel = testModel\n",
    "        self.fResult = fResult\n",
    "        self.fParams = fParams\n",
    "        self.patient = patient\n",
    "        self.num_patient = patient\n",
    "        self.best_epoch = 0\n",
    "        self.best_loss = 100000.0\n",
    "\n",
    "        self.do_test = False\n",
    "        if len(data) == 2 or len(data) == 4:\n",
    "            self.isSeq = 0\n",
    "            self.valid_x = data[0]\n",
    "            self.valid_y = data[1]\n",
    "            self.valid_mask = data[0]\n",
    "\n",
    "            if len(data) == 4:\n",
    "                self.do_test = True\n",
    "                self.test_x = data[2]\n",
    "                self.test_y = data[3]\n",
    "                self.test_mask = data[2]\n",
    "\n",
    "        else:\n",
    "            self.isSeq = 1\n",
    "            self.valid_x, self.valid_y, self.valid_mask = data[0], data[1], data[2]\n",
    "            if len(data) == 6:\n",
    "                self.do_test = True\n",
    "                self.test_x, self.test_y, self.test_mask = data[3], data[4], data[5]\n",
    "\n",
    "        flog = open(fResult, 'w')\n",
    "        flog.write('epoch\\ttr_loss\\tv_ppl\\n')\n",
    "        flog.close()\n",
    "        super(NCETestCallback, self).__init__()\n",
    "\n",
    "    def _compute_result(self, x, y, mask):\n",
    "        y_pred = self.testModel.predict([x, y], batch_size=30)\n",
    "        if self.isSeq:\n",
    "            per = perplexity(mask, y_pred, 1)\n",
    "        else:\n",
    "            per = perplexity(mask, y_pred, 0)\n",
    "        return per\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        weights = self.model.get_weights()\n",
    "        self.testModel.set_weights(weights)\n",
    "\n",
    "        v_per = self._compute_result(self.valid_x, self.valid_y, self.valid_mask)\n",
    "        if self.do_test:\n",
    "            t_per = self._compute_result(self.test_x, self.test_y, self.test_mask)\n",
    "\n",
    "        if self.best_loss < v_per:\n",
    "            self.patient -= 1\n",
    "\n",
    "            if self.patient == 0:\n",
    "                lr = self.model.optimizer.lr / 2.0\n",
    "                self.model.optimizer.lr = lr\n",
    "                self.patient = self.num_patient\n",
    "        else:\n",
    "            self.patient = self.num_patient\n",
    "            self.best_loss = v_per\n",
    "            self.best_epoch = epoch\n",
    "            self.model.save_weights(self.fParams, overwrite=True)\n",
    "\n",
    "        print ('validation perplexity: %.4f' % v_per)\n",
    "\n",
    "        train_loss = 0\n",
    "        if 'loss' in logs:\n",
    "            train_loss = logs['loss']\n",
    "\n",
    "        f = open(self.fResult, 'a')\n",
    "        f.write('%d\\t%.4f\\t%.4f' % (epoch, train_loss, v_per))\n",
    "        if self.do_test:\n",
    "            f.write('\\t%.4f' % t_per)\n",
    "        f.write('\\tBest at epoch %d' % self.best_epoch)\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "\n",
    "def NCE_seq_loss(y_true, y_pred):\n",
    "    # y_true[:, :, 0]: masking matrix\n",
    "    # y_true[:, :, 1] = 1: words from data\n",
    "    # y_true[:, :, 2:] = 0: words from noise distribution\n",
    "    # y_pred: probability of the word to be from data - shape: n_samples * n_steps * (n_noise + 1)\n",
    "\n",
    "    loss = K.binary_crossentropy(y_pred, y_true[:, :, 1:])\n",
    "    loss = loss.sum(axis=-1)\n",
    "    loss *= y_true[:, :, 0] # masking matrix\n",
    "    return K.sum(loss) / K.sum(y_true[:, :, 0])\n",
    "\n",
    "def NCE_seq_loss_test(y_true, y_pred):\n",
    "    # y_pred: n_samples * n_steps - probability of next word to be the corresponding word in y_true\n",
    "    # y_true: masking matrix\n",
    "\n",
    "    loss = -tensor.log(y_pred)\n",
    "    loss *= y_true\n",
    "    loss = K.sum(loss) / K.sum(y_true)\n",
    "    return K.exp(loss)\n",
    "\n",
    "def NCE_loss(y_true, y_pred): #n_samples * n_next\n",
    "    loss = K.binary_crossentropy(y_pred, y_true)\n",
    "    loss = K.mean(loss.sum(axis=-1))\n",
    "    return loss\n",
    "\n",
    "def NCE_loss_test(y_true, y_pred): #(n_samples,)\n",
    "    loss = - tensor.log(y_pred)\n",
    "    loss = K.mean(loss)\n",
    "    loss = K.exp(loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def perplexity(y_true, y_pred, isSeq = 0): #(n_samples,) or (n_samples, n_steps)\n",
    "    eps = 1e-4\n",
    "    loss = - numpy.log(y_pred + eps)\n",
    "\n",
    "    if isSeq: # sequence\n",
    "        loss *= y_true\n",
    "        loss = numpy.sum(loss) / numpy.sum(y_true)\n",
    "    else:\n",
    "        loss = numpy.mean(loss)\n",
    "\n",
    "    loss = numpy.exp(loss)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def calc_dist(seqs, maxword):\n",
    "    Pn = numpy.zeros((maxword,))\n",
    "\n",
    "    for s in seqs:\n",
    "        for w in s:\n",
    "            if w >= maxword: Pn[0] += 1\n",
    "            else: Pn[w] += 1\n",
    "\n",
    "    Pn = 1.0 * Pn / sum(Pn)\n",
    "\n",
    "    return Pn\n",
    "\n",
    "def generate_noise(n_samples, n_noise, Pn):\n",
    "    noise = numpy.zeros((n_samples, n_noise), dtype='int64')\n",
    "    for i in range(n_samples):\n",
    "        noise[i] = numpy.random.choice(len(Pn), n_noise, p=Pn)\n",
    "\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "def load(path):\n",
    "    f = gzip.open(path, 'rb')\n",
    "    train, valid, test = pickle.load(f)\n",
    "    #print path, len(train[0]), len(valid[0])\n",
    "\n",
    "    return train, valid, test\n",
    "\n",
    "def prepare_lm(seqs, vocab_size=10000, max_len=100):\n",
    "    new_seqs = []\n",
    "    for i, s in enumerate(seqs):\n",
    "        new_s = [w if w < vocab_size else 0 for w in s]\n",
    "        new_seqs.append(new_s)\n",
    "    seqs = new_seqs\n",
    "\n",
    "    lengths = [min(max_len, len(s)-1) for s in seqs]\n",
    "    maxlen = max(lengths)\n",
    "    n_samples = numpy.count_nonzero(lengths)\n",
    "\n",
    "    x = numpy.zeros((n_samples, maxlen)).astype('int64')\n",
    "    y = numpy.zeros((n_samples, maxlen)).astype('int64')\n",
    "    mask = numpy.zeros((n_samples, maxlen)).astype('int64')\n",
    "\n",
    "    idx = 0\n",
    "    for i, s in enumerate(seqs):\n",
    "        l = lengths[i]\n",
    "        if l < 1: continue\n",
    "        mask[idx, :l] = 1\n",
    "        x[idx, :l] = s[:l]\n",
    "        y[idx, :l] = s[1 : l+1]\n",
    "        x[idx] += mask[idx]\n",
    "        y[idx] += mask[idx]\n",
    "        idx += 1\n",
    "\n",
    "    return x, y, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "vocab:  5000\n",
      "Data size: Train: 67794, valid: 4869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/theano/gpuarray/dnn.py:184: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to a version >= v5 and <= v7.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_inp (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 200)     1000200     main_inp[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 100, 200)     320800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "next_inp (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "nce_seq_1 (NCE_seq)             [(None, 100, 200), ( 1005201     lstm_1[0][0]                     \n",
      "                                                                 next_inp[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,326,201\n",
      "Trainable params: 2,326,201\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      " 2350/67794 [>.............................] - ETA: 49:17 - loss: 66.8555"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fa981655e786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m his = model.fit([train_x, train_y], labels,\n\u001b[0;32m---> 44\u001b[0;31m           batch_size=50, epochs=20)\n\u001b[0m",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.constraints import *\n",
    "from keras.regularizers import *\n",
    "import numpy\n",
    "\n",
    "dataset = 'dados/pretreino/moodle_pretrain.pkl.gz'\n",
    "emb_dim = 200\n",
    "max_len = 100\n",
    "\n",
    "n_noise = 100\n",
    "print ('Loading data...')\n",
    "train, valid, test = load(dataset)\n",
    "valid = valid[-5000:]\n",
    "vocab_size = 5000\n",
    "\n",
    "print ('vocab: ', vocab_size)\n",
    "\n",
    "train_x, train_y, train_mask = prepare_lm(train, vocab_size, max_len)\n",
    "valid_x, valid_y, valid_mask = prepare_lm(valid, vocab_size, max_len)\n",
    "\n",
    "print ('Data size: Train: %d, valid: %d' % (len(train_x), len(valid_x)))\n",
    "\n",
    "vocab_size += 1\n",
    "n_samples, inp_len = train_x.shape\n",
    "Pn = calc_dist(train, vocab_size)\n",
    "labels = numpy.zeros((n_samples, inp_len, n_noise + 2), dtype='int64')\n",
    "labels[:, :, 0] = train_mask\n",
    "labels[:, :, 1] = 1\n",
    "\n",
    "main_inp = Input(shape=(inp_len,), dtype='int64', name='main_inp')\n",
    "next_inp = Input(shape=(inp_len,), dtype='int64', name='next_inp')\n",
    "emb_vec = Embedding(output_dim=emb_dim, input_dim=vocab_size, input_length=inp_len,\n",
    "                    #dropout=0.2,\n",
    "                    mask_zero=True)(main_inp)\n",
    "GRU_context = LSTM(emb_dim, input_shape=(None, emb_dim), return_sequences=True)(emb_vec)\n",
    "nce_out = NCE_seq(input_dim=emb_dim, input_len=inp_len, vocab_size=vocab_size, n_noise=n_noise, Pn=Pn,\n",
    "              )([GRU_context, next_inp])\n",
    "model = Model(inputs=[main_inp, next_inp], outputs=[nce_out])\n",
    "optimizer = RMSprop(lr=0.02, rho=0.99, epsilon=1e-7) #optimizer = RMSprop(lr=0.01)\n",
    "model.compile(optimizer=optimizer, loss=NCE_seq_loss)\n",
    "print(model.summary())\n",
    "his = model.fit([train_x, train_y], labels,\n",
    "          batch_size=50, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_treinamento(n_classes, vocab_size, inp_len, emb_dim,\n",
    "                 seq_model='lstm', nnet_model='highway', pool_mode='mean',\n",
    "                 dropout_inp=False, dropout_hid=True, emb_weight=None, hidden_layer=None):\n",
    "    if emb_weight is not None:\n",
    "        emb_weight = [emb_weight[:vocab_size]]\n",
    "    seq_dict = {'lstm': LSTM, 'gru': GRU, 'rnn': SimpleRNN}\n",
    "    nnet_dict = {'highway': create_highway, 'dense': create_dense}\n",
    "    if n_classes == -1:\n",
    "        top_act = 'linear'\n",
    "    elif n_classes == 1:\n",
    "        top_act = 'sigmoid'\n",
    "    else:\n",
    "        top_act = 'softmax'\n",
    "\n",
    "    title_inp = Input(shape=(inp_len,), dtype='int64', name='title_inp')\n",
    "    descr_inp = Input(shape=(inp_len,), dtype='int64', name='descr_inp')\n",
    "\n",
    "    title_mask = Input(shape=(inp_len,), dtype='float32', name='title_mask')\n",
    "    descr_mask = Input(shape=(inp_len,), dtype='float32', name='descr_mask')\n",
    "\n",
    "    if dropout_inp:\n",
    "        drop_rate = 0.2\n",
    "    else:\n",
    "        drop_rate = 0.0\n",
    "\n",
    "    embedding = Embedding(output_dim=emb_dim, input_dim=vocab_size, input_length=inp_len,\n",
    "                          mask_zero=True, weights=emb_weight,\n",
    "                          dropout=drop_rate)\n",
    "    seq_layer = seq_dict[seq_model](input_dim=emb_dim, output_dim=emb_dim,\n",
    "                                    return_sequences=True, dropout_U=drop_rate, dropout_W=drop_rate)\n",
    "\n",
    "    title_emb = embedding(title_inp)\n",
    "    descr_emb = embedding(descr_inp)\n",
    "\n",
    "    title_hid = seq_layer(title_emb)\n",
    "    descr_hid = seq_layer(descr_emb)\n",
    "\n",
    "    pooled_title = PoolingSeq(mode=pool_mode)([title_hid, title_mask])\n",
    "    pooled_descr = PoolingSeq(mode=pool_mode)([descr_hid, descr_mask])\n",
    "\n",
    "    hidd = Average()([pooled_title, pooled_descr])\n",
    "    if dropout_hid:\n",
    "        hidd = Dropout(0.5)(hidd)\n",
    "\n",
    "    hidd = nnet_dict[nnet_model](hidd, emb_dim, hidden_layer)\n",
    "    hidd = Dropout(0.5)(hidd)\n",
    "    top_hidd = Dense(output_dim=abs(n_classes), activation=top_act)(hidd)\n",
    "\n",
    "    model = Model(input=[title_inp, title_mask, descr_inp, descr_mask], output=top_hidd)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Average train sequence length: 238\n",
      "Average test sequence length: 230\n",
      "Adding 2-gram features\n",
      "Average train sequence length: 476\n",
      "Average test sequence length: 428\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.5862 - acc: 0.7874 - val_loss: 0.4372 - val_acc: 0.8548\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 34s 1ms/step - loss: 0.2877 - acc: 0.9276 - val_loss: 0.3030 - val_acc: 0.8904\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 34s 1ms/step - loss: 0.1432 - acc: 0.9696 - val_loss: 0.2628 - val_acc: 0.8998\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 34s 1ms/step - loss: 0.0775 - acc: 0.9871 - val_loss: 0.2441 - val_acc: 0.9030\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 34s 1ms/step - loss: 0.0437 - acc: 0.9950 - val_loss: 0.2376 - val_acc: 0.9040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa688dd99e8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#This example demonstrates the use of fasttext for text classification\n",
    "Based on Joulin et al's paper:\n",
    "[Bags of Tricks for Efficient Text Classification\n",
    "](https://arxiv.org/abs/1607.01759)\n",
    "Results on IMDB datasets with uni and bi-gram embeddings:\n",
    "Embedding|Accuracy, 5 epochs|Speed (s/epoch)|Hardware\n",
    ":--------|-----------------:|----:|:-------\n",
    "Uni-gram |            0.8813|    8|i7 CPU\n",
    "Bi-gram  |            0.9056|    2|GTx 980M GPU\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 2\n",
    "max_features = 20000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "# we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "# of all words in the document\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'numpy' from '/usr/lib64/python3.6/site-packages/numpy/__init__.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n",
      "Processing text dataset\n",
      "Found 19997 texts.\n",
      "Found 174074 unique tokens.\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 20)\n",
      "Preparing embedding matrix.\n",
      "Training model.\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/10\n",
      "15998/15998 [==============================] - 5s 340us/step - loss: 2.4037 - acc: 0.2187 - val_loss: 1.8067 - val_acc: 0.3768\n",
      "Epoch 2/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 1.5519 - acc: 0.4621 - val_loss: 1.4603 - val_acc: 0.5064\n",
      "Epoch 3/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 1.1902 - acc: 0.5953 - val_loss: 1.1682 - val_acc: 0.5969\n",
      "Epoch 4/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.9690 - acc: 0.6748 - val_loss: 1.0070 - val_acc: 0.6624\n",
      "Epoch 5/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.8137 - acc: 0.7250 - val_loss: 0.9541 - val_acc: 0.6784\n",
      "Epoch 6/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.6990 - acc: 0.7604 - val_loss: 1.0010 - val_acc: 0.6557\n",
      "Epoch 7/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.5930 - acc: 0.7985 - val_loss: 0.9482 - val_acc: 0.6899\n",
      "Epoch 8/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.5045 - acc: 0.8270 - val_loss: 0.8498 - val_acc: 0.7367\n",
      "Epoch 9/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.4280 - acc: 0.8572 - val_loss: 0.8620 - val_acc: 0.7339\n",
      "Epoch 10/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.3622 - acc: 0.8782 - val_loss: 0.9350 - val_acc: 0.7302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7699e01080>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This script loads pre-trained word embeddings (GloVe embeddings)\n",
    "into a frozen Keras Embedding layer, and uses it to\n",
    "train a text classification model on the 20 Newsgroup dataset\n",
    "(classification of newsgroup messages into 20 different categories).\n",
    "GloVe embedding data can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/)\n",
    "20 Newsgroup data can be found at:\n",
    "http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove_en')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "8982 train sequences\n",
      "2246 test sequences\n",
      "46 classes\n",
      "Vectorizing sequence data...\n",
      "x_train shape: (8982, 1000)\n",
      "x_test shape: (2246, 1000)\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n",
      "Building model...\n",
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 1.5670 - acc: 0.6538 - val_loss: 1.1814 - val_acc: 0.7442\n",
      "Epoch 2/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.8898 - acc: 0.7982 - val_loss: 0.9999 - val_acc: 0.7798\n",
      "Epoch 3/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.6416 - acc: 0.8458 - val_loss: 0.8985 - val_acc: 0.7875\n",
      "Epoch 4/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.4929 - acc: 0.8820 - val_loss: 0.8817 - val_acc: 0.8031\n",
      "Epoch 5/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.3945 - acc: 0.9042 - val_loss: 0.8789 - val_acc: 0.7976\n",
      "Epoch 6/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.3191 - acc: 0.9209 - val_loss: 0.8845 - val_acc: 0.8131\n",
      "Epoch 7/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.2819 - acc: 0.9289 - val_loss: 0.8867 - val_acc: 0.8120\n",
      "Epoch 8/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.2442 - acc: 0.9398 - val_loss: 0.9005 - val_acc: 0.8076\n",
      "Epoch 9/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.2170 - acc: 0.9433 - val_loss: 0.9392 - val_acc: 0.7976\n",
      "Epoch 10/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1934 - acc: 0.9482 - val_loss: 0.9641 - val_acc: 0.7898\n",
      "Epoch 11/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1861 - acc: 0.9469 - val_loss: 0.9742 - val_acc: 0.7998\n",
      "Epoch 12/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1678 - acc: 0.9529 - val_loss: 1.0137 - val_acc: 0.7931\n",
      "Epoch 13/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1673 - acc: 0.9526 - val_loss: 1.0068 - val_acc: 0.7831\n",
      "Epoch 14/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1584 - acc: 0.9551 - val_loss: 1.0321 - val_acc: 0.7909\n",
      "Epoch 15/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1542 - acc: 0.9547 - val_loss: 1.0219 - val_acc: 0.7909\n",
      "Epoch 16/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1503 - acc: 0.9545 - val_loss: 1.0426 - val_acc: 0.7853\n",
      "Epoch 17/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1451 - acc: 0.9568 - val_loss: 1.0406 - val_acc: 0.7831\n",
      "Epoch 18/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1369 - acc: 0.9588 - val_loss: 1.0810 - val_acc: 0.7887\n",
      "Epoch 19/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1308 - acc: 0.9620 - val_loss: 1.1056 - val_acc: 0.7853\n",
      "Epoch 20/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1256 - acc: 0.9594 - val_loss: 1.0935 - val_acc: 0.7820\n",
      "Epoch 21/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1311 - acc: 0.9592 - val_loss: 1.1259 - val_acc: 0.7875\n",
      "Epoch 22/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1273 - acc: 0.9602 - val_loss: 1.1245 - val_acc: 0.7931\n",
      "Epoch 23/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1320 - acc: 0.9604 - val_loss: 1.1163 - val_acc: 0.7920\n",
      "Epoch 24/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1326 - acc: 0.9577 - val_loss: 1.0951 - val_acc: 0.7998\n",
      "Epoch 25/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1266 - acc: 0.9587 - val_loss: 1.1417 - val_acc: 0.7853\n",
      "Epoch 26/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1225 - acc: 0.9592 - val_loss: 1.1342 - val_acc: 0.7942\n",
      "Epoch 27/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1226 - acc: 0.9604 - val_loss: 1.1586 - val_acc: 0.7887\n",
      "Epoch 28/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1187 - acc: 0.9616 - val_loss: 1.1299 - val_acc: 0.7987\n",
      "Epoch 29/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1217 - acc: 0.9604 - val_loss: 1.1817 - val_acc: 0.7875\n",
      "Epoch 30/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1275 - acc: 0.9587 - val_loss: 1.1431 - val_acc: 0.7998\n",
      "Epoch 31/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1167 - acc: 0.9620 - val_loss: 1.1619 - val_acc: 0.7976\n",
      "Epoch 32/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1115 - acc: 0.9626 - val_loss: 1.1774 - val_acc: 0.7909\n",
      "Epoch 33/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1256 - acc: 0.9584 - val_loss: 1.1786 - val_acc: 0.7898\n",
      "Epoch 34/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1200 - acc: 0.9609 - val_loss: 1.1845 - val_acc: 0.7887\n",
      "Epoch 35/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1198 - acc: 0.9619 - val_loss: 1.2025 - val_acc: 0.7998\n",
      "Epoch 36/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1187 - acc: 0.9621 - val_loss: 1.2237 - val_acc: 0.7731\n",
      "Epoch 37/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1199 - acc: 0.9607 - val_loss: 1.2167 - val_acc: 0.7909\n",
      "Epoch 38/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1150 - acc: 0.9620 - val_loss: 1.1866 - val_acc: 0.7998\n",
      "Epoch 39/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1161 - acc: 0.9607 - val_loss: 1.2133 - val_acc: 0.7898\n",
      "Epoch 40/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1220 - acc: 0.9595 - val_loss: 1.2217 - val_acc: 0.7853\n",
      "Epoch 41/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1141 - acc: 0.9631 - val_loss: 1.2411 - val_acc: 0.7887\n",
      "Epoch 42/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1113 - acc: 0.9610 - val_loss: 1.2606 - val_acc: 0.7853\n",
      "Epoch 43/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1102 - acc: 0.9628 - val_loss: 1.2209 - val_acc: 0.7875\n",
      "Epoch 44/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1075 - acc: 0.9629 - val_loss: 1.2654 - val_acc: 0.7853\n",
      "Epoch 45/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1139 - acc: 0.9614 - val_loss: 1.2527 - val_acc: 0.7875\n",
      "Epoch 46/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1122 - acc: 0.9608 - val_loss: 1.2485 - val_acc: 0.7820\n",
      "Epoch 47/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1092 - acc: 0.9630 - val_loss: 1.2679 - val_acc: 0.7864\n",
      "Epoch 48/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1096 - acc: 0.9623 - val_loss: 1.2907 - val_acc: 0.7887\n",
      "Epoch 49/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1086 - acc: 0.9624 - val_loss: 1.2607 - val_acc: 0.7775\n",
      "Epoch 50/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1048 - acc: 0.9634 - val_loss: 1.2892 - val_acc: 0.7831\n",
      "2246/2246 [==============================] - 0s 12us/step\n",
      "Test score: 1.2998778688408705\n",
      "Test accuracy: 0.7831700801424755\n"
     ]
    }
   ],
   "source": [
    "'''Trains and evaluate a simple MLP\n",
    "on the Reuters newswire topic classification task.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 1000\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n",
      "Vectorization...\n",
      "Training Data:\n",
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "Validation Data:\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n",
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 4, 12)             1548      \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 102us/step - loss: 1.8918 - acc: 0.3203 - val_loss: 1.7945 - val_acc: 0.3375\n",
      "Q 489+65  T 554  \u001b[91m☒\u001b[0m Q 685+99  T 784  \u001b[91m☒\u001b[0m Q 443+430 T 873  \u001b[91m☒\u001b[0m Q 81+934  T 1015 \u001b[91m☒\u001b[0m Q 423+374 T 797  \u001b[91m☒\u001b[0m Q 98+60   T 158  \u001b[91m☒\u001b[0m Q 526+89  T 615  \u001b[91m☒\u001b[0m Q 746+0   T 746  \u001b[91m☒\u001b[0m Q 10+444  T 454  \u001b[91m☒\u001b[0m Q 755+13  T 768  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 1.7337 - acc: 0.3593 - val_loss: 1.6606 - val_acc: 0.3797\n",
      "Q 200+78  T 278  \u001b[91m☒\u001b[0m Q 341+254 T 595  \u001b[91m☒\u001b[0m Q 22+99   T 121  \u001b[91m☒\u001b[0m Q 147+88  T 235  \u001b[91m☒\u001b[0m Q 891+246 T 1137 \u001b[91m☒\u001b[0m Q 484+570 T 1054 \u001b[91m☒\u001b[0m Q 1+453   T 454  \u001b[91m☒\u001b[0m Q 58+481  T 539  \u001b[91m☒\u001b[0m Q 553+60  T 613  \u001b[91m☒\u001b[0m Q 9+148   T 157  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 1.5852 - acc: 0.4074 - val_loss: 1.4965 - val_acc: 0.4399\n",
      "Q 959+13  T 972  \u001b[91m☒\u001b[0m Q 850+81  T 931  \u001b[91m☒\u001b[0m Q 817+17  T 834  \u001b[91m☒\u001b[0m Q 32+37   T 69   \u001b[91m☒\u001b[0m Q 7+564   T 571  \u001b[91m☒\u001b[0m Q 917+306 T 1223 \u001b[91m☒\u001b[0m Q 927+533 T 1460 \u001b[91m☒\u001b[0m Q 81+68   T 149  \u001b[91m☒\u001b[0m Q 893+945 T 1838 \u001b[91m☒\u001b[0m Q 877+87  T 964  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 1.3974 - acc: 0.4776 - val_loss: 1.3116 - val_acc: 0.5147\n",
      "Q 207+99  T 306  \u001b[91m☒\u001b[0m Q 66+8    T 74   \u001b[91m☒\u001b[0m Q 71+274  T 345  \u001b[91m☒\u001b[0m Q 10+736  T 746  \u001b[91m☒\u001b[0m Q 5+385   T 390  \u001b[91m☒\u001b[0m Q 401+708 T 1109 \u001b[91m☒\u001b[0m Q 600+709 T 1309 \u001b[91m☒\u001b[0m Q 458+85  T 543  \u001b[91m☒\u001b[0m Q 187+3   T 190  \u001b[91m☒\u001b[0m Q 82+870  T 952  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 1.2297 - acc: 0.5472 - val_loss: 1.1732 - val_acc: 0.5686\n",
      "Q 978+75  T 1053 \u001b[91m☒\u001b[0m Q 208+65  T 273  \u001b[91m☒\u001b[0m Q 699+670 T 1369 \u001b[91m☒\u001b[0m Q 99+182  T 281  \u001b[91m☒\u001b[0m Q 42+379  T 421  \u001b[91m☒\u001b[0m Q 622+66  T 688  \u001b[91m☒\u001b[0m Q 70+987  T 1057 \u001b[91m☒\u001b[0m Q 603+610 T 1213 \u001b[91m☒\u001b[0m Q 820+937 T 1757 \u001b[91m☒\u001b[0m Q 393+36  T 429  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 1.0848 - acc: 0.6054 - val_loss: 1.0308 - val_acc: 0.6301\n",
      "Q 153+367 T 520  \u001b[91m☒\u001b[0m Q 40+514  T 554  \u001b[91m☒\u001b[0m Q 50+35   T 85   \u001b[91m☒\u001b[0m Q 224+297 T 521  \u001b[91m☒\u001b[0m Q 850+411 T 1261 \u001b[91m☒\u001b[0m Q 56+13   T 69   \u001b[91m☒\u001b[0m Q 352+141 T 493  \u001b[91m☒\u001b[0m Q 79+25   T 104  \u001b[91m☒\u001b[0m Q 420+80  T 500  \u001b[91m☒\u001b[0m Q 895+468 T 1363 \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.9858 - acc: 0.6466 - val_loss: 0.9636 - val_acc: 0.6456\n",
      "Q 931+36  T 967  \u001b[92m☑\u001b[0m Q 665+755 T 1420 \u001b[91m☒\u001b[0m Q 806+4   T 810  \u001b[91m☒\u001b[0m Q 56+21   T 77   \u001b[91m☒\u001b[0m Q 592+718 T 1310 \u001b[91m☒\u001b[0m Q 118+914 T 1032 \u001b[91m☒\u001b[0m Q 214+8   T 222  \u001b[92m☑\u001b[0m Q 687+8   T 695  \u001b[91m☒\u001b[0m Q 149+18  T 167  \u001b[91m☒\u001b[0m Q 469+605 T 1074 \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.9069 - acc: 0.6766 - val_loss: 0.8838 - val_acc: 0.6861\n",
      "Q 180+425 T 605  \u001b[91m☒\u001b[0m Q 368+306 T 674  \u001b[91m☒\u001b[0m Q 348+21  T 369  \u001b[91m☒\u001b[0m Q 320+88  T 408  \u001b[91m☒\u001b[0m Q 3+401   T 404  \u001b[91m☒\u001b[0m Q 35+946  T 981  \u001b[91m☒\u001b[0m Q 414+820 T 1234 \u001b[91m☒\u001b[0m Q 0+891   T 891  \u001b[91m☒\u001b[0m Q 68+59   T 127  \u001b[91m☒\u001b[0m Q 21+323  T 344  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.8333 - acc: 0.7034 - val_loss: 0.8017 - val_acc: 0.7172\n",
      "Q 2+419   T 421  \u001b[92m☑\u001b[0m Q 108+238 T 346  \u001b[91m☒\u001b[0m Q 627+668 T 1295 \u001b[91m☒\u001b[0m Q 649+27  T 676  \u001b[91m☒\u001b[0m Q 99+411  T 510  \u001b[91m☒\u001b[0m Q 586+73  T 659  \u001b[91m☒\u001b[0m Q 638+65  T 703  \u001b[91m☒\u001b[0m Q 393+39  T 432  \u001b[91m☒\u001b[0m Q 221+642 T 863  \u001b[91m☒\u001b[0m Q 605+394 T 999  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.7649 - acc: 0.7300 - val_loss: 0.7401 - val_acc: 0.7435\n",
      "Q 96+968  T 1064 \u001b[91m☒\u001b[0m Q 96+100  T 196  \u001b[91m☒\u001b[0m Q 97+31   T 128  \u001b[92m☑\u001b[0m Q 96+80   T 176  \u001b[91m☒\u001b[0m Q 874+1   T 875  \u001b[92m☑\u001b[0m Q 356+171 T 527  \u001b[91m☒\u001b[0m Q 448+0   T 448  \u001b[92m☑\u001b[0m Q 457+586 T 1043 \u001b[91m☒\u001b[0m Q 939+911 T 1850 \u001b[91m☒\u001b[0m Q 369+61  T 430  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.7075 - acc: 0.7520 - val_loss: 0.7053 - val_acc: 0.7475\n",
      "Q 513+48  T 561  \u001b[92m☑\u001b[0m Q 29+70   T 99   \u001b[91m☒\u001b[0m Q 373+3   T 376  \u001b[92m☑\u001b[0m Q 88+69   T 157  \u001b[91m☒\u001b[0m Q 266+13  T 279  \u001b[91m☒\u001b[0m Q 28+94   T 122  \u001b[91m☒\u001b[0m Q 291+61  T 352  \u001b[92m☑\u001b[0m Q 842+21  T 863  \u001b[91m☒\u001b[0m Q 994+5   T 999  \u001b[91m☒\u001b[0m Q 349+21  T 370  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.6411 - acc: 0.7765 - val_loss: 0.5972 - val_acc: 0.7941\n",
      "Q 130+89  T 219  \u001b[92m☑\u001b[0m Q 9+298   T 307  \u001b[92m☑\u001b[0m Q 35+675  T 710  \u001b[92m☑\u001b[0m Q 77+796  T 873  \u001b[92m☑\u001b[0m Q 779+416 T 1195 \u001b[91m☒\u001b[0m Q 907+1   T 908  \u001b[92m☑\u001b[0m Q 779+416 T 1195 \u001b[91m☒\u001b[0m Q 34+96   T 130  \u001b[91m☒\u001b[0m Q 461+301 T 762  \u001b[91m☒\u001b[0m Q 531+3   T 534  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.5110 - acc: 0.8221 - val_loss: 0.4356 - val_acc: 0.8536\n",
      "Q 304+1   T 305  \u001b[91m☒\u001b[0m Q 172+301 T 473  \u001b[91m☒\u001b[0m Q 19+199  T 218  \u001b[91m☒\u001b[0m Q 583+26  T 609  \u001b[92m☑\u001b[0m Q 235+339 T 574  \u001b[91m☒\u001b[0m Q 6+142   T 148  \u001b[91m☒\u001b[0m Q 193+127 T 320  \u001b[91m☒\u001b[0m Q 29+293  T 322  \u001b[91m☒\u001b[0m Q 902+153 T 1055 \u001b[91m☒\u001b[0m Q 14+388  T 402  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.3637 - acc: 0.8866 - val_loss: 0.3101 - val_acc: 0.9121\n",
      "Q 75+315  T 390  \u001b[92m☑\u001b[0m Q 2+114   T 116  \u001b[92m☑\u001b[0m Q 750+244 T 994  \u001b[92m☑\u001b[0m Q 53+135  T 188  \u001b[92m☑\u001b[0m Q 39+590  T 629  \u001b[92m☑\u001b[0m Q 193+85  T 278  \u001b[92m☑\u001b[0m Q 712+87  T 799  \u001b[92m☑\u001b[0m Q 370+228 T 598  \u001b[92m☑\u001b[0m Q 453+37  T 490  \u001b[92m☑\u001b[0m Q 761+7   T 768  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.2601 - acc: 0.9329 - val_loss: 0.2259 - val_acc: 0.9476\n",
      "Q 31+209  T 240  \u001b[92m☑\u001b[0m Q 800+952 T 1752 \u001b[91m☒\u001b[0m Q 795+24  T 819  \u001b[92m☑\u001b[0m Q 674+668 T 1342 \u001b[92m☑\u001b[0m Q 63+85   T 148  \u001b[92m☑\u001b[0m Q 834+409 T 1243 \u001b[92m☑\u001b[0m Q 292+700 T 992  \u001b[92m☑\u001b[0m Q 8+629   T 637  \u001b[92m☑\u001b[0m Q 764+282 T 1046 \u001b[92m☑\u001b[0m Q 85+187  T 272  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.1878 - acc: 0.9603 - val_loss: 0.1780 - val_acc: 0.9577\n",
      "Q 773+511 T 1284 \u001b[92m☑\u001b[0m Q 336+527 T 863  \u001b[92m☑\u001b[0m Q 755+30  T 785  \u001b[92m☑\u001b[0m Q 32+724  T 756  \u001b[92m☑\u001b[0m Q 773+4   T 777  \u001b[92m☑\u001b[0m Q 2+42    T 44   \u001b[92m☑\u001b[0m Q 998+21  T 1019 \u001b[92m☑\u001b[0m Q 698+676 T 1374 \u001b[91m☒\u001b[0m Q 238+80  T 318  \u001b[92m☑\u001b[0m Q 73+60   T 133  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.1460 - acc: 0.9711 - val_loss: 0.1520 - val_acc: 0.9634\n",
      "Q 60+442  T 502  \u001b[92m☑\u001b[0m Q 50+563  T 613  \u001b[92m☑\u001b[0m Q 707+782 T 1489 \u001b[92m☑\u001b[0m Q 30+233  T 263  \u001b[92m☑\u001b[0m Q 48+385  T 433  \u001b[92m☑\u001b[0m Q 243+880 T 1123 \u001b[92m☑\u001b[0m Q 85+16   T 101  \u001b[92m☑\u001b[0m Q 575+76  T 651  \u001b[92m☑\u001b[0m Q 265+0   T 265  \u001b[92m☑\u001b[0m Q 46+464  T 510  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.1072 - acc: 0.9824 - val_loss: 0.1039 - val_acc: 0.9810\n",
      "Q 792+810 T 1602 \u001b[92m☑\u001b[0m Q 81+200  T 281  \u001b[92m☑\u001b[0m Q 978+66  T 1044 \u001b[92m☑\u001b[0m Q 3+983   T 986  \u001b[92m☑\u001b[0m Q 624+76  T 700  \u001b[91m☒\u001b[0m Q 91+354  T 445  \u001b[92m☑\u001b[0m Q 833+353 T 1186 \u001b[92m☑\u001b[0m Q 19+189  T 208  \u001b[92m☑\u001b[0m Q 8+750   T 758  \u001b[92m☑\u001b[0m Q 336+827 T 1163 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0884 - acc: 0.9851 - val_loss: 0.0857 - val_acc: 0.9847\n",
      "Q 483+0   T 483  \u001b[92m☑\u001b[0m Q 51+568  T 619  \u001b[92m☑\u001b[0m Q 94+86   T 180  \u001b[92m☑\u001b[0m Q 2+439   T 441  \u001b[92m☑\u001b[0m Q 64+73   T 137  \u001b[92m☑\u001b[0m Q 81+934  T 1015 \u001b[92m☑\u001b[0m Q 370+228 T 598  \u001b[92m☑\u001b[0m Q 195+339 T 534  \u001b[92m☑\u001b[0m Q 496+94  T 590  \u001b[92m☑\u001b[0m Q 92+40   T 132  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 20\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0833 - acc: 0.9836 - val_loss: 0.0663 - val_acc: 0.9899\n",
      "Q 608+3   T 611  \u001b[92m☑\u001b[0m Q 841+16  T 857  \u001b[92m☑\u001b[0m Q 92+709  T 801  \u001b[92m☑\u001b[0m Q 49+34   T 83   \u001b[92m☑\u001b[0m Q 628+5   T 633  \u001b[92m☑\u001b[0m Q 840+969 T 1809 \u001b[92m☑\u001b[0m Q 641+73  T 714  \u001b[92m☑\u001b[0m Q 714+38  T 752  \u001b[92m☑\u001b[0m Q 90+328  T 418  \u001b[92m☑\u001b[0m Q 52+956  T 1008 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 21\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0637 - acc: 0.9895 - val_loss: 0.0574 - val_acc: 0.9911\n",
      "Q 63+178  T 241  \u001b[92m☑\u001b[0m Q 447+559 T 1006 \u001b[92m☑\u001b[0m Q 13+512  T 525  \u001b[92m☑\u001b[0m Q 872+336 T 1208 \u001b[92m☑\u001b[0m Q 12+475  T 487  \u001b[92m☑\u001b[0m Q 297+86  T 383  \u001b[92m☑\u001b[0m Q 56+535  T 591  \u001b[92m☑\u001b[0m Q 268+43  T 311  \u001b[92m☑\u001b[0m Q 95+859  T 954  \u001b[92m☑\u001b[0m Q 9+776   T 785  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 22\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0474 - acc: 0.9937 - val_loss: 0.0598 - val_acc: 0.9881\n",
      "Q 97+743  T 840  \u001b[91m☒\u001b[0m Q 512+488 T 1000 \u001b[91m☒\u001b[0m Q 91+4    T 95   \u001b[92m☑\u001b[0m Q 521+8   T 529  \u001b[92m☑\u001b[0m Q 9+859   T 868  \u001b[92m☑\u001b[0m Q 2+381   T 383  \u001b[92m☑\u001b[0m Q 569+17  T 586  \u001b[92m☑\u001b[0m Q 909+247 T 1156 \u001b[92m☑\u001b[0m Q 35+452  T 487  \u001b[92m☑\u001b[0m Q 368+306 T 674  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 23\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0621 - acc: 0.9859 - val_loss: 0.0471 - val_acc: 0.9916\n",
      "Q 3+553   T 556  \u001b[92m☑\u001b[0m Q 292+515 T 807  \u001b[91m☒\u001b[0m Q 56+384  T 440  \u001b[92m☑\u001b[0m Q 215+546 T 761  \u001b[92m☑\u001b[0m Q 568+60  T 628  \u001b[92m☑\u001b[0m Q 545+46  T 591  \u001b[92m☑\u001b[0m Q 9+570   T 579  \u001b[92m☑\u001b[0m Q 113+227 T 340  \u001b[92m☑\u001b[0m Q 536+774 T 1310 \u001b[92m☑\u001b[0m Q 836+546 T 1382 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 24\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0305 - acc: 0.9973 - val_loss: 0.0330 - val_acc: 0.9952\n",
      "Q 963+54  T 1017 \u001b[92m☑\u001b[0m Q 73+944  T 1017 \u001b[92m☑\u001b[0m Q 312+82  T 394  \u001b[92m☑\u001b[0m Q 798+805 T 1603 \u001b[92m☑\u001b[0m Q 43+532  T 575  \u001b[92m☑\u001b[0m Q 97+800  T 897  \u001b[92m☑\u001b[0m Q 848+28  T 876  \u001b[92m☑\u001b[0m Q 421+20  T 441  \u001b[92m☑\u001b[0m Q 750+59  T 809  \u001b[92m☑\u001b[0m Q 1+970   T 971  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 25\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0256 - acc: 0.9978 - val_loss: 0.0312 - val_acc: 0.9948\n",
      "Q 164+19  T 183  \u001b[92m☑\u001b[0m Q 89+200  T 289  \u001b[92m☑\u001b[0m Q 35+540  T 575  \u001b[92m☑\u001b[0m Q 459+324 T 783  \u001b[92m☑\u001b[0m Q 423+62  T 485  \u001b[92m☑\u001b[0m Q 89+888  T 977  \u001b[92m☑\u001b[0m Q 90+94   T 184  \u001b[92m☑\u001b[0m Q 982+892 T 1874 \u001b[92m☑\u001b[0m Q 34+355  T 389  \u001b[92m☑\u001b[0m Q 448+0   T 448  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 26\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0476 - acc: 0.9882 - val_loss: 0.0333 - val_acc: 0.9946\n",
      "Q 482+1   T 483  \u001b[92m☑\u001b[0m Q 699+6   T 705  \u001b[92m☑\u001b[0m Q 478+16  T 494  \u001b[92m☑\u001b[0m Q 489+65  T 554  \u001b[92m☑\u001b[0m Q 6+997   T 1003 \u001b[92m☑\u001b[0m Q 18+35   T 53   \u001b[92m☑\u001b[0m Q 74+275  T 349  \u001b[92m☑\u001b[0m Q 541+924 T 1465 \u001b[92m☑\u001b[0m Q 919+555 T 1474 \u001b[92m☑\u001b[0m Q 950+712 T 1662 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 27\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0198 - acc: 0.9984 - val_loss: 0.0215 - val_acc: 0.9974\n",
      "Q 27+293  T 320  \u001b[92m☑\u001b[0m Q 625+48  T 673  \u001b[92m☑\u001b[0m Q 833+5   T 838  \u001b[92m☑\u001b[0m Q 758+733 T 1491 \u001b[92m☑\u001b[0m Q 79+84   T 163  \u001b[92m☑\u001b[0m Q 441+691 T 1132 \u001b[92m☑\u001b[0m Q 92+173  T 265  \u001b[92m☑\u001b[0m Q 558+95  T 653  \u001b[92m☑\u001b[0m Q 764+57  T 821  \u001b[92m☑\u001b[0m Q 121+29  T 150  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 28\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0164 - acc: 0.9988 - val_loss: 0.0209 - val_acc: 0.9969\n",
      "Q 414+85  T 499  \u001b[92m☑\u001b[0m Q 237+202 T 439  \u001b[92m☑\u001b[0m Q 94+498  T 592  \u001b[92m☑\u001b[0m Q 451+0   T 451  \u001b[92m☑\u001b[0m Q 56+29   T 85   \u001b[92m☑\u001b[0m Q 27+55   T 82   \u001b[92m☑\u001b[0m Q 6+59    T 65   \u001b[92m☑\u001b[0m Q 350+496 T 846  \u001b[92m☑\u001b[0m Q 382+29  T 411  \u001b[92m☑\u001b[0m Q 926+89  T 1015 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 29\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0298 - acc: 0.9939 - val_loss: 0.2711 - val_acc: 0.9139\n",
      "Q 9+850   T 859  \u001b[92m☑\u001b[0m Q 10+769  T 779  \u001b[92m☑\u001b[0m Q 382+17  T 399  \u001b[92m☑\u001b[0m Q 79+478  T 557  \u001b[91m☒\u001b[0m Q 79+930  T 1009 \u001b[92m☑\u001b[0m Q 45+883  T 928  \u001b[92m☑\u001b[0m Q 0+63    T 63   \u001b[91m☒\u001b[0m Q 858+44  T 902  \u001b[92m☑\u001b[0m Q 158+494 T 652  \u001b[91m☒\u001b[0m Q 556+70  T 626  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 30\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0495 - acc: 0.9872 - val_loss: 0.0205 - val_acc: 0.9967\n",
      "Q 973+3   T 976  \u001b[92m☑\u001b[0m Q 841+44  T 885  \u001b[92m☑\u001b[0m Q 386+508 T 894  \u001b[92m☑\u001b[0m Q 384+73  T 457  \u001b[92m☑\u001b[0m Q 8+844   T 852  \u001b[92m☑\u001b[0m Q 955+64  T 1019 \u001b[92m☑\u001b[0m Q 25+289  T 314  \u001b[92m☑\u001b[0m Q 201+945 T 1146 \u001b[92m☑\u001b[0m Q 144+295 T 439  \u001b[92m☑\u001b[0m Q 19+32   T 51   \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 31\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0114 - acc: 0.9994 - val_loss: 0.0142 - val_acc: 0.9982\n",
      "Q 399+662 T 1061 \u001b[92m☑\u001b[0m Q 2+802   T 804  \u001b[92m☑\u001b[0m Q 732+55  T 787  \u001b[92m☑\u001b[0m Q 87+757  T 844  \u001b[92m☑\u001b[0m Q 45+906  T 951  \u001b[92m☑\u001b[0m Q 926+6   T 932  \u001b[92m☑\u001b[0m Q 82+967  T 1049 \u001b[92m☑\u001b[0m Q 19+474  T 493  \u001b[92m☑\u001b[0m Q 98+741  T 839  \u001b[92m☑\u001b[0m Q 22+891  T 913  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 32\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0095 - acc: 0.9995 - val_loss: 0.0133 - val_acc: 0.9980\n",
      "Q 621+54  T 675  \u001b[92m☑\u001b[0m Q 251+8   T 259  \u001b[92m☑\u001b[0m Q 116+187 T 303  \u001b[92m☑\u001b[0m Q 72+44   T 116  \u001b[92m☑\u001b[0m Q 952+42  T 994  \u001b[92m☑\u001b[0m Q 765+9   T 774  \u001b[92m☑\u001b[0m Q 101+4   T 105  \u001b[92m☑\u001b[0m Q 49+603  T 652  \u001b[92m☑\u001b[0m Q 88+783  T 871  \u001b[92m☑\u001b[0m Q 606+67  T 673  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 33\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0101 - acc: 0.9991 - val_loss: 0.0487 - val_acc: 0.9843\n",
      "Q 29+299  T 328  \u001b[92m☑\u001b[0m Q 3+401   T 404  \u001b[92m☑\u001b[0m Q 267+904 T 1171 \u001b[92m☑\u001b[0m Q 39+876  T 915  \u001b[92m☑\u001b[0m Q 279+280 T 559  \u001b[92m☑\u001b[0m Q 20+226  T 246  \u001b[92m☑\u001b[0m Q 926+6   T 932  \u001b[92m☑\u001b[0m Q 15+793  T 808  \u001b[92m☑\u001b[0m Q 280+97  T 377  \u001b[92m☑\u001b[0m Q 95+240  T 335  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 34\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0617 - acc: 0.9810 - val_loss: 0.0158 - val_acc: 0.9972\n",
      "Q 83+277  T 360  \u001b[92m☑\u001b[0m Q 314+3   T 317  \u001b[92m☑\u001b[0m Q 227+70  T 297  \u001b[92m☑\u001b[0m Q 113+81  T 194  \u001b[92m☑\u001b[0m Q 617+514 T 1131 \u001b[92m☑\u001b[0m Q 656+72  T 728  \u001b[92m☑\u001b[0m Q 918+88  T 1006 \u001b[92m☑\u001b[0m Q 872+53  T 925  \u001b[92m☑\u001b[0m Q 41+549  T 590  \u001b[92m☑\u001b[0m Q 854+753 T 1607 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 35\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0083 - acc: 0.9995 - val_loss: 0.0106 - val_acc: 0.9985\n",
      "Q 50+563  T 613  \u001b[92m☑\u001b[0m Q 97+743  T 840  \u001b[92m☑\u001b[0m Q 630+22  T 652  \u001b[92m☑\u001b[0m Q 2+99    T 101  \u001b[92m☑\u001b[0m Q 672+422 T 1094 \u001b[92m☑\u001b[0m Q 65+784  T 849  \u001b[92m☑\u001b[0m Q 730+47  T 777  \u001b[92m☑\u001b[0m Q 42+582  T 624  \u001b[92m☑\u001b[0m Q 97+31   T 128  \u001b[92m☑\u001b[0m Q 877+898 T 1775 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 36\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0066 - acc: 0.9998 - val_loss: 0.0093 - val_acc: 0.9988\n",
      "Q 532+5   T 537  \u001b[92m☑\u001b[0m Q 310+929 T 1239 \u001b[92m☑\u001b[0m Q 606+8   T 614  \u001b[92m☑\u001b[0m Q 20+405  T 425  \u001b[92m☑\u001b[0m Q 8+881   T 889  \u001b[92m☑\u001b[0m Q 60+199  T 259  \u001b[92m☑\u001b[0m Q 7+175   T 182  \u001b[92m☑\u001b[0m Q 37+403  T 440  \u001b[92m☑\u001b[0m Q 73+357  T 430  \u001b[92m☑\u001b[0m Q 60+495  T 555  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 37\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0061 - acc: 0.9997 - val_loss: 0.0096 - val_acc: 0.9980\n",
      "Q 21+172  T 193  \u001b[92m☑\u001b[0m Q 761+95  T 856  \u001b[92m☑\u001b[0m Q 517+77  T 594  \u001b[92m☑\u001b[0m Q 682+74  T 756  \u001b[92m☑\u001b[0m Q 640+964 T 1604 \u001b[92m☑\u001b[0m Q 794+1   T 795  \u001b[92m☑\u001b[0m Q 750+821 T 1571 \u001b[92m☑\u001b[0m Q 63+165  T 228  \u001b[92m☑\u001b[0m Q 221+46  T 267  \u001b[92m☑\u001b[0m Q 263+98  T 361  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 38\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0411 - acc: 0.9875 - val_loss: 0.0199 - val_acc: 0.9950\n",
      "Q 223+6   T 229  \u001b[92m☑\u001b[0m Q 361+77  T 438  \u001b[92m☑\u001b[0m Q 337+1   T 338  \u001b[92m☑\u001b[0m Q 581+2   T 583  \u001b[92m☑\u001b[0m Q 75+220  T 295  \u001b[92m☑\u001b[0m Q 74+85   T 159  \u001b[92m☑\u001b[0m Q 50+900  T 950  \u001b[92m☑\u001b[0m Q 277+485 T 762  \u001b[92m☑\u001b[0m Q 466+202 T 668  \u001b[92m☑\u001b[0m Q 451+7   T 458  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 39\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0075 - acc: 0.9994 - val_loss: 0.0090 - val_acc: 0.9987\n",
      "Q 775+651 T 1426 \u001b[92m☑\u001b[0m Q 9+859   T 868  \u001b[92m☑\u001b[0m Q 2+972   T 974  \u001b[92m☑\u001b[0m Q 2+964   T 966  \u001b[92m☑\u001b[0m Q 31+496  T 527  \u001b[92m☑\u001b[0m Q 471+822 T 1293 \u001b[92m☑\u001b[0m Q 83+3    T 86   \u001b[92m☑\u001b[0m Q 25+43   T 68   \u001b[92m☑\u001b[0m Q 507+29  T 536  \u001b[92m☑\u001b[0m Q 28+512  T 540  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 40\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0048 - acc: 0.9998 - val_loss: 0.0073 - val_acc: 0.9990\n",
      "Q 7+230   T 237  \u001b[92m☑\u001b[0m Q 390+538 T 928  \u001b[92m☑\u001b[0m Q 526+89  T 615  \u001b[92m☑\u001b[0m Q 43+835  T 878  \u001b[92m☑\u001b[0m Q 54+167  T 221  \u001b[92m☑\u001b[0m Q 404+881 T 1285 \u001b[92m☑\u001b[0m Q 25+289  T 314  \u001b[92m☑\u001b[0m Q 271+588 T 859  \u001b[92m☑\u001b[0m Q 302+774 T 1076 \u001b[92m☑\u001b[0m Q 80+519  T 599  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 41\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0042 - acc: 0.9998 - val_loss: 0.0071 - val_acc: 0.9988\n",
      "Q 145+27  T 172  \u001b[92m☑\u001b[0m Q 210+656 T 866  \u001b[92m☑\u001b[0m Q 971+87  T 1058 \u001b[92m☑\u001b[0m Q 66+97   T 163  \u001b[92m☑\u001b[0m Q 81+51   T 132  \u001b[92m☑\u001b[0m Q 158+64  T 222  \u001b[92m☑\u001b[0m Q 765+53  T 818  \u001b[92m☑\u001b[0m Q 62+94   T 156  \u001b[92m☑\u001b[0m Q 212+910 T 1122 \u001b[92m☑\u001b[0m Q 937+709 T 1646 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 42\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0041 - acc: 0.9998 - val_loss: 0.0069 - val_acc: 0.9989\n",
      "Q 779+8   T 787  \u001b[92m☑\u001b[0m Q 447+559 T 1006 \u001b[92m☑\u001b[0m Q 76+259  T 335  \u001b[92m☑\u001b[0m Q 611+391 T 1002 \u001b[92m☑\u001b[0m Q 3+934   T 937  \u001b[92m☑\u001b[0m Q 599+454 T 1053 \u001b[92m☑\u001b[0m Q 281+884 T 1165 \u001b[92m☑\u001b[0m Q 2+976   T 978  \u001b[92m☑\u001b[0m Q 656+36  T 692  \u001b[92m☑\u001b[0m Q 8+690   T 698  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 43\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0038 - acc: 0.9998 - val_loss: 0.0078 - val_acc: 0.9986\n",
      "Q 12+92   T 104  \u001b[92m☑\u001b[0m Q 71+124  T 195  \u001b[92m☑\u001b[0m Q 812+372 T 1184 \u001b[92m☑\u001b[0m Q 551+74  T 625  \u001b[92m☑\u001b[0m Q 28+512  T 540  \u001b[92m☑\u001b[0m Q 112+41  T 153  \u001b[92m☑\u001b[0m Q 835+70  T 905  \u001b[92m☑\u001b[0m Q 92+168  T 260  \u001b[92m☑\u001b[0m Q 434+68  T 502  \u001b[92m☑\u001b[0m Q 955+64  T 1019 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 44\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0490 - acc: 0.9853 - val_loss: 0.0090 - val_acc: 0.9983\n",
      "Q 717+928 T 1645 \u001b[92m☑\u001b[0m Q 357+255 T 612  \u001b[92m☑\u001b[0m Q 274+962 T 1236 \u001b[92m☑\u001b[0m Q 2+802   T 804  \u001b[92m☑\u001b[0m Q 989+296 T 1285 \u001b[92m☑\u001b[0m Q 606+766 T 1372 \u001b[92m☑\u001b[0m Q 142+34  T 176  \u001b[92m☑\u001b[0m Q 119+23  T 142  \u001b[92m☑\u001b[0m Q 989+516 T 1505 \u001b[92m☑\u001b[0m Q 885+85  T 970  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 45\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0045 - acc: 0.9998 - val_loss: 0.0067 - val_acc: 0.9989\n",
      "Q 59+583  T 642  \u001b[92m☑\u001b[0m Q 5+861   T 866  \u001b[92m☑\u001b[0m Q 0+722   T 722  \u001b[92m☑\u001b[0m Q 270+687 T 957  \u001b[92m☑\u001b[0m Q 51+198  T 249  \u001b[92m☑\u001b[0m Q 9+277   T 286  \u001b[92m☑\u001b[0m Q 34+254  T 288  \u001b[92m☑\u001b[0m Q 91+346  T 437  \u001b[92m☑\u001b[0m Q 37+494  T 531  \u001b[92m☑\u001b[0m Q 40+69   T 109  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 46\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0032 - acc: 0.9999 - val_loss: 0.0055 - val_acc: 0.9991\n",
      "Q 760+714 T 1474 \u001b[92m☑\u001b[0m Q 321+21  T 342  \u001b[92m☑\u001b[0m Q 946+14  T 960  \u001b[92m☑\u001b[0m Q 1+829   T 830  \u001b[92m☑\u001b[0m Q 803+630 T 1433 \u001b[92m☑\u001b[0m Q 11+872  T 883  \u001b[92m☑\u001b[0m Q 77+796  T 873  \u001b[92m☑\u001b[0m Q 450+559 T 1009 \u001b[92m☑\u001b[0m Q 779+416 T 1195 \u001b[92m☑\u001b[0m Q 95+984  T 1079 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 47\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0029 - acc: 0.9999 - val_loss: 0.0073 - val_acc: 0.9981\n",
      "Q 228+10  T 238  \u001b[92m☑\u001b[0m Q 892+845 T 1737 \u001b[92m☑\u001b[0m Q 359+91  T 450  \u001b[92m☑\u001b[0m Q 27+105  T 132  \u001b[92m☑\u001b[0m Q 53+222  T 275  \u001b[92m☑\u001b[0m Q 812+74  T 886  \u001b[92m☑\u001b[0m Q 7+908   T 915  \u001b[92m☑\u001b[0m Q 643+498 T 1141 \u001b[92m☑\u001b[0m Q 649+13  T 662  \u001b[92m☑\u001b[0m Q 243+639 T 882  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 48\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0028 - acc: 0.9998 - val_loss: 0.0050 - val_acc: 0.9991\n",
      "Q 271+940 T 1211 \u001b[92m☑\u001b[0m Q 839+419 T 1258 \u001b[92m☑\u001b[0m Q 528+35  T 563  \u001b[92m☑\u001b[0m Q 378+7   T 385  \u001b[92m☑\u001b[0m Q 47+555  T 602  \u001b[92m☑\u001b[0m Q 414+3   T 417  \u001b[92m☑\u001b[0m Q 2+668   T 670  \u001b[92m☑\u001b[0m Q 4+227   T 231  \u001b[92m☑\u001b[0m Q 171+23  T 194  \u001b[92m☑\u001b[0m Q 700+533 T 1233 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 49\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0520 - acc: 0.9844 - val_loss: 0.0217 - val_acc: 0.9941\n",
      "Q 814+990 T 1804 \u001b[92m☑\u001b[0m Q 778+18  T 796  \u001b[92m☑\u001b[0m Q 818+2   T 820  \u001b[92m☑\u001b[0m Q 98+52   T 150  \u001b[92m☑\u001b[0m Q 5+168   T 173  \u001b[92m☑\u001b[0m Q 923+779 T 1702 \u001b[92m☑\u001b[0m Q 68+868  T 936  \u001b[92m☑\u001b[0m Q 97+693  T 790  \u001b[92m☑\u001b[0m Q 12+392  T 404  \u001b[92m☑\u001b[0m Q 5+700   T 705  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 50\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0060 - acc: 0.9992 - val_loss: 0.0058 - val_acc: 0.9992\n",
      "Q 43+919  T 962  \u001b[92m☑\u001b[0m Q 942+85  T 1027 \u001b[92m☑\u001b[0m Q 67+826  T 893  \u001b[92m☑\u001b[0m Q 92+51   T 143  \u001b[92m☑\u001b[0m Q 637+577 T 1214 \u001b[92m☑\u001b[0m Q 325+8   T 333  \u001b[92m☑\u001b[0m Q 6+197   T 203  \u001b[92m☑\u001b[0m Q 199+215 T 414  \u001b[92m☑\u001b[0m Q 163+504 T 667  \u001b[92m☑\u001b[0m Q 307+5   T 312  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 51\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0027 - acc: 0.9999 - val_loss: 0.0048 - val_acc: 0.9994\n",
      "Q 6+142   T 148  \u001b[92m☑\u001b[0m Q 860+91  T 951  \u001b[92m☑\u001b[0m Q 358+814 T 1172 \u001b[92m☑\u001b[0m Q 211+31  T 242  \u001b[92m☑\u001b[0m Q 739+82  T 821  \u001b[92m☑\u001b[0m Q 841+753 T 1594 \u001b[92m☑\u001b[0m Q 36+184  T 220  \u001b[92m☑\u001b[0m Q 49+1    T 50   \u001b[92m☑\u001b[0m Q 77+644  T 721  \u001b[92m☑\u001b[0m Q 335+0   T 335  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 52\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0024 - acc: 0.9999 - val_loss: 0.0042 - val_acc: 0.9994\n",
      "Q 841+73  T 914  \u001b[92m☑\u001b[0m Q 360+95  T 455  \u001b[92m☑\u001b[0m Q 294+52  T 346  \u001b[92m☑\u001b[0m Q 559+695 T 1254 \u001b[92m☑\u001b[0m Q 99+36   T 135  \u001b[92m☑\u001b[0m Q 24+273  T 297  \u001b[92m☑\u001b[0m Q 128+987 T 1115 \u001b[92m☑\u001b[0m Q 146+5   T 151  \u001b[92m☑\u001b[0m Q 325+70  T 395  \u001b[92m☑\u001b[0m Q 200+736 T 936  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 53\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0021 - acc: 0.9999 - val_loss: 0.0043 - val_acc: 0.9993\n",
      "Q 34+355  T 389  \u001b[92m☑\u001b[0m Q 961+177 T 1138 \u001b[92m☑\u001b[0m Q 43+892  T 935  \u001b[92m☑\u001b[0m Q 906+94  T 1000 \u001b[92m☑\u001b[0m Q 918+3   T 921  \u001b[92m☑\u001b[0m Q 811+447 T 1258 \u001b[92m☑\u001b[0m Q 494+899 T 1393 \u001b[92m☑\u001b[0m Q 143+945 T 1088 \u001b[92m☑\u001b[0m Q 92+268  T 360  \u001b[92m☑\u001b[0m Q 580+20  T 600  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 54\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0122 - acc: 0.9967 - val_loss: 0.2125 - val_acc: 0.9336\n",
      "Q 52+16   T 68   \u001b[92m☑\u001b[0m Q 66+8    T 74   \u001b[92m☑\u001b[0m Q 5+453   T 458  \u001b[92m☑\u001b[0m Q 985+0   T 985  \u001b[92m☑\u001b[0m Q 398+923 T 1321 \u001b[92m☑\u001b[0m Q 994+5   T 999  \u001b[92m☑\u001b[0m Q 828+876 T 1704 \u001b[92m☑\u001b[0m Q 806+4   T 810  \u001b[92m☑\u001b[0m Q 268+43  T 311  \u001b[92m☑\u001b[0m Q 73+357  T 430  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 55\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0418 - acc: 0.9877 - val_loss: 0.0058 - val_acc: 0.9990\n",
      "Q 158+64  T 222  \u001b[92m☑\u001b[0m Q 73+258  T 331  \u001b[92m☑\u001b[0m Q 106+817 T 923  \u001b[92m☑\u001b[0m Q 473+468 T 941  \u001b[92m☑\u001b[0m Q 514+136 T 650  \u001b[92m☑\u001b[0m Q 868+70  T 938  \u001b[92m☑\u001b[0m Q 4+761   T 765  \u001b[92m☑\u001b[0m Q 2+998   T 1000 \u001b[92m☑\u001b[0m Q 870+33  T 903  \u001b[92m☑\u001b[0m Q 56+29   T 85   \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 56\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0026 - acc: 0.9999 - val_loss: 0.0042 - val_acc: 0.9992\n",
      "Q 87+62   T 149  \u001b[92m☑\u001b[0m Q 66+62   T 128  \u001b[92m☑\u001b[0m Q 335+0   T 335  \u001b[92m☑\u001b[0m Q 177+13  T 190  \u001b[92m☑\u001b[0m Q 1+441   T 442  \u001b[92m☑\u001b[0m Q 478+14  T 492  \u001b[92m☑\u001b[0m Q 87+287  T 374  \u001b[92m☑\u001b[0m Q 631+4   T 635  \u001b[92m☑\u001b[0m Q 9+202   T 211  \u001b[92m☑\u001b[0m Q 387+820 T 1207 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 57\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 0.9994\n",
      "Q 266+637 T 903  \u001b[92m☑\u001b[0m Q 286+95  T 381  \u001b[92m☑\u001b[0m Q 93+266  T 359  \u001b[92m☑\u001b[0m Q 243+32  T 275  \u001b[92m☑\u001b[0m Q 529+51  T 580  \u001b[92m☑\u001b[0m Q 686+4   T 690  \u001b[92m☑\u001b[0m Q 892+366 T 1258 \u001b[92m☑\u001b[0m Q 17+728  T 745  \u001b[92m☑\u001b[0m Q 30+801  T 831  \u001b[92m☑\u001b[0m Q 96+289  T 385  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 58\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0018 - acc: 0.9999 - val_loss: 0.0035 - val_acc: 0.9994\n",
      "Q 332+7   T 339  \u001b[92m☑\u001b[0m Q 597+375 T 972  \u001b[92m☑\u001b[0m Q 16+8    T 24   \u001b[92m☑\u001b[0m Q 685+361 T 1046 \u001b[92m☑\u001b[0m Q 410+2   T 412  \u001b[92m☑\u001b[0m Q 352+409 T 761  \u001b[92m☑\u001b[0m Q 976+272 T 1248 \u001b[92m☑\u001b[0m Q 315+5   T 320  \u001b[92m☑\u001b[0m Q 15+15   T 30   \u001b[92m☑\u001b[0m Q 630+99  T 729  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 59\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0016 - acc: 0.9999 - val_loss: 0.0040 - val_acc: 0.9992\n",
      "Q 81+287  T 368  \u001b[92m☑\u001b[0m Q 79+853  T 932  \u001b[92m☑\u001b[0m Q 510+13  T 523  \u001b[92m☑\u001b[0m Q 31+674  T 705  \u001b[92m☑\u001b[0m Q 338+177 T 515  \u001b[92m☑\u001b[0m Q 119+74  T 193  \u001b[92m☑\u001b[0m Q 109+446 T 555  \u001b[92m☑\u001b[0m Q 52+114  T 166  \u001b[92m☑\u001b[0m Q 250+270 T 520  \u001b[92m☑\u001b[0m Q 38+590  T 628  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 60\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0016 - acc: 0.9999 - val_loss: 0.0040 - val_acc: 0.9991\n",
      "Q 5+453   T 458  \u001b[92m☑\u001b[0m Q 88+5    T 93   \u001b[92m☑\u001b[0m Q 693+649 T 1342 \u001b[92m☑\u001b[0m Q 711+345 T 1056 \u001b[92m☑\u001b[0m Q 349+588 T 937  \u001b[92m☑\u001b[0m Q 893+221 T 1114 \u001b[92m☑\u001b[0m Q 773+511 T 1284 \u001b[92m☑\u001b[0m Q 90+901  T 991  \u001b[92m☑\u001b[0m Q 595+608 T 1203 \u001b[92m☑\u001b[0m Q 746+48  T 794  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 61\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0449 - acc: 0.9860 - val_loss: 0.0068 - val_acc: 0.9987\n",
      "Q 906+53  T 959  \u001b[92m☑\u001b[0m Q 734+780 T 1514 \u001b[92m☑\u001b[0m Q 48+559  T 607  \u001b[92m☑\u001b[0m Q 679+3   T 682  \u001b[92m☑\u001b[0m Q 578+537 T 1115 \u001b[92m☑\u001b[0m Q 34+35   T 69   \u001b[92m☑\u001b[0m Q 87+62   T 149  \u001b[92m☑\u001b[0m Q 982+892 T 1874 \u001b[92m☑\u001b[0m Q 908+26  T 934  \u001b[92m☑\u001b[0m Q 540+27  T 567  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 62\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0027 - acc: 0.9999 - val_loss: 0.0039 - val_acc: 0.9997\n",
      "Q 6+416   T 422  \u001b[92m☑\u001b[0m Q 243+880 T 1123 \u001b[92m☑\u001b[0m Q 280+97  T 377  \u001b[92m☑\u001b[0m Q 959+9   T 968  \u001b[92m☑\u001b[0m Q 46+49   T 95   \u001b[92m☑\u001b[0m Q 6+145   T 151  \u001b[92m☑\u001b[0m Q 19+645  T 664  \u001b[92m☑\u001b[0m Q 22+15   T 37   \u001b[92m☑\u001b[0m Q 585+973 T 1558 \u001b[92m☑\u001b[0m Q 56+515  T 571  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 63\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 0.9997\n",
      "Q 27+1    T 28   \u001b[92m☑\u001b[0m Q 616+766 T 1382 \u001b[92m☑\u001b[0m Q 621+75  T 696  \u001b[92m☑\u001b[0m Q 78+408  T 486  \u001b[92m☑\u001b[0m Q 94+186  T 280  \u001b[92m☑\u001b[0m Q 708+304 T 1012 \u001b[92m☑\u001b[0m Q 616+3   T 619  \u001b[92m☑\u001b[0m Q 891+8   T 899  \u001b[92m☑\u001b[0m Q 406+4   T 410  \u001b[92m☑\u001b[0m Q 58+629  T 687  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 64\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9997\n",
      "Q 74+871  T 945  \u001b[92m☑\u001b[0m Q 630+22  T 652  \u001b[92m☑\u001b[0m Q 234+60  T 294  \u001b[92m☑\u001b[0m Q 24+38   T 62   \u001b[92m☑\u001b[0m Q 73+6    T 79   \u001b[92m☑\u001b[0m Q 708+0   T 708  \u001b[92m☑\u001b[0m Q 495+79  T 574  \u001b[92m☑\u001b[0m Q 33+952  T 985  \u001b[92m☑\u001b[0m Q 556+70  T 626  \u001b[92m☑\u001b[0m Q 317+289 T 606  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 65\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.0032 - val_acc: 0.9995\n",
      "Q 429+70  T 499  \u001b[92m☑\u001b[0m Q 405+16  T 421  \u001b[92m☑\u001b[0m Q 215+303 T 518  \u001b[92m☑\u001b[0m Q 68+541  T 609  \u001b[92m☑\u001b[0m Q 674+64  T 738  \u001b[92m☑\u001b[0m Q 87+885  T 972  \u001b[92m☑\u001b[0m Q 3+934   T 937  \u001b[92m☑\u001b[0m Q 337+76  T 413  \u001b[92m☑\u001b[0m Q 242+158 T 400  \u001b[92m☑\u001b[0m Q 236+457 T 693  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 66\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0266 - acc: 0.9920 - val_loss: 0.0939 - val_acc: 0.9669\n",
      "Q 509+4   T 513  \u001b[92m☑\u001b[0m Q 525+680 T 1205 \u001b[92m☑\u001b[0m Q 498+235 T 733  \u001b[92m☑\u001b[0m Q 112+41  T 153  \u001b[92m☑\u001b[0m Q 563+60  T 623  \u001b[92m☑\u001b[0m Q 936+4   T 940  \u001b[92m☑\u001b[0m Q 606+7   T 613  \u001b[92m☑\u001b[0m Q 674+64  T 738  \u001b[92m☑\u001b[0m Q 357+22  T 379  \u001b[91m☒\u001b[0m Q 977+837 T 1814 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 67\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0148 - acc: 0.9959 - val_loss: 0.0052 - val_acc: 0.9991\n",
      "Q 83+346  T 429  \u001b[92m☑\u001b[0m Q 786+378 T 1164 \u001b[92m☑\u001b[0m Q 506+53  T 559  \u001b[92m☑\u001b[0m Q 293+504 T 797  \u001b[92m☑\u001b[0m Q 6+641   T 647  \u001b[92m☑\u001b[0m Q 764+57  T 821  \u001b[92m☑\u001b[0m Q 529+55  T 584  \u001b[92m☑\u001b[0m Q 3+866   T 869  \u001b[92m☑\u001b[0m Q 8+185   T 193  \u001b[92m☑\u001b[0m Q 71+109  T 180  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 68\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 0.9994\n",
      "Q 774+807 T 1581 \u001b[92m☑\u001b[0m Q 8+394   T 402  \u001b[92m☑\u001b[0m Q 29+505  T 534  \u001b[92m☑\u001b[0m Q 995+847 T 1842 \u001b[92m☑\u001b[0m Q 652+354 T 1006 \u001b[92m☑\u001b[0m Q 48+612  T 660  \u001b[92m☑\u001b[0m Q 706+184 T 890  \u001b[92m☑\u001b[0m Q 901+63  T 964  \u001b[92m☑\u001b[0m Q 752+933 T 1685 \u001b[92m☑\u001b[0m Q 830+538 T 1368 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 69\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9994\n",
      "Q 85+378  T 463  \u001b[92m☑\u001b[0m Q 534+83  T 617  \u001b[92m☑\u001b[0m Q 4+600   T 604  \u001b[92m☑\u001b[0m Q 84+428  T 512  \u001b[92m☑\u001b[0m Q 458+936 T 1394 \u001b[92m☑\u001b[0m Q 842+6   T 848  \u001b[92m☑\u001b[0m Q 233+7   T 240  \u001b[92m☑\u001b[0m Q 851+623 T 1474 \u001b[92m☑\u001b[0m Q 76+259  T 335  \u001b[92m☑\u001b[0m Q 569+6   T 575  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 70\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 0.9996\n",
      "Q 85+780  T 865  \u001b[92m☑\u001b[0m Q 928+7   T 935  \u001b[92m☑\u001b[0m Q 841+73  T 914  \u001b[92m☑\u001b[0m Q 20+24   T 44   \u001b[92m☑\u001b[0m Q 42+91   T 133  \u001b[92m☑\u001b[0m Q 607+552 T 1159 \u001b[92m☑\u001b[0m Q 639+71  T 710  \u001b[92m☑\u001b[0m Q 639+6   T 645  \u001b[92m☑\u001b[0m Q 80+687  T 767  \u001b[92m☑\u001b[0m Q 476+396 T 872  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 71\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.0037 - val_acc: 0.9993\n",
      "Q 75+147  T 222  \u001b[92m☑\u001b[0m Q 367+56  T 423  \u001b[92m☑\u001b[0m Q 646+6   T 652  \u001b[92m☑\u001b[0m Q 30+974  T 1004 \u001b[92m☑\u001b[0m Q 229+815 T 1044 \u001b[92m☑\u001b[0m Q 677+448 T 1125 \u001b[92m☑\u001b[0m Q 300+49  T 349  \u001b[92m☑\u001b[0m Q 510+55  T 565  \u001b[92m☑\u001b[0m Q 75+330  T 405  \u001b[92m☑\u001b[0m Q 227+547 T 774  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 72\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0404 - acc: 0.9886 - val_loss: 0.0053 - val_acc: 0.9991\n",
      "Q 456+933 T 1389 \u001b[92m☑\u001b[0m Q 91+243  T 334  \u001b[92m☑\u001b[0m Q 49+194  T 243  \u001b[92m☑\u001b[0m Q 0+606   T 606  \u001b[92m☑\u001b[0m Q 0+393   T 393  \u001b[92m☑\u001b[0m Q 672+81  T 753  \u001b[92m☑\u001b[0m Q 56+17   T 73   \u001b[92m☑\u001b[0m Q 681+1   T 682  \u001b[92m☑\u001b[0m Q 668+89  T 757  \u001b[92m☑\u001b[0m Q 1+473   T 474  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 73\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 0.9997\n",
      "Q 144+26  T 170  \u001b[92m☑\u001b[0m Q 23+275  T 298  \u001b[92m☑\u001b[0m Q 325+163 T 488  \u001b[92m☑\u001b[0m Q 690+91  T 781  \u001b[92m☑\u001b[0m Q 48+947  T 995  \u001b[92m☑\u001b[0m Q 5+619   T 624  \u001b[92m☑\u001b[0m Q 690+603 T 1293 \u001b[92m☑\u001b[0m Q 9+47    T 56   \u001b[92m☑\u001b[0m Q 28+969  T 997  \u001b[92m☑\u001b[0m Q 483+32  T 515  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 74\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 0.9997\n",
      "Q 610+6   T 616  \u001b[92m☑\u001b[0m Q 173+84  T 257  \u001b[92m☑\u001b[0m Q 466+202 T 668  \u001b[92m☑\u001b[0m Q 570+1   T 571  \u001b[92m☑\u001b[0m Q 690+545 T 1235 \u001b[92m☑\u001b[0m Q 49+292  T 341  \u001b[92m☑\u001b[0m Q 97+618  T 715  \u001b[92m☑\u001b[0m Q 5+252   T 257  \u001b[92m☑\u001b[0m Q 19+915  T 934  \u001b[92m☑\u001b[0m Q 79+40   T 119  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 75\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 0.9996\n",
      "Q 866+6   T 872  \u001b[92m☑\u001b[0m Q 736+246 T 982  \u001b[92m☑\u001b[0m Q 350+515 T 865  \u001b[92m☑\u001b[0m Q 91+187  T 278  \u001b[92m☑\u001b[0m Q 2+42    T 44   \u001b[92m☑\u001b[0m Q 536+758 T 1294 \u001b[92m☑\u001b[0m Q 895+468 T 1363 \u001b[92m☑\u001b[0m Q 524+6   T 530  \u001b[92m☑\u001b[0m Q 516+75  T 591  \u001b[92m☑\u001b[0m Q 608+3   T 611  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 76\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 0.9996\n",
      "Q 560+44  T 604  \u001b[92m☑\u001b[0m Q 989+516 T 1505 \u001b[92m☑\u001b[0m Q 963+54  T 1017 \u001b[92m☑\u001b[0m Q 547+3   T 550  \u001b[92m☑\u001b[0m Q 174+816 T 990  \u001b[92m☑\u001b[0m Q 94+56   T 150  \u001b[92m☑\u001b[0m Q 353+68  T 421  \u001b[92m☑\u001b[0m Q 768+744 T 1512 \u001b[92m☑\u001b[0m Q 310+929 T 1239 \u001b[92m☑\u001b[0m Q 869+272 T 1141 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 77\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 102us/step - loss: 9.5912e-04 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9991\n",
      "Q 406+92  T 498  \u001b[92m☑\u001b[0m Q 283+8   T 291  \u001b[92m☑\u001b[0m Q 847+23  T 870  \u001b[92m☑\u001b[0m Q 512+7   T 519  \u001b[92m☑\u001b[0m Q 32+52   T 84   \u001b[92m☑\u001b[0m Q 352+371 T 723  \u001b[92m☑\u001b[0m Q 6+577   T 583  \u001b[92m☑\u001b[0m Q 5+861   T 866  \u001b[92m☑\u001b[0m Q 502+24  T 526  \u001b[92m☑\u001b[0m Q 584+66  T 650  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 78\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 8.4884e-04 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 0.9993\n",
      "Q 0+367   T 367  \u001b[92m☑\u001b[0m Q 42+406  T 448  \u001b[92m☑\u001b[0m Q 599+481 T 1080 \u001b[92m☑\u001b[0m Q 750+244 T 994  \u001b[92m☑\u001b[0m Q 327+48  T 375  \u001b[92m☑\u001b[0m Q 57+40   T 97   \u001b[92m☑\u001b[0m Q 755+30  T 785  \u001b[92m☑\u001b[0m Q 287+797 T 1084 \u001b[92m☑\u001b[0m Q 995+96  T 1091 \u001b[92m☑\u001b[0m Q 68+369  T 437  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 79\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0408 - acc: 0.9875 - val_loss: 0.0273 - val_acc: 0.9907\n",
      "Q 72+86   T 158  \u001b[92m☑\u001b[0m Q 25+289  T 314  \u001b[92m☑\u001b[0m Q 19+910  T 929  \u001b[92m☑\u001b[0m Q 66+71   T 137  \u001b[92m☑\u001b[0m Q 72+53   T 125  \u001b[92m☑\u001b[0m Q 315+828 T 1143 \u001b[92m☑\u001b[0m Q 117+748 T 865  \u001b[92m☑\u001b[0m Q 381+10  T 391  \u001b[92m☑\u001b[0m Q 743+702 T 1445 \u001b[92m☑\u001b[0m Q 426+351 T 777  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 80\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0043 - acc: 0.9993 - val_loss: 0.0036 - val_acc: 0.9994\n",
      "Q 652+354 T 1006 \u001b[92m☑\u001b[0m Q 80+403  T 483  \u001b[92m☑\u001b[0m Q 878+541 T 1419 \u001b[92m☑\u001b[0m Q 76+532  T 608  \u001b[92m☑\u001b[0m Q 739+36  T 775  \u001b[92m☑\u001b[0m Q 761+95  T 856  \u001b[92m☑\u001b[0m Q 74+409  T 483  \u001b[92m☑\u001b[0m Q 284+980 T 1264 \u001b[92m☑\u001b[0m Q 3+94    T 97   \u001b[92m☑\u001b[0m Q 476+11  T 487  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 81\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 0.9995\n",
      "Q 626+37  T 663  \u001b[92m☑\u001b[0m Q 47+367  T 414  \u001b[92m☑\u001b[0m Q 670+529 T 1199 \u001b[92m☑\u001b[0m Q 627+68  T 695  \u001b[92m☑\u001b[0m Q 83+80   T 163  \u001b[92m☑\u001b[0m Q 30+912  T 942  \u001b[92m☑\u001b[0m Q 40+774  T 814  \u001b[92m☑\u001b[0m Q 61+403  T 464  \u001b[92m☑\u001b[0m Q 859+1   T 860  \u001b[92m☑\u001b[0m Q 191+97  T 288  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 82\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9995\n",
      "Q 880+54  T 934  \u001b[92m☑\u001b[0m Q 433+85  T 518  \u001b[92m☑\u001b[0m Q 988+72  T 1060 \u001b[92m☑\u001b[0m Q 946+316 T 1262 \u001b[92m☑\u001b[0m Q 50+893  T 943  \u001b[92m☑\u001b[0m Q 290+629 T 919  \u001b[92m☑\u001b[0m Q 47+665  T 712  \u001b[92m☑\u001b[0m Q 39+557  T 596  \u001b[92m☑\u001b[0m Q 491+3   T 494  \u001b[92m☑\u001b[0m Q 2+381   T 383  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 83\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 8.6651e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 0.9997\n",
      "Q 7+604   T 611  \u001b[92m☑\u001b[0m Q 80+18   T 98   \u001b[92m☑\u001b[0m Q 187+567 T 754  \u001b[92m☑\u001b[0m Q 484+1   T 485  \u001b[92m☑\u001b[0m Q 763+298 T 1061 \u001b[92m☑\u001b[0m Q 130+89  T 219  \u001b[92m☑\u001b[0m Q 898+197 T 1095 \u001b[92m☑\u001b[0m Q 0+326   T 326  \u001b[92m☑\u001b[0m Q 28+328  T 356  \u001b[92m☑\u001b[0m Q 547+3   T 550  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 84\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 8.1137e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9994\n",
      "Q 702+979 T 1681 \u001b[92m☑\u001b[0m Q 632+19  T 651  \u001b[92m☑\u001b[0m Q 30+390  T 420  \u001b[92m☑\u001b[0m Q 820+300 T 1120 \u001b[92m☑\u001b[0m Q 257+75  T 332  \u001b[92m☑\u001b[0m Q 549+92  T 641  \u001b[92m☑\u001b[0m Q 29+70   T 99   \u001b[92m☑\u001b[0m Q 841+42  T 883  \u001b[92m☑\u001b[0m Q 82+870  T 952  \u001b[92m☑\u001b[0m Q 911+38  T 949  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 85\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 7.1844e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9996\n",
      "Q 35+946  T 981  \u001b[92m☑\u001b[0m Q 47+845  T 892  \u001b[92m☑\u001b[0m Q 948+87  T 1035 \u001b[92m☑\u001b[0m Q 62+646  T 708  \u001b[92m☑\u001b[0m Q 405+54  T 459  \u001b[92m☑\u001b[0m Q 572+634 T 1206 \u001b[92m☑\u001b[0m Q 8+166   T 174  \u001b[92m☑\u001b[0m Q 45+56   T 101  \u001b[92m☑\u001b[0m Q 494+899 T 1393 \u001b[92m☑\u001b[0m Q 32+52   T 84   \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 86\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0082 - val_acc: 0.9972\n",
      "Q 227+914 T 1141 \u001b[92m☑\u001b[0m Q 639+6   T 645  \u001b[92m☑\u001b[0m Q 358+191 T 549  \u001b[92m☑\u001b[0m Q 287+797 T 1084 \u001b[92m☑\u001b[0m Q 621+54  T 675  \u001b[92m☑\u001b[0m Q 841+44  T 885  \u001b[92m☑\u001b[0m Q 253+136 T 389  \u001b[92m☑\u001b[0m Q 73+669  T 742  \u001b[92m☑\u001b[0m Q 48+947  T 995  \u001b[92m☑\u001b[0m Q 512+9   T 521  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 87\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0452 - acc: 0.9864 - val_loss: 0.0060 - val_acc: 0.9987\n",
      "Q 432+88  T 520  \u001b[92m☑\u001b[0m Q 337+0   T 337  \u001b[92m☑\u001b[0m Q 292+24  T 316  \u001b[92m☑\u001b[0m Q 393+17  T 410  \u001b[92m☑\u001b[0m Q 635+540 T 1175 \u001b[92m☑\u001b[0m Q 553+740 T 1293 \u001b[92m☑\u001b[0m Q 16+36   T 52   \u001b[92m☑\u001b[0m Q 67+150  T 217  \u001b[92m☑\u001b[0m Q 49+99   T 148  \u001b[92m☑\u001b[0m Q 303+795 T 1098 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 88\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0016 - acc: 0.9999 - val_loss: 0.0028 - val_acc: 0.9995\n",
      "Q 60+17   T 77   \u001b[92m☑\u001b[0m Q 24+227  T 251  \u001b[92m☑\u001b[0m Q 850+81  T 931  \u001b[92m☑\u001b[0m Q 127+545 T 672  \u001b[92m☑\u001b[0m Q 56+282  T 338  \u001b[92m☑\u001b[0m Q 65+662  T 727  \u001b[92m☑\u001b[0m Q 26+725  T 751  \u001b[92m☑\u001b[0m Q 549+3   T 552  \u001b[92m☑\u001b[0m Q 963+179 T 1142 \u001b[92m☑\u001b[0m Q 875+962 T 1837 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 89\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 9.6438e-04 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9996\n",
      "Q 270+687 T 957  \u001b[92m☑\u001b[0m Q 67+456  T 523  \u001b[92m☑\u001b[0m Q 552+318 T 870  \u001b[92m☑\u001b[0m Q 161+7   T 168  \u001b[92m☑\u001b[0m Q 74+409  T 483  \u001b[92m☑\u001b[0m Q 61+78   T 139  \u001b[92m☑\u001b[0m Q 73+60   T 133  \u001b[92m☑\u001b[0m Q 73+260  T 333  \u001b[92m☑\u001b[0m Q 48+185  T 233  \u001b[92m☑\u001b[0m Q 690+91  T 781  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 90\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 7.8806e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9996\n",
      "Q 773+511 T 1284 \u001b[92m☑\u001b[0m Q 690+3   T 693  \u001b[92m☑\u001b[0m Q 52+60   T 112  \u001b[92m☑\u001b[0m Q 109+53  T 162  \u001b[92m☑\u001b[0m Q 832+81  T 913  \u001b[92m☑\u001b[0m Q 10+720  T 730  \u001b[92m☑\u001b[0m Q 96+968  T 1064 \u001b[92m☑\u001b[0m Q 1+440   T 441  \u001b[92m☑\u001b[0m Q 904+968 T 1872 \u001b[92m☑\u001b[0m Q 536+186 T 722  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 91\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 6.9665e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9994\n",
      "Q 9+725   T 734  \u001b[92m☑\u001b[0m Q 402+300 T 702  \u001b[92m☑\u001b[0m Q 166+87  T 253  \u001b[92m☑\u001b[0m Q 78+14   T 92   \u001b[92m☑\u001b[0m Q 227+914 T 1141 \u001b[92m☑\u001b[0m Q 673+48  T 721  \u001b[92m☑\u001b[0m Q 247+77  T 324  \u001b[92m☑\u001b[0m Q 7+118   T 125  \u001b[92m☑\u001b[0m Q 215+6   T 221  \u001b[92m☑\u001b[0m Q 96+948  T 1044 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 92\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 6.2176e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9995\n",
      "Q 250+299 T 549  \u001b[92m☑\u001b[0m Q 333+273 T 606  \u001b[92m☑\u001b[0m Q 8+4     T 12   \u001b[92m☑\u001b[0m Q 223+835 T 1058 \u001b[92m☑\u001b[0m Q 710+217 T 927  \u001b[92m☑\u001b[0m Q 435+7   T 442  \u001b[92m☑\u001b[0m Q 598+718 T 1316 \u001b[92m☑\u001b[0m Q 9+184   T 193  \u001b[92m☑\u001b[0m Q 36+7    T 43   \u001b[92m☑\u001b[0m Q 19+922  T 941  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 93\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 5s 101us/step - loss: 5.8933e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9995\n",
      "Q 1+518   T 519  \u001b[92m☑\u001b[0m Q 874+342 T 1216 \u001b[92m☑\u001b[0m Q 774+807 T 1581 \u001b[92m☑\u001b[0m Q 5+568   T 573  \u001b[92m☑\u001b[0m Q 88+65   T 153  \u001b[92m☑\u001b[0m Q 803+14  T 817  \u001b[92m☑\u001b[0m Q 622+814 T 1436 \u001b[92m☑\u001b[0m Q 227+603 T 830  \u001b[92m☑\u001b[0m Q 695+50  T 745  \u001b[92m☑\u001b[0m Q 880+54  T 934  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 94\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0130 - acc: 0.9964 - val_loss: 0.1569 - val_acc: 0.9524\n",
      "Q 59+463  T 522  \u001b[92m☑\u001b[0m Q 103+74  T 177  \u001b[92m☑\u001b[0m Q 526+398 T 924  \u001b[92m☑\u001b[0m Q 418+23  T 441  \u001b[92m☑\u001b[0m Q 275+11  T 286  \u001b[92m☑\u001b[0m Q 952+17  T 969  \u001b[92m☑\u001b[0m Q 82+112  T 194  \u001b[92m☑\u001b[0m Q 853+621 T 1474 \u001b[91m☒\u001b[0m Q 229+815 T 1044 \u001b[92m☑\u001b[0m Q 218+511 T 729  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 95\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0231 - acc: 0.9932 - val_loss: 0.0040 - val_acc: 0.9994\n",
      "Q 3+933   T 936  \u001b[92m☑\u001b[0m Q 498+235 T 733  \u001b[92m☑\u001b[0m Q 596+61  T 657  \u001b[92m☑\u001b[0m Q 19+476  T 495  \u001b[92m☑\u001b[0m Q 537+0   T 537  \u001b[92m☑\u001b[0m Q 2+690   T 692  \u001b[92m☑\u001b[0m Q 33+387  T 420  \u001b[92m☑\u001b[0m Q 272+211 T 483  \u001b[92m☑\u001b[0m Q 59+42   T 101  \u001b[92m☑\u001b[0m Q 5+523   T 528  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 96\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9996\n",
      "Q 491+3   T 494  \u001b[92m☑\u001b[0m Q 2+762   T 764  \u001b[92m☑\u001b[0m Q 10+829  T 839  \u001b[92m☑\u001b[0m Q 389+996 T 1385 \u001b[92m☑\u001b[0m Q 899+400 T 1299 \u001b[92m☑\u001b[0m Q 4+198   T 202  \u001b[92m☑\u001b[0m Q 8+724   T 732  \u001b[92m☑\u001b[0m Q 17+528  T 545  \u001b[92m☑\u001b[0m Q 65+153  T 218  \u001b[92m☑\u001b[0m Q 699+6   T 705  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 97\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 7.4669e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9996\n",
      "Q 234+12  T 246  \u001b[92m☑\u001b[0m Q 456+933 T 1389 \u001b[92m☑\u001b[0m Q 28+436  T 464  \u001b[92m☑\u001b[0m Q 66+985  T 1051 \u001b[92m☑\u001b[0m Q 842+6   T 848  \u001b[92m☑\u001b[0m Q 393+497 T 890  \u001b[92m☑\u001b[0m Q 789+16  T 805  \u001b[92m☑\u001b[0m Q 906+1   T 907  \u001b[92m☑\u001b[0m Q 290+629 T 919  \u001b[92m☑\u001b[0m Q 177+85  T 262  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 98\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 6.3131e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9996\n",
      "Q 38+400  T 438  \u001b[92m☑\u001b[0m Q 517+89  T 606  \u001b[92m☑\u001b[0m Q 231+11  T 242  \u001b[92m☑\u001b[0m Q 907+1   T 908  \u001b[92m☑\u001b[0m Q 72+55   T 127  \u001b[92m☑\u001b[0m Q 624+36  T 660  \u001b[92m☑\u001b[0m Q 278+473 T 751  \u001b[92m☑\u001b[0m Q 306+8   T 314  \u001b[92m☑\u001b[0m Q 803+630 T 1433 \u001b[92m☑\u001b[0m Q 80+846  T 926  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 99\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 5.6383e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9996\n",
      "Q 420+851 T 1271 \u001b[92m☑\u001b[0m Q 5+57    T 62   \u001b[92m☑\u001b[0m Q 637+5   T 642  \u001b[92m☑\u001b[0m Q 619+8   T 627  \u001b[92m☑\u001b[0m Q 585+497 T 1082 \u001b[92m☑\u001b[0m Q 27+24   T 51   \u001b[92m☑\u001b[0m Q 626+37  T 663  \u001b[92m☑\u001b[0m Q 121+29  T 150  \u001b[92m☑\u001b[0m Q 905+97  T 1002 \u001b[92m☑\u001b[0m Q 297+86  T 383  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 100\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 6.7258e-04 - acc: 0.9999 - val_loss: 0.0024 - val_acc: 0.9996\n",
      "Q 3+532   T 535  \u001b[92m☑\u001b[0m Q 278+473 T 751  \u001b[92m☑\u001b[0m Q 144+26  T 170  \u001b[92m☑\u001b[0m Q 394+63  T 457  \u001b[92m☑\u001b[0m Q 28+910  T 938  \u001b[92m☑\u001b[0m Q 695+50  T 745  \u001b[92m☑\u001b[0m Q 31+483  T 514  \u001b[92m☑\u001b[0m Q 2+972   T 974  \u001b[92m☑\u001b[0m Q 22+678  T 700  \u001b[92m☑\u001b[0m Q 98+137  T 235  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 101\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0194 - acc: 0.9940 - val_loss: 0.0466 - val_acc: 0.9830\n",
      "Q 383+355 T 738  \u001b[92m☑\u001b[0m Q 670+529 T 1199 \u001b[92m☑\u001b[0m Q 339+289 T 628  \u001b[92m☑\u001b[0m Q 13+929  T 942  \u001b[92m☑\u001b[0m Q 547+18  T 565  \u001b[92m☑\u001b[0m Q 396+81  T 477  \u001b[92m☑\u001b[0m Q 795+97  T 892  \u001b[92m☑\u001b[0m Q 78+408  T 486  \u001b[92m☑\u001b[0m Q 58+46   T 104  \u001b[92m☑\u001b[0m Q 10+872  T 882  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 102\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0196 - acc: 0.9945 - val_loss: 0.0037 - val_acc: 0.9993\n",
      "Q 235+50  T 285  \u001b[92m☑\u001b[0m Q 85+512  T 597  \u001b[92m☑\u001b[0m Q 954+631 T 1585 \u001b[92m☑\u001b[0m Q 3+933   T 936  \u001b[92m☑\u001b[0m Q 891+668 T 1559 \u001b[92m☑\u001b[0m Q 770+322 T 1092 \u001b[92m☑\u001b[0m Q 2+633   T 635  \u001b[92m☑\u001b[0m Q 328+71  T 399  \u001b[92m☑\u001b[0m Q 74+644  T 718  \u001b[92m☑\u001b[0m Q 117+46  T 163  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 103\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9996\n",
      "Q 86+429  T 515  \u001b[92m☑\u001b[0m Q 99+182  T 281  \u001b[92m☑\u001b[0m Q 892+366 T 1258 \u001b[92m☑\u001b[0m Q 605+394 T 999  \u001b[92m☑\u001b[0m Q 238+93  T 331  \u001b[92m☑\u001b[0m Q 396+81  T 477  \u001b[92m☑\u001b[0m Q 6+835   T 841  \u001b[92m☑\u001b[0m Q 61+78   T 139  \u001b[92m☑\u001b[0m Q 740+699 T 1439 \u001b[92m☑\u001b[0m Q 63+178  T 241  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 104\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 6.8956e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9996\n",
      "Q 43+45   T 88   \u001b[92m☑\u001b[0m Q 83+618  T 701  \u001b[92m☑\u001b[0m Q 544+15  T 559  \u001b[92m☑\u001b[0m Q 296+241 T 537  \u001b[92m☑\u001b[0m Q 526+72  T 598  \u001b[92m☑\u001b[0m Q 67+656  T 723  \u001b[92m☑\u001b[0m Q 104+15  T 119  \u001b[92m☑\u001b[0m Q 604+260 T 864  \u001b[92m☑\u001b[0m Q 507+867 T 1374 \u001b[92m☑\u001b[0m Q 183+61  T 244  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 105\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 5.7517e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9997\n",
      "Q 926+8   T 934  \u001b[92m☑\u001b[0m Q 901+44  T 945  \u001b[92m☑\u001b[0m Q 26+6    T 32   \u001b[92m☑\u001b[0m Q 73+707  T 780  \u001b[92m☑\u001b[0m Q 53+22   T 75   \u001b[92m☑\u001b[0m Q 41+407  T 448  \u001b[92m☑\u001b[0m Q 11+431  T 442  \u001b[92m☑\u001b[0m Q 94+821  T 915  \u001b[92m☑\u001b[0m Q 877+206 T 1083 \u001b[92m☑\u001b[0m Q 280+97  T 377  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 106\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 5.1156e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9996\n",
      "Q 40+85   T 125  \u001b[92m☑\u001b[0m Q 297+742 T 1039 \u001b[92m☑\u001b[0m Q 561+69  T 630  \u001b[92m☑\u001b[0m Q 367+58  T 425  \u001b[92m☑\u001b[0m Q 851+92  T 943  \u001b[92m☑\u001b[0m Q 63+794  T 857  \u001b[92m☑\u001b[0m Q 200+736 T 936  \u001b[92m☑\u001b[0m Q 606+67  T 673  \u001b[92m☑\u001b[0m Q 192+516 T 708  \u001b[92m☑\u001b[0m Q 563+25  T 588  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 107\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 4.6194e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Q 973+814 T 1787 \u001b[92m☑\u001b[0m Q 877+21  T 898  \u001b[92m☑\u001b[0m Q 260+213 T 473  \u001b[92m☑\u001b[0m Q 846+73  T 919  \u001b[92m☑\u001b[0m Q 351+718 T 1069 \u001b[92m☑\u001b[0m Q 70+986  T 1056 \u001b[92m☑\u001b[0m Q 34+86   T 120  \u001b[92m☑\u001b[0m Q 559+85  T 644  \u001b[92m☑\u001b[0m Q 85+605  T 690  \u001b[92m☑\u001b[0m Q 48+545  T 593  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 108\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 4.1488e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9997\n",
      "Q 83+199  T 282  \u001b[92m☑\u001b[0m Q 98+76   T 174  \u001b[92m☑\u001b[0m Q 369+476 T 845  \u001b[92m☑\u001b[0m Q 841+20  T 861  \u001b[92m☑\u001b[0m Q 1+189   T 190  \u001b[92m☑\u001b[0m Q 305+4   T 309  \u001b[92m☑\u001b[0m Q 281+884 T 1165 \u001b[92m☑\u001b[0m Q 369+61  T 430  \u001b[92m☑\u001b[0m Q 80+775  T 855  \u001b[92m☑\u001b[0m Q 441+768 T 1209 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 109\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 3.9230e-04 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 0.9994\n",
      "Q 30+170  T 200  \u001b[92m☑\u001b[0m Q 478+14  T 492  \u001b[92m☑\u001b[0m Q 665+755 T 1420 \u001b[92m☑\u001b[0m Q 672+557 T 1229 \u001b[92m☑\u001b[0m Q 57+882  T 939  \u001b[92m☑\u001b[0m Q 73+646  T 719  \u001b[92m☑\u001b[0m Q 693+649 T 1342 \u001b[92m☑\u001b[0m Q 273+63  T 336  \u001b[92m☑\u001b[0m Q 942+85  T 1027 \u001b[92m☑\u001b[0m Q 80+775  T 855  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 110\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0287 - acc: 0.9915 - val_loss: 0.0039 - val_acc: 0.9993\n",
      "Q 238+69  T 307  \u001b[92m☑\u001b[0m Q 38+704  T 742  \u001b[92m☑\u001b[0m Q 86+785  T 871  \u001b[92m☑\u001b[0m Q 441+691 T 1132 \u001b[92m☑\u001b[0m Q 904+387 T 1291 \u001b[92m☑\u001b[0m Q 3+306   T 309  \u001b[92m☑\u001b[0m Q 907+1   T 908  \u001b[92m☑\u001b[0m Q 629+741 T 1370 \u001b[92m☑\u001b[0m Q 974+41  T 1015 \u001b[92m☑\u001b[0m Q 866+6   T 872  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 111\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 0.9994\n",
      "Q 667+88  T 755  \u001b[92m☑\u001b[0m Q 873+93  T 966  \u001b[92m☑\u001b[0m Q 52+404  T 456  \u001b[92m☑\u001b[0m Q 996+75  T 1071 \u001b[92m☑\u001b[0m Q 358+814 T 1172 \u001b[92m☑\u001b[0m Q 36+40   T 76   \u001b[92m☑\u001b[0m Q 11+19   T 30   \u001b[92m☑\u001b[0m Q 888+718 T 1606 \u001b[92m☑\u001b[0m Q 2+122   T 124  \u001b[92m☑\u001b[0m Q 91+821  T 912  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 112\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 6.4008e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9996\n",
      "Q 24+744  T 768  \u001b[92m☑\u001b[0m Q 541+9   T 550  \u001b[92m☑\u001b[0m Q 73+944  T 1017 \u001b[92m☑\u001b[0m Q 88+446  T 534  \u001b[92m☑\u001b[0m Q 796+599 T 1395 \u001b[92m☑\u001b[0m Q 60+442  T 502  \u001b[92m☑\u001b[0m Q 778+137 T 915  \u001b[92m☑\u001b[0m Q 556+70  T 626  \u001b[92m☑\u001b[0m Q 91+858  T 949  \u001b[92m☑\u001b[0m Q 861+487 T 1348 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 113\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 5.0590e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9995\n",
      "Q 12+92   T 104  \u001b[92m☑\u001b[0m Q 573+696 T 1269 \u001b[92m☑\u001b[0m Q 189+74  T 263  \u001b[92m☑\u001b[0m Q 37+64   T 101  \u001b[92m☑\u001b[0m Q 335+199 T 534  \u001b[92m☑\u001b[0m Q 94+911  T 1005 \u001b[92m☑\u001b[0m Q 3+115   T 118  \u001b[92m☑\u001b[0m Q 88+15   T 103  \u001b[92m☑\u001b[0m Q 788+350 T 1138 \u001b[92m☑\u001b[0m Q 617+514 T 1131 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 114\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 4.4814e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9994\n",
      "Q 785+13  T 798  \u001b[92m☑\u001b[0m Q 94+56   T 150  \u001b[92m☑\u001b[0m Q 665+41  T 706  \u001b[92m☑\u001b[0m Q 317+750 T 1067 \u001b[92m☑\u001b[0m Q 828+488 T 1316 \u001b[92m☑\u001b[0m Q 884+55  T 939  \u001b[92m☑\u001b[0m Q 28+75   T 103  \u001b[92m☑\u001b[0m Q 4+372   T 376  \u001b[92m☑\u001b[0m Q 23+357  T 380  \u001b[92m☑\u001b[0m Q 367+58  T 425  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 115\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 3.9156e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9995\n",
      "Q 817+17  T 834  \u001b[92m☑\u001b[0m Q 812+167 T 979  \u001b[92m☑\u001b[0m Q 786+52  T 838  \u001b[92m☑\u001b[0m Q 54+167  T 221  \u001b[92m☑\u001b[0m Q 250+177 T 427  \u001b[92m☑\u001b[0m Q 43+234  T 277  \u001b[92m☑\u001b[0m Q 710+37  T 747  \u001b[92m☑\u001b[0m Q 96+80   T 176  \u001b[92m☑\u001b[0m Q 879+536 T 1415 \u001b[92m☑\u001b[0m Q 99+478  T 577  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 116\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 3.4414e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9995\n",
      "Q 215+983 T 1198 \u001b[91m☒\u001b[0m Q 17+415  T 432  \u001b[92m☑\u001b[0m Q 37+65   T 102  \u001b[92m☑\u001b[0m Q 19+693  T 712  \u001b[92m☑\u001b[0m Q 73+884  T 957  \u001b[92m☑\u001b[0m Q 429+465 T 894  \u001b[92m☑\u001b[0m Q 455+30  T 485  \u001b[92m☑\u001b[0m Q 809+767 T 1576 \u001b[92m☑\u001b[0m Q 3+36    T 39   \u001b[92m☑\u001b[0m Q 11+10   T 21   \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 117\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 3.0804e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Q 29+590  T 619  \u001b[92m☑\u001b[0m Q 768+94  T 862  \u001b[92m☑\u001b[0m Q 137+680 T 817  \u001b[92m☑\u001b[0m Q 98+484  T 582  \u001b[92m☑\u001b[0m Q 128+987 T 1115 \u001b[92m☑\u001b[0m Q 9+800   T 809  \u001b[92m☑\u001b[0m Q 152+913 T 1065 \u001b[92m☑\u001b[0m Q 710+74  T 784  \u001b[92m☑\u001b[0m Q 66+817  T 883  \u001b[92m☑\u001b[0m Q 96+853  T 949  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 118\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 2.8470e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9996\n",
      "Q 68+776  T 844  \u001b[92m☑\u001b[0m Q 72+974  T 1046 \u001b[92m☑\u001b[0m Q 96+853  T 949  \u001b[92m☑\u001b[0m Q 542+40  T 582  \u001b[92m☑\u001b[0m Q 956+3   T 959  \u001b[92m☑\u001b[0m Q 43+532  T 575  \u001b[92m☑\u001b[0m Q 280+97  T 377  \u001b[92m☑\u001b[0m Q 75+147  T 222  \u001b[92m☑\u001b[0m Q 1+518   T 519  \u001b[92m☑\u001b[0m Q 906+2   T 908  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 119\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0378 - acc: 0.9887 - val_loss: 0.0123 - val_acc: 0.9965\n",
      "Q 28+35   T 63   \u001b[92m☑\u001b[0m Q 142+34  T 176  \u001b[92m☑\u001b[0m Q 88+178  T 266  \u001b[92m☑\u001b[0m Q 414+85  T 499  \u001b[92m☑\u001b[0m Q 768+744 T 1512 \u001b[92m☑\u001b[0m Q 76+257  T 333  \u001b[92m☑\u001b[0m Q 42+91   T 133  \u001b[92m☑\u001b[0m Q 58+481  T 539  \u001b[92m☑\u001b[0m Q 959+13  T 972  \u001b[92m☑\u001b[0m Q 296+432 T 728  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 120\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0028 - acc: 0.9995 - val_loss: 0.0063 - val_acc: 0.9983\n",
      "Q 5+385   T 390  \u001b[92m☑\u001b[0m Q 975+498 T 1473 \u001b[91m☒\u001b[0m Q 714+38  T 752  \u001b[92m☑\u001b[0m Q 11+19   T 30   \u001b[92m☑\u001b[0m Q 52+25   T 77   \u001b[92m☑\u001b[0m Q 705+3   T 708  \u001b[92m☑\u001b[0m Q 91+493  T 584  \u001b[92m☑\u001b[0m Q 429+51  T 480  \u001b[92m☑\u001b[0m Q 848+894 T 1742 \u001b[92m☑\u001b[0m Q 4+600   T 604  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 121\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.0024 - val_acc: 0.9994\n",
      "Q 749+56  T 805  \u001b[92m☑\u001b[0m Q 22+849  T 871  \u001b[92m☑\u001b[0m Q 451+0   T 451  \u001b[92m☑\u001b[0m Q 765+70  T 835  \u001b[92m☑\u001b[0m Q 74+270  T 344  \u001b[92m☑\u001b[0m Q 468+2   T 470  \u001b[92m☑\u001b[0m Q 999+63  T 1062 \u001b[92m☑\u001b[0m Q 725+121 T 846  \u001b[92m☑\u001b[0m Q 264+68  T 332  \u001b[92m☑\u001b[0m Q 1+288   T 289  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 122\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 5.1667e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9994\n",
      "Q 798+805 T 1603 \u001b[92m☑\u001b[0m Q 274+52  T 326  \u001b[92m☑\u001b[0m Q 964+14  T 978  \u001b[92m☑\u001b[0m Q 271+301 T 572  \u001b[92m☑\u001b[0m Q 362+899 T 1261 \u001b[92m☑\u001b[0m Q 0+158   T 158  \u001b[92m☑\u001b[0m Q 606+7   T 613  \u001b[92m☑\u001b[0m Q 18+94   T 112  \u001b[92m☑\u001b[0m Q 38+704  T 742  \u001b[92m☑\u001b[0m Q 8+398   T 406  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 123\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 4.2753e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9996\n",
      "Q 994+91  T 1085 \u001b[92m☑\u001b[0m Q 82+149  T 231  \u001b[92m☑\u001b[0m Q 9+105   T 114  \u001b[92m☑\u001b[0m Q 5+276   T 281  \u001b[92m☑\u001b[0m Q 629+741 T 1370 \u001b[92m☑\u001b[0m Q 57+98   T 155  \u001b[92m☑\u001b[0m Q 50+81   T 131  \u001b[92m☑\u001b[0m Q 13+9    T 22   \u001b[92m☑\u001b[0m Q 6+48    T 54   \u001b[92m☑\u001b[0m Q 37+363  T 400  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 124\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 5s 101us/step - loss: 3.7567e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9997\n",
      "Q 463+497 T 960  \u001b[92m☑\u001b[0m Q 620+5   T 625  \u001b[92m☑\u001b[0m Q 804+0   T 804  \u001b[92m☑\u001b[0m Q 73+353  T 426  \u001b[92m☑\u001b[0m Q 63+43   T 106  \u001b[92m☑\u001b[0m Q 86+155  T 241  \u001b[92m☑\u001b[0m Q 27+1    T 28   \u001b[92m☑\u001b[0m Q 978+27  T 1005 \u001b[92m☑\u001b[0m Q 400+597 T 997  \u001b[92m☑\u001b[0m Q 85+605  T 690  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 125\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 3.3243e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9997\n",
      "Q 9+800   T 809  \u001b[92m☑\u001b[0m Q 772+860 T 1632 \u001b[92m☑\u001b[0m Q 123+87  T 210  \u001b[92m☑\u001b[0m Q 91+605  T 696  \u001b[92m☑\u001b[0m Q 14+10   T 24   \u001b[92m☑\u001b[0m Q 25+960  T 985  \u001b[92m☑\u001b[0m Q 410+37  T 447  \u001b[92m☑\u001b[0m Q 88+85   T 173  \u001b[92m☑\u001b[0m Q 435+34  T 469  \u001b[92m☑\u001b[0m Q 961+828 T 1789 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 126\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 4.9742e-04 - acc: 0.9999 - val_loss: 0.0023 - val_acc: 0.9994\n",
      "Q 952+2   T 954  \u001b[92m☑\u001b[0m Q 423+491 T 914  \u001b[92m☑\u001b[0m Q 127+153 T 280  \u001b[92m☑\u001b[0m Q 79+18   T 97   \u001b[92m☑\u001b[0m Q 46+911  T 957  \u001b[92m☑\u001b[0m Q 716+915 T 1631 \u001b[92m☑\u001b[0m Q 43+732  T 775  \u001b[92m☑\u001b[0m Q 869+778 T 1647 \u001b[92m☑\u001b[0m Q 93+266  T 359  \u001b[92m☑\u001b[0m Q 485+7   T 492  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 127\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 6.2542e-04 - acc: 0.9999 - val_loss: 0.0038 - val_acc: 0.9990\n",
      "Q 869+272 T 1141 \u001b[92m☑\u001b[0m Q 130+42  T 172  \u001b[92m☑\u001b[0m Q 556+45  T 601  \u001b[92m☑\u001b[0m Q 68+733  T 801  \u001b[92m☑\u001b[0m Q 918+778 T 1696 \u001b[92m☑\u001b[0m Q 23+123  T 146  \u001b[92m☑\u001b[0m Q 638+311 T 949  \u001b[92m☑\u001b[0m Q 173+174 T 347  \u001b[92m☑\u001b[0m Q 40+218  T 258  \u001b[92m☑\u001b[0m Q 2+122   T 124  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 128\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0293 - acc: 0.9918 - val_loss: 0.0051 - val_acc: 0.9987\n",
      "Q 974+7   T 981  \u001b[92m☑\u001b[0m Q 79+465  T 544  \u001b[92m☑\u001b[0m Q 721+317 T 1038 \u001b[92m☑\u001b[0m Q 82+295  T 377  \u001b[92m☑\u001b[0m Q 144+921 T 1065 \u001b[92m☑\u001b[0m Q 392+138 T 530  \u001b[92m☑\u001b[0m Q 35+634  T 669  \u001b[92m☑\u001b[0m Q 60+236  T 296  \u001b[92m☑\u001b[0m Q 551+9   T 560  \u001b[92m☑\u001b[0m Q 958+56  T 1014 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 129\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 8.4501e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 0.9995\n",
      "Q 86+70   T 156  \u001b[92m☑\u001b[0m Q 71+109  T 180  \u001b[92m☑\u001b[0m Q 157+977 T 1134 \u001b[92m☑\u001b[0m Q 4+688   T 692  \u001b[92m☑\u001b[0m Q 57+98   T 155  \u001b[92m☑\u001b[0m Q 349+588 T 937  \u001b[92m☑\u001b[0m Q 997+33  T 1030 \u001b[92m☑\u001b[0m Q 457+3   T 460  \u001b[92m☑\u001b[0m Q 534+821 T 1355 \u001b[92m☑\u001b[0m Q 49+975  T 1024 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 130\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 4.9240e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9994\n",
      "Q 469+789 T 1258 \u001b[92m☑\u001b[0m Q 623+7   T 630  \u001b[92m☑\u001b[0m Q 830+48  T 878  \u001b[92m☑\u001b[0m Q 8+993   T 1001 \u001b[92m☑\u001b[0m Q 679+407 T 1086 \u001b[92m☑\u001b[0m Q 470+8   T 478  \u001b[92m☑\u001b[0m Q 5+39    T 44   \u001b[92m☑\u001b[0m Q 423+491 T 914  \u001b[92m☑\u001b[0m Q 601+120 T 721  \u001b[92m☑\u001b[0m Q 99+55   T 154  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 131\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 4.0165e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9995\n",
      "Q 497+783 T 1280 \u001b[92m☑\u001b[0m Q 91+605  T 696  \u001b[92m☑\u001b[0m Q 433+703 T 1136 \u001b[92m☑\u001b[0m Q 21+26   T 47   \u001b[92m☑\u001b[0m Q 594+87  T 681  \u001b[92m☑\u001b[0m Q 843+779 T 1622 \u001b[92m☑\u001b[0m Q 19+189  T 208  \u001b[92m☑\u001b[0m Q 851+4   T 855  \u001b[92m☑\u001b[0m Q 549+44  T 593  \u001b[92m☑\u001b[0m Q 110+425 T 535  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 132\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 3.4588e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9995\n",
      "Q 12+995  T 1007 \u001b[92m☑\u001b[0m Q 627+89  T 716  \u001b[92m☑\u001b[0m Q 298+99  T 397  \u001b[91m☒\u001b[0m Q 551+74  T 625  \u001b[92m☑\u001b[0m Q 726+672 T 1398 \u001b[92m☑\u001b[0m Q 837+492 T 1329 \u001b[92m☑\u001b[0m Q 858+44  T 902  \u001b[92m☑\u001b[0m Q 57+707  T 764  \u001b[92m☑\u001b[0m Q 126+649 T 775  \u001b[92m☑\u001b[0m Q 972+72  T 1044 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 133\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 3.0550e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Q 632+0   T 632  \u001b[92m☑\u001b[0m Q 84+11   T 95   \u001b[92m☑\u001b[0m Q 411+89  T 500  \u001b[92m☑\u001b[0m Q 117+46  T 163  \u001b[92m☑\u001b[0m Q 692+307 T 999  \u001b[92m☑\u001b[0m Q 196+0   T 196  \u001b[92m☑\u001b[0m Q 700+533 T 1233 \u001b[92m☑\u001b[0m Q 825+3   T 828  \u001b[92m☑\u001b[0m Q 737+67  T 804  \u001b[92m☑\u001b[0m Q 104+15  T 119  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 134\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.7152e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "Q 460+902 T 1362 \u001b[92m☑\u001b[0m Q 355+32  T 387  \u001b[92m☑\u001b[0m Q 99+946  T 1045 \u001b[92m☑\u001b[0m Q 481+33  T 514  \u001b[92m☑\u001b[0m Q 74+91   T 165  \u001b[92m☑\u001b[0m Q 581+2   T 583  \u001b[92m☑\u001b[0m Q 623+7   T 630  \u001b[92m☑\u001b[0m Q 112+325 T 437  \u001b[92m☑\u001b[0m Q 594+2   T 596  \u001b[92m☑\u001b[0m Q 310+685 T 995  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 135\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.4669e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9996\n",
      "Q 3+125   T 128  \u001b[92m☑\u001b[0m Q 58+491  T 549  \u001b[92m☑\u001b[0m Q 98+484  T 582  \u001b[92m☑\u001b[0m Q 596+483 T 1079 \u001b[92m☑\u001b[0m Q 755+30  T 785  \u001b[92m☑\u001b[0m Q 462+5   T 467  \u001b[92m☑\u001b[0m Q 334+929 T 1263 \u001b[92m☑\u001b[0m Q 451+7   T 458  \u001b[92m☑\u001b[0m Q 45+11   T 56   \u001b[92m☑\u001b[0m Q 12+475  T 487  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 136\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.2055e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Q 575+86  T 661  \u001b[92m☑\u001b[0m Q 402+300 T 702  \u001b[92m☑\u001b[0m Q 82+20   T 102  \u001b[92m☑\u001b[0m Q 9+166   T 175  \u001b[92m☑\u001b[0m Q 669+94  T 763  \u001b[92m☑\u001b[0m Q 7+753   T 760  \u001b[92m☑\u001b[0m Q 25+825  T 850  \u001b[92m☑\u001b[0m Q 9+47    T 56   \u001b[92m☑\u001b[0m Q 211+2   T 213  \u001b[92m☑\u001b[0m Q 208+65  T 273  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 137\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.0157e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9997\n",
      "Q 390+7   T 397  \u001b[92m☑\u001b[0m Q 67+488  T 555  \u001b[92m☑\u001b[0m Q 411+89  T 500  \u001b[92m☑\u001b[0m Q 295+8   T 303  \u001b[92m☑\u001b[0m Q 750+136 T 886  \u001b[92m☑\u001b[0m Q 420+417 T 837  \u001b[92m☑\u001b[0m Q 17+216  T 233  \u001b[92m☑\u001b[0m Q 7+655   T 662  \u001b[92m☑\u001b[0m Q 667+495 T 1162 \u001b[92m☑\u001b[0m Q 3+306   T 309  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 138\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0317 - acc: 0.9914 - val_loss: 0.0231 - val_acc: 0.9933\n",
      "Q 2+394   T 396  \u001b[92m☑\u001b[0m Q 770+26  T 796  \u001b[92m☑\u001b[0m Q 6+587   T 593  \u001b[92m☑\u001b[0m Q 67+972  T 1039 \u001b[92m☑\u001b[0m Q 227+586 T 813  \u001b[92m☑\u001b[0m Q 961+2   T 963  \u001b[92m☑\u001b[0m Q 924+45  T 969  \u001b[92m☑\u001b[0m Q 493+59  T 552  \u001b[92m☑\u001b[0m Q 5+796   T 801  \u001b[92m☑\u001b[0m Q 70+407  T 477  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 139\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.0022 - val_acc: 0.9996\n",
      "Q 330+717 T 1047 \u001b[92m☑\u001b[0m Q 258+13  T 271  \u001b[92m☑\u001b[0m Q 656+72  T 728  \u001b[92m☑\u001b[0m Q 367+56  T 423  \u001b[92m☑\u001b[0m Q 82+911  T 993  \u001b[92m☑\u001b[0m Q 732+55  T 787  \u001b[92m☑\u001b[0m Q 278+479 T 757  \u001b[92m☑\u001b[0m Q 352+167 T 519  \u001b[92m☑\u001b[0m Q 758+271 T 1029 \u001b[92m☑\u001b[0m Q 141+26  T 167  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 140\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 5s 100us/step - loss: 5.7683e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9996\n",
      "Q 275+6   T 281  \u001b[92m☑\u001b[0m Q 72+55   T 127  \u001b[92m☑\u001b[0m Q 453+44  T 497  \u001b[92m☑\u001b[0m Q 12+392  T 404  \u001b[92m☑\u001b[0m Q 30+302  T 332  \u001b[92m☑\u001b[0m Q 350+496 T 846  \u001b[92m☑\u001b[0m Q 588+367 T 955  \u001b[92m☑\u001b[0m Q 81+308  T 389  \u001b[92m☑\u001b[0m Q 300+58  T 358  \u001b[92m☑\u001b[0m Q 669+409 T 1078 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 141\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 6.4655e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Q 53+22   T 75   \u001b[92m☑\u001b[0m Q 752+261 T 1013 \u001b[92m☑\u001b[0m Q 679+3   T 682  \u001b[92m☑\u001b[0m Q 650+998 T 1648 \u001b[92m☑\u001b[0m Q 679+192 T 871  \u001b[92m☑\u001b[0m Q 863+601 T 1464 \u001b[92m☑\u001b[0m Q 3+94    T 97   \u001b[92m☑\u001b[0m Q 964+110 T 1074 \u001b[92m☑\u001b[0m Q 171+23  T 194  \u001b[92m☑\u001b[0m Q 107+50  T 157  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 142\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 3.5080e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9997\n",
      "Q 442+66  T 508  \u001b[92m☑\u001b[0m Q 215+6   T 221  \u001b[92m☑\u001b[0m Q 779+50  T 829  \u001b[92m☑\u001b[0m Q 585+973 T 1558 \u001b[92m☑\u001b[0m Q 340+924 T 1264 \u001b[92m☑\u001b[0m Q 906+1   T 907  \u001b[92m☑\u001b[0m Q 48+23   T 71   \u001b[92m☑\u001b[0m Q 3+36    T 39   \u001b[92m☑\u001b[0m Q 742+311 T 1053 \u001b[92m☑\u001b[0m Q 375+515 T 890  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 143\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.9650e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Q 892+228 T 1120 \u001b[92m☑\u001b[0m Q 902+7   T 909  \u001b[92m☑\u001b[0m Q 640+23  T 663  \u001b[92m☑\u001b[0m Q 68+988  T 1056 \u001b[92m☑\u001b[0m Q 11+957  T 968  \u001b[92m☑\u001b[0m Q 615+741 T 1356 \u001b[92m☑\u001b[0m Q 691+822 T 1513 \u001b[92m☑\u001b[0m Q 89+410  T 499  \u001b[92m☑\u001b[0m Q 665+41  T 706  \u001b[92m☑\u001b[0m Q 603+610 T 1213 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 144\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.6126e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9997\n",
      "Q 178+70  T 248  \u001b[92m☑\u001b[0m Q 55+342  T 397  \u001b[92m☑\u001b[0m Q 598+718 T 1316 \u001b[92m☑\u001b[0m Q 3+75    T 78   \u001b[92m☑\u001b[0m Q 572+634 T 1206 \u001b[92m☑\u001b[0m Q 89+576  T 665  \u001b[92m☑\u001b[0m Q 71+298  T 369  \u001b[92m☑\u001b[0m Q 379+233 T 612  \u001b[92m☑\u001b[0m Q 80+52   T 132  \u001b[92m☑\u001b[0m Q 9+811   T 820  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 145\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.3367e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Q 75+959  T 1034 \u001b[92m☑\u001b[0m Q 83+77   T 160  \u001b[92m☑\u001b[0m Q 146+67  T 213  \u001b[92m☑\u001b[0m Q 52+567  T 619  \u001b[92m☑\u001b[0m Q 214+57  T 271  \u001b[92m☑\u001b[0m Q 40+85   T 125  \u001b[92m☑\u001b[0m Q 66+8    T 74   \u001b[92m☑\u001b[0m Q 29+14   T 43   \u001b[92m☑\u001b[0m Q 42+406  T 448  \u001b[92m☑\u001b[0m Q 926+8   T 934  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 146\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.1029e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9997\n",
      "Q 86+35   T 121  \u001b[92m☑\u001b[0m Q 184+81  T 265  \u001b[92m☑\u001b[0m Q 50+199  T 249  \u001b[92m☑\u001b[0m Q 93+78   T 171  \u001b[92m☑\u001b[0m Q 30+990  T 1020 \u001b[92m☑\u001b[0m Q 89+12   T 101  \u001b[92m☑\u001b[0m Q 9+277   T 286  \u001b[92m☑\u001b[0m Q 672+557 T 1229 \u001b[92m☑\u001b[0m Q 575+71  T 646  \u001b[92m☑\u001b[0m Q 66+714  T 780  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 147\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.9082e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n",
      "Q 203+91  T 294  \u001b[92m☑\u001b[0m Q 423+491 T 914  \u001b[92m☑\u001b[0m Q 621+556 T 1177 \u001b[92m☑\u001b[0m Q 13+315  T 328  \u001b[92m☑\u001b[0m Q 95+164  T 259  \u001b[92m☑\u001b[0m Q 1+288   T 289  \u001b[92m☑\u001b[0m Q 96+926  T 1022 \u001b[92m☑\u001b[0m Q 954+631 T 1585 \u001b[92m☑\u001b[0m Q 75+147  T 222  \u001b[92m☑\u001b[0m Q 16+597  T 613  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 148\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.7249e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9998\n",
      "Q 35+84   T 119  \u001b[92m☑\u001b[0m Q 83+558  T 641  \u001b[92m☑\u001b[0m Q 222+42  T 264  \u001b[92m☑\u001b[0m Q 566+394 T 960  \u001b[92m☑\u001b[0m Q 187+3   T 190  \u001b[92m☑\u001b[0m Q 162+98  T 260  \u001b[92m☑\u001b[0m Q 75+42   T 117  \u001b[92m☑\u001b[0m Q 57+707  T 764  \u001b[92m☑\u001b[0m Q 715+798 T 1513 \u001b[92m☑\u001b[0m Q 842+6   T 848  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 149\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.6317e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9997\n",
      "Q 911+5   T 916  \u001b[92m☑\u001b[0m Q 6+641   T 647  \u001b[92m☑\u001b[0m Q 741+3   T 744  \u001b[92m☑\u001b[0m Q 96+152  T 248  \u001b[92m☑\u001b[0m Q 853+621 T 1474 \u001b[92m☑\u001b[0m Q 292+24  T 316  \u001b[92m☑\u001b[0m Q 304+352 T 656  \u001b[92m☑\u001b[0m Q 25+637  T 662  \u001b[92m☑\u001b[0m Q 363+633 T 996  \u001b[92m☑\u001b[0m Q 29+286  T 315  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 150\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0295 - acc: 0.9920 - val_loss: 0.0048 - val_acc: 0.9990\n",
      "Q 39+222  T 261  \u001b[92m☑\u001b[0m Q 208+443 T 651  \u001b[92m☑\u001b[0m Q 999+63  T 1062 \u001b[92m☑\u001b[0m Q 682+74  T 756  \u001b[92m☑\u001b[0m Q 605+80  T 685  \u001b[92m☑\u001b[0m Q 468+2   T 470  \u001b[92m☑\u001b[0m Q 426+351 T 777  \u001b[92m☑\u001b[0m Q 87+328  T 415  \u001b[92m☑\u001b[0m Q 462+5   T 467  \u001b[92m☑\u001b[0m Q 321+10  T 331  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 151\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0024 - val_acc: 0.9993\n",
      "Q 188+783 T 971  \u001b[92m☑\u001b[0m Q 68+990  T 1058 \u001b[92m☑\u001b[0m Q 802+75  T 877  \u001b[92m☑\u001b[0m Q 94+821  T 915  \u001b[92m☑\u001b[0m Q 210+854 T 1064 \u001b[92m☑\u001b[0m Q 553+740 T 1293 \u001b[92m☑\u001b[0m Q 3+326   T 329  \u001b[92m☑\u001b[0m Q 448+0   T 448  \u001b[92m☑\u001b[0m Q 65+51   T 116  \u001b[92m☑\u001b[0m Q 59+578  T 637  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 152\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 4.1194e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9994\n",
      "Q 632+0   T 632  \u001b[92m☑\u001b[0m Q 50+900  T 950  \u001b[92m☑\u001b[0m Q 633+38  T 671  \u001b[92m☑\u001b[0m Q 54+629  T 683  \u001b[92m☑\u001b[0m Q 871+46  T 917  \u001b[92m☑\u001b[0m Q 266+637 T 903  \u001b[92m☑\u001b[0m Q 22+18   T 40   \u001b[92m☑\u001b[0m Q 764+117 T 881  \u001b[92m☑\u001b[0m Q 39+557  T 596  \u001b[92m☑\u001b[0m Q 75+975  T 1050 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 153\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 3.1279e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9995\n",
      "Q 753+348 T 1101 \u001b[92m☑\u001b[0m Q 73+325  T 398  \u001b[92m☑\u001b[0m Q 9+672   T 681  \u001b[92m☑\u001b[0m Q 256+68  T 324  \u001b[92m☑\u001b[0m Q 72+974  T 1046 \u001b[92m☑\u001b[0m Q 501+47  T 548  \u001b[92m☑\u001b[0m Q 994+9   T 1003 \u001b[92m☑\u001b[0m Q 216+46  T 262  \u001b[92m☑\u001b[0m Q 6+113   T 119  \u001b[92m☑\u001b[0m Q 906+486 T 1392 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 154\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.6716e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Q 847+358 T 1205 \u001b[92m☑\u001b[0m Q 95+565  T 660  \u001b[92m☑\u001b[0m Q 336+827 T 1163 \u001b[92m☑\u001b[0m Q 986+74  T 1060 \u001b[92m☑\u001b[0m Q 832+81  T 913  \u001b[92m☑\u001b[0m Q 12+535  T 547  \u001b[92m☑\u001b[0m Q 164+0   T 164  \u001b[92m☑\u001b[0m Q 94+911  T 1005 \u001b[92m☑\u001b[0m Q 38+834  T 872  \u001b[92m☑\u001b[0m Q 736+99  T 835  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 155\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.3419e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9996\n",
      "Q 73+669  T 742  \u001b[92m☑\u001b[0m Q 30+769  T 799  \u001b[92m☑\u001b[0m Q 91+36   T 127  \u001b[92m☑\u001b[0m Q 17+189  T 206  \u001b[92m☑\u001b[0m Q 846+79  T 925  \u001b[92m☑\u001b[0m Q 842+60  T 902  \u001b[92m☑\u001b[0m Q 151+35  T 186  \u001b[92m☑\u001b[0m Q 152+913 T 1065 \u001b[92m☑\u001b[0m Q 35+25   T 60   \u001b[92m☑\u001b[0m Q 77+29   T 106  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 156\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.0895e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9996\n",
      "Q 837+664 T 1501 \u001b[92m☑\u001b[0m Q 6+150   T 156  \u001b[92m☑\u001b[0m Q 90+901  T 991  \u001b[92m☑\u001b[0m Q 2+424   T 426  \u001b[92m☑\u001b[0m Q 113+351 T 464  \u001b[92m☑\u001b[0m Q 6+521   T 527  \u001b[92m☑\u001b[0m Q 178+80  T 258  \u001b[92m☑\u001b[0m Q 87+35   T 122  \u001b[92m☑\u001b[0m Q 290+37  T 327  \u001b[92m☑\u001b[0m Q 3+983   T 986  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 157\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.8735e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "Q 583+62  T 645  \u001b[92m☑\u001b[0m Q 269+860 T 1129 \u001b[92m☑\u001b[0m Q 690+91  T 781  \u001b[92m☑\u001b[0m Q 8+712   T 720  \u001b[92m☑\u001b[0m Q 72+923  T 995  \u001b[92m☑\u001b[0m Q 78+874  T 952  \u001b[92m☑\u001b[0m Q 976+652 T 1628 \u001b[92m☑\u001b[0m Q 221+46  T 267  \u001b[92m☑\u001b[0m Q 888+5   T 893  \u001b[92m☑\u001b[0m Q 99+946  T 1045 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 158\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.7021e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Q 868+654 T 1522 \u001b[92m☑\u001b[0m Q 9+418   T 427  \u001b[92m☑\u001b[0m Q 70+197  T 267  \u001b[92m☑\u001b[0m Q 270+390 T 660  \u001b[92m☑\u001b[0m Q 789+16  T 805  \u001b[92m☑\u001b[0m Q 148+33  T 181  \u001b[92m☑\u001b[0m Q 11+49   T 60   \u001b[92m☑\u001b[0m Q 66+714  T 780  \u001b[92m☑\u001b[0m Q 9+811   T 820  \u001b[92m☑\u001b[0m Q 714+38  T 752  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 159\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0305 - acc: 0.9915 - val_loss: 0.0292 - val_acc: 0.9900\n",
      "Q 441+98  T 539  \u001b[92m☑\u001b[0m Q 63+618  T 681  \u001b[92m☑\u001b[0m Q 549+44  T 593  \u001b[92m☑\u001b[0m Q 429+70  T 499  \u001b[92m☑\u001b[0m Q 686+753 T 1439 \u001b[92m☑\u001b[0m Q 450+733 T 1183 \u001b[92m☑\u001b[0m Q 296+241 T 537  \u001b[92m☑\u001b[0m Q 39+227  T 266  \u001b[92m☑\u001b[0m Q 42+91   T 133  \u001b[92m☑\u001b[0m Q 22+891  T 913  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 160\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 0.0052 - acc: 0.9987 - val_loss: 0.0048 - val_acc: 0.9984\n",
      "Q 201+66  T 267  \u001b[92m☑\u001b[0m Q 306+78  T 384  \u001b[92m☑\u001b[0m Q 306+78  T 384  \u001b[92m☑\u001b[0m Q 54+412  T 466  \u001b[92m☑\u001b[0m Q 853+395 T 1248 \u001b[92m☑\u001b[0m Q 459+55  T 514  \u001b[92m☑\u001b[0m Q 13+98   T 111  \u001b[92m☑\u001b[0m Q 94+472  T 566  \u001b[92m☑\u001b[0m Q 521+434 T 955  \u001b[92m☑\u001b[0m Q 85+175  T 260  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 161\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 6.2316e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9997\n",
      "Q 230+590 T 820  \u001b[92m☑\u001b[0m Q 316+382 T 698  \u001b[92m☑\u001b[0m Q 2+394   T 396  \u001b[92m☑\u001b[0m Q 603+848 T 1451 \u001b[92m☑\u001b[0m Q 597+375 T 972  \u001b[92m☑\u001b[0m Q 76+141  T 217  \u001b[92m☑\u001b[0m Q 13+98   T 111  \u001b[92m☑\u001b[0m Q 50+35   T 85   \u001b[92m☑\u001b[0m Q 473+468 T 941  \u001b[92m☑\u001b[0m Q 879+211 T 1090 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 162\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 3.7668e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9997\n",
      "Q 40+514  T 554  \u001b[92m☑\u001b[0m Q 116+187 T 303  \u001b[92m☑\u001b[0m Q 169+891 T 1060 \u001b[92m☑\u001b[0m Q 491+3   T 494  \u001b[92m☑\u001b[0m Q 21+323  T 344  \u001b[92m☑\u001b[0m Q 451+52  T 503  \u001b[92m☑\u001b[0m Q 57+98   T 155  \u001b[92m☑\u001b[0m Q 967+7   T 974  \u001b[92m☑\u001b[0m Q 799+539 T 1338 \u001b[92m☑\u001b[0m Q 117+923 T 1040 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 163\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 3.0262e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9997\n",
      "Q 21+837  T 858  \u001b[92m☑\u001b[0m Q 557+0   T 557  \u001b[92m☑\u001b[0m Q 130+89  T 219  \u001b[92m☑\u001b[0m Q 72+842  T 914  \u001b[92m☑\u001b[0m Q 5+333   T 338  \u001b[92m☑\u001b[0m Q 94+827  T 921  \u001b[92m☑\u001b[0m Q 640+23  T 663  \u001b[92m☑\u001b[0m Q 760+157 T 917  \u001b[92m☑\u001b[0m Q 177+3   T 180  \u001b[92m☑\u001b[0m Q 1+627   T 628  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 164\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.5776e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9997\n",
      "Q 900+393 T 1293 \u001b[92m☑\u001b[0m Q 841+98  T 939  \u001b[92m☑\u001b[0m Q 61+272  T 333  \u001b[92m☑\u001b[0m Q 668+89  T 757  \u001b[92m☑\u001b[0m Q 685+99  T 784  \u001b[92m☑\u001b[0m Q 9+329   T 338  \u001b[92m☑\u001b[0m Q 88+238  T 326  \u001b[92m☑\u001b[0m Q 661+1   T 662  \u001b[92m☑\u001b[0m Q 708+304 T 1012 \u001b[92m☑\u001b[0m Q 73+201  T 274  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 165\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.2600e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Q 673+5   T 678  \u001b[92m☑\u001b[0m Q 923+235 T 1158 \u001b[92m☑\u001b[0m Q 916+632 T 1548 \u001b[92m☑\u001b[0m Q 746+48  T 794  \u001b[92m☑\u001b[0m Q 4+420   T 424  \u001b[92m☑\u001b[0m Q 12+439  T 451  \u001b[92m☑\u001b[0m Q 830+538 T 1368 \u001b[92m☑\u001b[0m Q 78+14   T 92   \u001b[92m☑\u001b[0m Q 53+92   T 145  \u001b[92m☑\u001b[0m Q 993+894 T 1887 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 166\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.0151e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Q 447+331 T 778  \u001b[92m☑\u001b[0m Q 92+51   T 143  \u001b[92m☑\u001b[0m Q 388+74  T 462  \u001b[92m☑\u001b[0m Q 307+2   T 309  \u001b[92m☑\u001b[0m Q 123+71  T 194  \u001b[92m☑\u001b[0m Q 257+75  T 332  \u001b[92m☑\u001b[0m Q 885+838 T 1723 \u001b[92m☑\u001b[0m Q 259+60  T 319  \u001b[92m☑\u001b[0m Q 77+871  T 948  \u001b[92m☑\u001b[0m Q 860+435 T 1295 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 167\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.7869e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9997\n",
      "Q 23+275  T 298  \u001b[92m☑\u001b[0m Q 75+824  T 899  \u001b[92m☑\u001b[0m Q 227+72  T 299  \u001b[92m☑\u001b[0m Q 460+77  T 537  \u001b[92m☑\u001b[0m Q 67+174  T 241  \u001b[92m☑\u001b[0m Q 972+52  T 1024 \u001b[92m☑\u001b[0m Q 75+42   T 117  \u001b[92m☑\u001b[0m Q 39+680  T 719  \u001b[92m☑\u001b[0m Q 42+582  T 624  \u001b[92m☑\u001b[0m Q 578+91  T 669  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 168\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.6104e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n",
      "Q 960+69  T 1029 \u001b[92m☑\u001b[0m Q 686+3   T 689  \u001b[92m☑\u001b[0m Q 113+81  T 194  \u001b[92m☑\u001b[0m Q 877+11  T 888  \u001b[92m☑\u001b[0m Q 207+90  T 297  \u001b[92m☑\u001b[0m Q 13+495  T 508  \u001b[92m☑\u001b[0m Q 272+211 T 483  \u001b[92m☑\u001b[0m Q 60+178  T 238  \u001b[92m☑\u001b[0m Q 971+20  T 991  \u001b[92m☑\u001b[0m Q 75+727  T 802  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 169\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 1.5625e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9995\n",
      "Q 484+17  T 501  \u001b[92m☑\u001b[0m Q 80+52   T 132  \u001b[92m☑\u001b[0m Q 905+98  T 1003 \u001b[92m☑\u001b[0m Q 90+83   T 173  \u001b[92m☑\u001b[0m Q 28+35   T 63   \u001b[92m☑\u001b[0m Q 549+84  T 633  \u001b[92m☑\u001b[0m Q 409+69  T 478  \u001b[92m☑\u001b[0m Q 181+43  T 224  \u001b[92m☑\u001b[0m Q 24+84   T 108  \u001b[92m☑\u001b[0m Q 686+361 T 1047 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 170\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0295 - acc: 0.9912 - val_loss: 0.0032 - val_acc: 0.9991\n",
      "Q 856+36  T 892  \u001b[92m☑\u001b[0m Q 9+277   T 286  \u001b[92m☑\u001b[0m Q 481+54  T 535  \u001b[92m☑\u001b[0m Q 597+375 T 972  \u001b[92m☑\u001b[0m Q 48+545  T 593  \u001b[92m☑\u001b[0m Q 405+879 T 1284 \u001b[92m☑\u001b[0m Q 154+551 T 705  \u001b[92m☑\u001b[0m Q 7+99    T 106  \u001b[92m☑\u001b[0m Q 74+602  T 676  \u001b[92m☑\u001b[0m Q 352+98  T 450  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 171\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 9.2295e-04 - acc: 0.9999 - val_loss: 0.0020 - val_acc: 0.9996\n",
      "Q 64+607  T 671  \u001b[92m☑\u001b[0m Q 457+8   T 465  \u001b[92m☑\u001b[0m Q 75+727  T 802  \u001b[92m☑\u001b[0m Q 418+23  T 441  \u001b[92m☑\u001b[0m Q 891+33  T 924  \u001b[92m☑\u001b[0m Q 71+97   T 168  \u001b[92m☑\u001b[0m Q 842+4   T 846  \u001b[92m☑\u001b[0m Q 497+970 T 1467 \u001b[92m☑\u001b[0m Q 396+758 T 1154 \u001b[92m☑\u001b[0m Q 46+72   T 118  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 172\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 4s 100us/step - loss: 4.1313e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9996\n",
      "Q 32+237  T 269  \u001b[92m☑\u001b[0m Q 34+254  T 288  \u001b[92m☑\u001b[0m Q 15+35   T 50   \u001b[92m☑\u001b[0m Q 859+814 T 1673 \u001b[92m☑\u001b[0m Q 24+620  T 644  \u001b[92m☑\u001b[0m Q 782+79  T 861  \u001b[92m☑\u001b[0m Q 183+61  T 244  \u001b[92m☑\u001b[0m Q 488+23  T 511  \u001b[92m☑\u001b[0m Q 679+713 T 1392 \u001b[92m☑\u001b[0m Q 520+224 T 744  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 173\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 3.0820e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9997\n",
      "Q 564+696 T 1260 \u001b[92m☑\u001b[0m Q 12+475  T 487  \u001b[92m☑\u001b[0m Q 802+3   T 805  \u001b[92m☑\u001b[0m Q 528+116 T 644  \u001b[92m☑\u001b[0m Q 35+342  T 377  \u001b[92m☑\u001b[0m Q 75+330  T 405  \u001b[92m☑\u001b[0m Q 509+36  T 545  \u001b[92m☑\u001b[0m Q 624+76  T 700  \u001b[92m☑\u001b[0m Q 90+669  T 759  \u001b[92m☑\u001b[0m Q 4+346   T 350  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 174\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.5728e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Q 30+641  T 671  \u001b[92m☑\u001b[0m Q 861+3   T 864  \u001b[92m☑\u001b[0m Q 560+688 T 1248 \u001b[92m☑\u001b[0m Q 331+14  T 345  \u001b[92m☑\u001b[0m Q 712+49  T 761  \u001b[92m☑\u001b[0m Q 737+77  T 814  \u001b[92m☑\u001b[0m Q 994+5   T 999  \u001b[92m☑\u001b[0m Q 456+6   T 462  \u001b[92m☑\u001b[0m Q 71+938  T 1009 \u001b[92m☑\u001b[0m Q 819+144 T 963  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 175\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.2247e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Q 29+590  T 619  \u001b[92m☑\u001b[0m Q 288+0   T 288  \u001b[92m☑\u001b[0m Q 43+892  T 935  \u001b[92m☑\u001b[0m Q 497+233 T 730  \u001b[92m☑\u001b[0m Q 488+46  T 534  \u001b[92m☑\u001b[0m Q 73+40   T 113  \u001b[92m☑\u001b[0m Q 78+0    T 78   \u001b[92m☑\u001b[0m Q 101+61  T 162  \u001b[92m☑\u001b[0m Q 340+80  T 420  \u001b[92m☑\u001b[0m Q 79+25   T 104  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 176\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.9613e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9997\n",
      "Q 7+642   T 649  \u001b[92m☑\u001b[0m Q 664+3   T 667  \u001b[92m☑\u001b[0m Q 59+46   T 105  \u001b[92m☑\u001b[0m Q 797+830 T 1627 \u001b[92m☑\u001b[0m Q 105+446 T 551  \u001b[92m☑\u001b[0m Q 28+946  T 974  \u001b[92m☑\u001b[0m Q 9+725   T 734  \u001b[92m☑\u001b[0m Q 490+273 T 763  \u001b[92m☑\u001b[0m Q 502+17  T 519  \u001b[92m☑\u001b[0m Q 68+493  T 561  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 177\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.7453e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9997\n",
      "Q 154+421 T 575  \u001b[92m☑\u001b[0m Q 836+201 T 1037 \u001b[92m☑\u001b[0m Q 37+409  T 446  \u001b[92m☑\u001b[0m Q 66+365  T 431  \u001b[92m☑\u001b[0m Q 921+311 T 1232 \u001b[92m☑\u001b[0m Q 421+51  T 472  \u001b[92m☑\u001b[0m Q 31+209  T 240  \u001b[92m☑\u001b[0m Q 23+357  T 380  \u001b[92m☑\u001b[0m Q 93+441  T 534  \u001b[92m☑\u001b[0m Q 828+94  T 922  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 178\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 1.5586e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9997\n",
      "Q 61+272  T 333  \u001b[92m☑\u001b[0m Q 229+63  T 292  \u001b[92m☑\u001b[0m Q 38+967  T 1005 \u001b[92m☑\u001b[0m Q 671+416 T 1087 \u001b[92m☑\u001b[0m Q 88+178  T 266  \u001b[92m☑\u001b[0m Q 131+334 T 465  \u001b[92m☑\u001b[0m Q 50+352  T 402  \u001b[92m☑\u001b[0m Q 5+333   T 338  \u001b[92m☑\u001b[0m Q 851+5   T 856  \u001b[92m☑\u001b[0m Q 70+773  T 843  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 179\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.4058e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9997\n",
      "Q 223+648 T 871  \u001b[92m☑\u001b[0m Q 871+753 T 1624 \u001b[92m☑\u001b[0m Q 37+94   T 131  \u001b[92m☑\u001b[0m Q 257+75  T 332  \u001b[92m☑\u001b[0m Q 2+972   T 974  \u001b[92m☑\u001b[0m Q 5+872   T 877  \u001b[92m☑\u001b[0m Q 4+464   T 468  \u001b[92m☑\u001b[0m Q 523+883 T 1406 \u001b[92m☑\u001b[0m Q 401+708 T 1109 \u001b[92m☑\u001b[0m Q 961+886 T 1847 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 180\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.2806e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9997\n",
      "Q 368+91  T 459  \u001b[92m☑\u001b[0m Q 516+1   T 517  \u001b[92m☑\u001b[0m Q 528+116 T 644  \u001b[92m☑\u001b[0m Q 649+27  T 676  \u001b[92m☑\u001b[0m Q 661+701 T 1362 \u001b[92m☑\u001b[0m Q 1+807   T 808  \u001b[92m☑\u001b[0m Q 189+74  T 263  \u001b[92m☑\u001b[0m Q 6+192   T 198  \u001b[92m☑\u001b[0m Q 250+299 T 549  \u001b[92m☑\u001b[0m Q 243+90  T 333  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 181\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 1.1623e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9997\n",
      "Q 390+538 T 928  \u001b[92m☑\u001b[0m Q 12+535  T 547  \u001b[92m☑\u001b[0m Q 48+759  T 807  \u001b[92m☑\u001b[0m Q 69+593  T 662  \u001b[92m☑\u001b[0m Q 284+980 T 1264 \u001b[92m☑\u001b[0m Q 21+286  T 307  \u001b[92m☑\u001b[0m Q 95+221  T 316  \u001b[92m☑\u001b[0m Q 80+36   T 116  \u001b[92m☑\u001b[0m Q 82+659  T 741  \u001b[92m☑\u001b[0m Q 201+945 T 1146 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 182\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0196 - acc: 0.9951 - val_loss: 0.0924 - val_acc: 0.9712\n",
      "Q 707+782 T 1489 \u001b[92m☑\u001b[0m Q 47+367  T 414  \u001b[92m☑\u001b[0m Q 1+189   T 190  \u001b[92m☑\u001b[0m Q 430+13  T 443  \u001b[92m☑\u001b[0m Q 214+57  T 271  \u001b[92m☑\u001b[0m Q 3+866   T 869  \u001b[92m☑\u001b[0m Q 673+48  T 721  \u001b[92m☑\u001b[0m Q 32+904  T 936  \u001b[92m☑\u001b[0m Q 321+21  T 342  \u001b[92m☑\u001b[0m Q 66+87   T 153  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 183\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 0.0133 - acc: 0.9963 - val_loss: 0.0032 - val_acc: 0.9992\n",
      "Q 46+911  T 957  \u001b[92m☑\u001b[0m Q 402+300 T 702  \u001b[92m☑\u001b[0m Q 818+8   T 826  \u001b[92m☑\u001b[0m Q 616+56  T 672  \u001b[92m☑\u001b[0m Q 666+793 T 1459 \u001b[92m☑\u001b[0m Q 12+409  T 421  \u001b[92m☑\u001b[0m Q 501+14  T 515  \u001b[92m☑\u001b[0m Q 76+505  T 581  \u001b[92m☑\u001b[0m Q 113+81  T 194  \u001b[92m☑\u001b[0m Q 12+310  T 322  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 184\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 5.1385e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9995\n",
      "Q 379+635 T 1014 \u001b[92m☑\u001b[0m Q 82+149  T 231  \u001b[92m☑\u001b[0m Q 933+42  T 975  \u001b[92m☑\u001b[0m Q 257+95  T 352  \u001b[92m☑\u001b[0m Q 0+326   T 326  \u001b[92m☑\u001b[0m Q 41+25   T 66   \u001b[92m☑\u001b[0m Q 511+7   T 518  \u001b[92m☑\u001b[0m Q 769+213 T 982  \u001b[92m☑\u001b[0m Q 49+871  T 920  \u001b[92m☑\u001b[0m Q 905+0   T 905  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 185\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 3.0489e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9996\n",
      "Q 305+1   T 306  \u001b[92m☑\u001b[0m Q 64+982  T 1046 \u001b[92m☑\u001b[0m Q 599+496 T 1095 \u001b[92m☑\u001b[0m Q 1+730   T 731  \u001b[92m☑\u001b[0m Q 923+779 T 1702 \u001b[92m☑\u001b[0m Q 715+695 T 1410 \u001b[92m☑\u001b[0m Q 5+252   T 257  \u001b[92m☑\u001b[0m Q 223+45  T 268  \u001b[92m☑\u001b[0m Q 895+294 T 1189 \u001b[92m☑\u001b[0m Q 79+956  T 1035 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 186\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.4762e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Q 677+822 T 1499 \u001b[92m☑\u001b[0m Q 717+680 T 1397 \u001b[92m☑\u001b[0m Q 0+756   T 756  \u001b[92m☑\u001b[0m Q 820+14  T 834  \u001b[92m☑\u001b[0m Q 312+82  T 394  \u001b[92m☑\u001b[0m Q 69+295  T 364  \u001b[92m☑\u001b[0m Q 350+50  T 400  \u001b[92m☑\u001b[0m Q 396+704 T 1100 \u001b[92m☑\u001b[0m Q 72+873  T 945  \u001b[92m☑\u001b[0m Q 467+3   T 470  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 187\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 2.1170e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9996\n",
      "Q 924+7   T 931  \u001b[92m☑\u001b[0m Q 8+70    T 78   \u001b[92m☑\u001b[0m Q 78+874  T 952  \u001b[92m☑\u001b[0m Q 66+97   T 163  \u001b[92m☑\u001b[0m Q 94+498  T 592  \u001b[92m☑\u001b[0m Q 22+69   T 91   \u001b[92m☑\u001b[0m Q 46+308  T 354  \u001b[92m☑\u001b[0m Q 40+544  T 584  \u001b[92m☑\u001b[0m Q 475+42  T 517  \u001b[92m☑\u001b[0m Q 699+50  T 749  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 188\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.8462e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9996\n",
      "Q 717+42  T 759  \u001b[92m☑\u001b[0m Q 76+49   T 125  \u001b[92m☑\u001b[0m Q 0+606   T 606  \u001b[92m☑\u001b[0m Q 79+934  T 1013 \u001b[92m☑\u001b[0m Q 6+625   T 631  \u001b[92m☑\u001b[0m Q 83+0    T 83   \u001b[92m☑\u001b[0m Q 63+57   T 120  \u001b[92m☑\u001b[0m Q 405+7   T 412  \u001b[92m☑\u001b[0m Q 42+280  T 322  \u001b[92m☑\u001b[0m Q 12+541  T 553  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 189\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.6379e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Q 59+512  T 571  \u001b[92m☑\u001b[0m Q 72+82   T 154  \u001b[92m☑\u001b[0m Q 21+388  T 409  \u001b[92m☑\u001b[0m Q 16+59   T 75   \u001b[92m☑\u001b[0m Q 326+221 T 547  \u001b[92m☑\u001b[0m Q 803+521 T 1324 \u001b[92m☑\u001b[0m Q 55+737  T 792  \u001b[92m☑\u001b[0m Q 2+548   T 550  \u001b[92m☑\u001b[0m Q 66+591  T 657  \u001b[92m☑\u001b[0m Q 330+681 T 1011 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 190\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 1.4627e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Q 884+40  T 924  \u001b[92m☑\u001b[0m Q 45+28   T 73   \u001b[92m☑\u001b[0m Q 57+707  T 764  \u001b[92m☑\u001b[0m Q 936+75  T 1011 \u001b[92m☑\u001b[0m Q 83+618  T 701  \u001b[92m☑\u001b[0m Q 437+2   T 439  \u001b[92m☑\u001b[0m Q 93+92   T 185  \u001b[92m☑\u001b[0m Q 624+6   T 630  \u001b[92m☑\u001b[0m Q 34+776  T 810  \u001b[92m☑\u001b[0m Q 777+787 T 1564 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 191\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 1.3163e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Q 551+91  T 642  \u001b[92m☑\u001b[0m Q 618+15  T 633  \u001b[92m☑\u001b[0m Q 606+67  T 673  \u001b[92m☑\u001b[0m Q 135+93  T 228  \u001b[92m☑\u001b[0m Q 81+934  T 1015 \u001b[92m☑\u001b[0m Q 57+81   T 138  \u001b[92m☑\u001b[0m Q 830+22  T 852  \u001b[92m☑\u001b[0m Q 54+629  T 683  \u001b[92m☑\u001b[0m Q 201+296 T 497  \u001b[92m☑\u001b[0m Q 24+686  T 710  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 192\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 1.1936e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Q 74+275  T 349  \u001b[92m☑\u001b[0m Q 927+617 T 1544 \u001b[92m☑\u001b[0m Q 65+51   T 116  \u001b[92m☑\u001b[0m Q 885+838 T 1723 \u001b[92m☑\u001b[0m Q 108+37  T 145  \u001b[92m☑\u001b[0m Q 637+400 T 1037 \u001b[92m☑\u001b[0m Q 232+115 T 347  \u001b[92m☑\u001b[0m Q 596+77  T 673  \u001b[92m☑\u001b[0m Q 295+8   T 303  \u001b[92m☑\u001b[0m Q 644+92  T 736  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 193\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 1.0813e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9996\n",
      "Q 544+68  T 612  \u001b[92m☑\u001b[0m Q 652+354 T 1006 \u001b[92m☑\u001b[0m Q 483+0   T 483  \u001b[92m☑\u001b[0m Q 320+30  T 350  \u001b[92m☑\u001b[0m Q 52+8    T 60   \u001b[92m☑\u001b[0m Q 139+810 T 949  \u001b[92m☑\u001b[0m Q 960+65  T 1025 \u001b[92m☑\u001b[0m Q 793+62  T 855  \u001b[92m☑\u001b[0m Q 64+79   T 143  \u001b[92m☑\u001b[0m Q 721+317 T 1038 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 194\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 9.8120e-05 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9997\n",
      "Q 682+74  T 756  \u001b[92m☑\u001b[0m Q 786+787 T 1573 \u001b[92m☑\u001b[0m Q 60+281  T 341  \u001b[92m☑\u001b[0m Q 1+829   T 830  \u001b[92m☑\u001b[0m Q 693+17  T 710  \u001b[92m☑\u001b[0m Q 66+97   T 163  \u001b[92m☑\u001b[0m Q 59+816  T 875  \u001b[92m☑\u001b[0m Q 293+504 T 797  \u001b[92m☑\u001b[0m Q 257+95  T 352  \u001b[92m☑\u001b[0m Q 480+107 T 587  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 195\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 0.0242 - acc: 0.9927 - val_loss: 0.0077 - val_acc: 0.9979\n",
      "Q 581+536 T 1117 \u001b[92m☑\u001b[0m Q 62+49   T 111  \u001b[92m☑\u001b[0m Q 304+1   T 305  \u001b[92m☑\u001b[0m Q 378+8   T 386  \u001b[92m☑\u001b[0m Q 157+49  T 206  \u001b[92m☑\u001b[0m Q 804+51  T 855  \u001b[92m☑\u001b[0m Q 377+548 T 925  \u001b[92m☑\u001b[0m Q 926+6   T 932  \u001b[92m☑\u001b[0m Q 76+151  T 227  \u001b[92m☑\u001b[0m Q 382+17  T 399  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 196\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0025 - val_acc: 0.9993\n",
      "Q 85+983  T 1068 \u001b[92m☑\u001b[0m Q 165+3   T 168  \u001b[92m☑\u001b[0m Q 61+762  T 823  \u001b[92m☑\u001b[0m Q 299+583 T 882  \u001b[92m☑\u001b[0m Q 669+913 T 1582 \u001b[92m☑\u001b[0m Q 38+12   T 50   \u001b[92m☑\u001b[0m Q 847+23  T 870  \u001b[92m☑\u001b[0m Q 82+911  T 993  \u001b[92m☑\u001b[0m Q 558+95  T 653  \u001b[92m☑\u001b[0m Q 227+70  T 297  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 197\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 3.5839e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9994\n",
      "Q 718+22  T 740  \u001b[92m☑\u001b[0m Q 936+75  T 1011 \u001b[92m☑\u001b[0m Q 806+4   T 810  \u001b[92m☑\u001b[0m Q 171+23  T 194  \u001b[92m☑\u001b[0m Q 1+730   T 731  \u001b[92m☑\u001b[0m Q 983+5   T 988  \u001b[92m☑\u001b[0m Q 979+143 T 1122 \u001b[92m☑\u001b[0m Q 38+872  T 910  \u001b[92m☑\u001b[0m Q 19+895  T 914  \u001b[92m☑\u001b[0m Q 339+624 T 963  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 198\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 2.4429e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Q 14+662  T 676  \u001b[92m☑\u001b[0m Q 595+2   T 597  \u001b[92m☑\u001b[0m Q 171+36  T 207  \u001b[92m☑\u001b[0m Q 50+95   T 145  \u001b[92m☑\u001b[0m Q 12+439  T 451  \u001b[92m☑\u001b[0m Q 50+563  T 613  \u001b[92m☑\u001b[0m Q 572+30  T 602  \u001b[92m☑\u001b[0m Q 351+58  T 409  \u001b[92m☑\u001b[0m Q 452+53  T 505  \u001b[92m☑\u001b[0m Q 492+923 T 1415 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 199\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 2.0393e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "Q 0+719   T 719  \u001b[92m☑\u001b[0m Q 33+228  T 261  \u001b[92m☑\u001b[0m Q 47+119  T 166  \u001b[92m☑\u001b[0m Q 530+981 T 1511 \u001b[92m☑\u001b[0m Q 332+996 T 1328 \u001b[92m☑\u001b[0m Q 650+61  T 711  \u001b[92m☑\u001b[0m Q 422+326 T 748  \u001b[92m☑\u001b[0m Q 386+1   T 387  \u001b[92m☑\u001b[0m Q 37+324  T 361  \u001b[92m☑\u001b[0m Q 761+34  T 795  \u001b[92m☑\u001b[0m 795 \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "# An implementation of sequence to sequence learning for performing addition\n",
    "Input: \"535+61\"\n",
    "Output: \"596\"\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "Input may optionally be reversed, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "Two digits reversed:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "Three digits reversed:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "Four digits reversed:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "Five digits reversed:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n",
    "'''  # noqa\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "\n",
    "\n",
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one-hot integer representation\n",
    "    + Decode the one-hot or integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One-hot encode given string C.\n",
    "        # Arguments\n",
    "            C: string, to be encoded.\n",
    "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"Decode the given vector or 2D array to their character output.\n",
    "        # Arguments\n",
    "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
    "                or a vector of character indices (used with `calc_argmax=False`).\n",
    "            calc_argmax: Whether to find the character index with maximum\n",
    "                probability, defaults to `True`.\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n",
    "\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "REVERSE = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# Try replacing GRU, or SimpleRNN.\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last output of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for iteration in range(1, 200):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val, y_val))\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if REVERSE else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "print(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.LSTM'>, 50, 100, 100\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\n",
      "11747328/11745123 [==============================] - 12s 1us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab = ['.', '?', 'Daniel', 'John', 'Mary', 'Sandra', 'Where', 'apple', 'back', 'bathroom', 'bedroom', 'discarded', 'down', 'dropped', 'football', 'garden', 'got', 'grabbed', 'hallway', 'is', 'journeyed', 'kitchen', 'left', 'milk', 'moved', 'office', 'picked', 'put', 'the', 'there', 'to', 'took', 'travelled', 'up', 'went']\n",
      "x.shape = (1000, 552)\n",
      "xq.shape = (1000, 5)\n",
      "y.shape = (1000, 36)\n",
      "story_maxlen, query_maxlen = 552, 5\n",
      "Build model...\n",
      "Training\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 2.6727 - acc: 0.1989 - val_loss: 1.7922 - val_acc: 0.3000\n",
      "Epoch 2/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.8014 - acc: 0.2242 - val_loss: 1.7732 - val_acc: 0.3000\n",
      "Epoch 3/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7899 - acc: 0.1874 - val_loss: 1.8582 - val_acc: 0.0600\n",
      "Epoch 4/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.8010 - acc: 0.2042 - val_loss: 1.8299 - val_acc: 0.0600\n",
      "Epoch 5/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.8024 - acc: 0.2042 - val_loss: 1.8543 - val_acc: 0.0600\n",
      "Epoch 6/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 1.7915 - acc: 0.2021 - val_loss: 1.7545 - val_acc: 0.2600\n",
      "Epoch 7/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 1.7963 - acc: 0.2021 - val_loss: 1.8071 - val_acc: 0.0600\n",
      "Epoch 8/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 1.7880 - acc: 0.2063 - val_loss: 1.8845 - val_acc: 0.0600\n",
      "Epoch 9/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7936 - acc: 0.2011 - val_loss: 1.8430 - val_acc: 0.0600\n",
      "Epoch 10/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7851 - acc: 0.2200 - val_loss: 1.8020 - val_acc: 0.1600\n",
      "Epoch 11/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 1.7805 - acc: 0.2189 - val_loss: 1.7574 - val_acc: 0.3200\n",
      "Epoch 12/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7715 - acc: 0.2432 - val_loss: 1.7897 - val_acc: 0.2000\n",
      "Epoch 13/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7683 - acc: 0.2358 - val_loss: 1.7844 - val_acc: 0.2000\n",
      "Epoch 14/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7615 - acc: 0.2579 - val_loss: 1.8155 - val_acc: 0.2200\n",
      "Epoch 15/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7518 - acc: 0.2716 - val_loss: 1.7915 - val_acc: 0.1800\n",
      "Epoch 16/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 1.7562 - acc: 0.2611 - val_loss: 1.7278 - val_acc: 0.3000\n",
      "Epoch 17/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7311 - acc: 0.2811 - val_loss: 1.7478 - val_acc: 0.2400\n",
      "Epoch 18/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 1.7301 - acc: 0.2737 - val_loss: 1.8646 - val_acc: 0.0600\n",
      "Epoch 19/20\n",
      "950/950 [==============================] - 15s 15ms/step - loss: 1.7418 - acc: 0.2737 - val_loss: 1.6838 - val_acc: 0.3800\n",
      "Epoch 20/20\n",
      "950/950 [==============================] - 15s 16ms/step - loss: 1.7312 - acc: 0.2768 - val_loss: 1.7554 - val_acc: 0.2600\n",
      "Evaluation\n",
      "1000/1000 [==============================] - 3s 3ms/step\n",
      "Test loss / test accuracy = 1.7967 / 0.2080\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Trains two recurrent neural networks based upon a story and a question.\n",
    "The resulting merged vector is then queried to answer a range of bAbI tasks.\n",
    "The results are comparable to those for an LSTM model provided in Weston et al.:\n",
    "\"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\"\n",
    "http://arxiv.org/abs/1502.05698\n",
    "Task Number                  | FB LSTM Baseline | Keras QA\n",
    "---                          | ---              | ---\n",
    "QA1 - Single Supporting Fact | 50               | 52.1\n",
    "QA2 - Two Supporting Facts   | 20               | 37.0\n",
    "QA3 - Three Supporting Facts | 20               | 20.5\n",
    "QA4 - Two Arg. Relations     | 61               | 62.9\n",
    "QA5 - Three Arg. Relations   | 70               | 61.9\n",
    "QA6 - yes/No Questions       | 48               | 50.7\n",
    "QA7 - Counting               | 49               | 78.9\n",
    "QA8 - Lists/Sets             | 45               | 77.2\n",
    "QA9 - Simple Negation        | 64               | 64.0\n",
    "QA10 - Indefinite Knowledge  | 44               | 47.7\n",
    "QA11 - Basic Coreference     | 72               | 74.9\n",
    "QA12 - Conjunction           | 74               | 76.4\n",
    "QA13 - Compound Coreference  | 94               | 94.4\n",
    "QA14 - Time Reasoning        | 27               | 34.8\n",
    "QA15 - Basic Deduction       | 21               | 32.4\n",
    "QA16 - Basic Induction       | 23               | 50.6\n",
    "QA17 - Positional Reasoning  | 51               | 49.1\n",
    "QA18 - Size Reasoning        | 52               | 90.8\n",
    "QA19 - Path Finding          | 8                | 9.0\n",
    "QA20 - Agent's Motivations   | 91               | 90.7\n",
    "For the resources related to the bAbI project, refer to:\n",
    "https://research.facebook.com/researchers/1543934539189348\n",
    "### Notes\n",
    "- With default word, sentence, and query vector sizes, the GRU model achieves:\n",
    "  - 52.1% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU)\n",
    "  - 37.0% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU)\n",
    "In comparison, the Facebook paper achieves 50% and 20% for the LSTM baseline.\n",
    "- The task does not traditionally parse the question separately. This likely\n",
    "improves accuracy and is a good example of merging two RNNs.\n",
    "- The word vector embeddings are not shared between the story and question RNNs.\n",
    "- See how the accuracy changes given 10,000 training samples (en-10k) instead\n",
    "of only 1000. 1000 was used in order to be comparable to the original paper.\n",
    "- Experiment with GRU, LSTM, and JZS1-3 as they give subtly different results.\n",
    "- The length and noise (i.e. 'useless' story components) impact the ability of\n",
    "LSTMs / GRUs to provide the correct answer. Given only the supporting facts,\n",
    "these RNNs can achieve 100% accuracy on many tasks. Memory networks and neural\n",
    "networks that use attentional processes can efficiently search through this\n",
    "noise to find the relevant statements, improving performance substantially.\n",
    "This becomes especially obvious on QA2 and QA3, both far longer than QA1.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true,\n",
    "    only the sentences that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file, retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data\n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        y[word_idx[answer]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return (pad_sequences(xs, maxlen=story_maxlen),\n",
    "            pad_sequences(xqs, maxlen=query_maxlen), np.array(ys))\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                    origin='https://s3.amazonaws.com/text-datasets/'\n",
    "                           'babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n",
    "          '.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "\n",
    "# Default QA1 with 1000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n",
    "# QA1 with 10,000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'\n",
    "# QA2 with 1000 samples\n",
    "challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt'\n",
    "# QA2 with 10,000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'\n",
    "with tarfile.open(path) as tar:\n",
    "    train = get_stories(tar.extractfile(challenge.format('train')))\n",
    "    test = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train + test:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "\n",
    "x, xq, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\n",
    "tx, txq, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n",
    "\n",
    "print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xq.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "sentence = layers.Input(shape=(story_maxlen,), dtype='int32')\n",
    "encoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)\n",
    "encoded_sentence = RNN(SENT_HIDDEN_SIZE)(encoded_sentence)\n",
    "\n",
    "question = layers.Input(shape=(query_maxlen,), dtype='int32')\n",
    "encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\n",
    "encoded_question = RNN(QUERY_HIDDEN_SIZE)(encoded_question)\n",
    "\n",
    "merged = layers.concatenate([encoded_sentence, encoded_question])\n",
    "preds = layers.Dense(vocab_size, activation='softmax')(merged)\n",
    "\n",
    "model = Model([sentence, question], preds)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training')\n",
    "model.fit([x, xq], y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05)\n",
    "\n",
    "print('Evaluation')\n",
    "loss, acc = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 5s 207us/step - loss: 0.4117 - acc: 0.7925 - val_loss: 0.2960 - val_acc: 0.8726\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 5s 204us/step - loss: 0.2309 - acc: 0.9072 - val_loss: 0.2960 - val_acc: 0.8762\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 5s 205us/step - loss: 0.1687 - acc: 0.9349 - val_loss: 0.2719 - val_acc: 0.8900\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 5s 203us/step - loss: 0.1195 - acc: 0.9569 - val_loss: 0.3363 - val_acc: 0.8826\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 5s 203us/step - loss: 0.0785 - acc: 0.9716 - val_loss: 0.3456 - val_acc: 0.8856\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 5s 203us/step - loss: 0.0593 - acc: 0.9787 - val_loss: 0.3652 - val_acc: 0.8836\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 5s 204us/step - loss: 0.0470 - acc: 0.9832 - val_loss: 0.4109 - val_acc: 0.8830\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 5s 203us/step - loss: 0.0368 - acc: 0.9870 - val_loss: 0.4949 - val_acc: 0.8764\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 5s 203us/step - loss: 0.0299 - acc: 0.9891 - val_loss: 0.5153 - val_acc: 0.8804\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 5s 203us/step - loss: 0.0316 - acc: 0.9883 - val_loss: 0.5183 - val_acc: 0.8794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f769018c8d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#This example demonstrates the use of Convolution1D for text classification.\n",
    "Gets to 0.89 test accuracy after 2 epochs. </br>\n",
    "90s/epoch on Intel i5 2.4Ghz CPU. </br>\n",
    "10s/epoch on Tesla K40 GPU.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
