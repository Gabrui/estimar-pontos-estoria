{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimativa de pontos de estória usando aprendizado profundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilização de aprendizado profundo por redes neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/usr/lib64/python3.6/site-packages/theano/gpuarray/dnn.py:184: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to a version >= v5 and <= v7.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n",
      "Using cuDNN version 7401 on context None\n",
      "Mapped name None to device cuda: GeForce GTX TITAN X (0000:04:00.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "#os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import keras\n",
    "import theano\n",
    "from keras.layers import Input, Embedding, LSTM\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "nlp = spacy.load('pt', disable=['parser', 'tagger', 'ner'])\n",
    "#tokenizar = spacy.tokenizer.Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão do conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treino, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_csv_texto_tokenizado(caminho_arquivo, tokenizador):\n",
    "    tabela_dados = pd.read_csv(caminho_arquivo)\n",
    "    titulos = tabela_dados.values[:, 1].astype('str')\n",
    "    separadores = np.full(len(titulos), '. ')\n",
    "    descricao = tabela_dados.values[:, 2].astype('str')\n",
    "    juntos = [a + b + c for a, b, c in zip(titulos, separadores, descricao)]\n",
    "    tokenizados = [[w.text.strip().lower() for w in tokenizador(frase) if not w.text.isspace()] for frase in juntos]\n",
    "    return tokenizados\n",
    "\n",
    "def gerar_dicionario(lista_tokens, vocabulario_maximo=100000):\n",
    "    contagem = defaultdict(int)\n",
    "    for tokens in lista_tokens:\n",
    "        for token in tokens:\n",
    "            contagem[token] += 1\n",
    "    ordenadas = sorted(contagem.items(), key=lambda x: x[1], reverse=True)\n",
    "    IDs = {}\n",
    "    for i in range(min(vocabulario_maximo, len(ordenadas))): # EOF = 0, ID_desconhecido = 1\n",
    "        IDs[ordenadas[i]] = i + 2\n",
    "    return IDs\n",
    "\n",
    "def converte_tokens(lista_tokens, dic, fracao_valida = 0.1):\n",
    "    lista_ids = [[(dic.get(tk) or 0) for tk in tks] + [0] for tks in lista_tokens]\n",
    "    quant_valida = int(fracao_valida * len(lista_ids))\n",
    "    return lista_ids[:-quant_valida], lista_ids[-quant_valida:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario:  32366 \n",
      "Treino:  10970 \n",
      " Validação:  1218\n"
     ]
    }
   ],
   "source": [
    "a = ler_csv_texto_tokenizado('dados/pretreino/ccasj.csv', nlp.tokenizer)\n",
    "dic = gerar_dicionario(a)\n",
    "treino, validacao = converte_tokens(a, dic)\n",
    "print('Vocabulario: ', len(dic), '\\nTreino: ', len(treino), '\\n Validação: ', len(validacao))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretreinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimativaConstrastivaRuidosa(keras.layers.Layer):\n",
    "    def __init__(self, init='glorot_uniform', comprimento=10,\n",
    "                 dimensao_entrada=None, vocabulario=None, ruidos = 25, distribuicao_ruidos=[0.5, 0.5], semente=19, **kwargs):\n",
    "        self.init = init\n",
    "        self.comprimento = comprimento\n",
    "        self.vocabulario = vocabulario\n",
    "        self.ruidos = ruidos\n",
    "        self.distribuicao_ruidos = theano.shared(np.array(distribuicao_ruidos).astype(theano.config.floatX)) #IMP\n",
    "        self.gerador_aleatorio = theano.tensor.shared_randomstreams.RandomStreams(seed=semente) #IMP\n",
    "        self.dimensao_entrada = dimensao_entrada\n",
    "        kwargs['input_shape'] = (self.dimensao_entrada, )\n",
    "        super(EstimativaConstrastivaRuidosa, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, dims_entrada):\n",
    "        self.W = self.add_weight(name='{}_W'.format(self.name), shape=(self.vocabulario, self.dimensao_entrada), initializer=self.init, trainable=True)\n",
    "        self.b = self.add_weight(name='{}_b'.format(self.name), shape=(self.vocabulario, )                     , initializer=self.init, trainable=True)\n",
    "        super(EstimativaConstrastivaRuidosa, self).build(dims_entrada)\n",
    "\n",
    "    def compute_output_shape(self, dims_entrada):\n",
    "        return (None, self.comprimento, self.ruidos + 1)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        contexto = inputs[0] #shape: amostras * passos * dim\n",
    "        proxima_palavra = inputs[1] #shape: amostras * passos\n",
    "\n",
    "        amostras, passos = proxima_palavra.shape\n",
    "        dim_saida = self.ruidos + 1\n",
    "\n",
    "        noise_w = self.gerador_aleatorio.choice(size=(amostras, passos, self.ruidos), a=self.distribuicao_ruidos.shape[0], p=self.distribuicao_ruidos)\n",
    "        proxima_palavra = proxima_palavra.flatten().reshape([amostras, passos, 1])\n",
    "        proxima_palavra = theano.tensor.concatenate([proxima_palavra, noise_w], axis=-1) #IMP shape: amostras * passos * dim_saida\n",
    "\n",
    "        W_ = self.W[proxima_palavra.flatten()].flatten().reshape([amostras, passos, dim_saida, self.dimensao_entrada])\n",
    "        b_ = self.b[proxima_palavra.flatten()].reshape([amostras, passos, dim_saida])\n",
    "\n",
    "        s_theta = (contexto[:, :, None, :] * W_).sum(axis=-1) + b_ # dims: amostras * passos * dim_saida\n",
    "        noiseP = self.distribuicao_ruidos[proxima_palavra.flatten()].reshape([amostras, passos, dim_saida])\n",
    "        noise_score = keras.backend.log(self.ruidos * noiseP) #log(k * distribuicao_ruidos(w))\n",
    "\n",
    "        return keras.activations.sigmoid(s_theta - noise_score) # dims: amostras, passos, dim_saida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_pretreinamento(dicionario, dim_vetorial, comprimento, ruidos, distribuicao_ruidos):\n",
    "    vocabulario = len(dicionario)\n",
    "    entrada_atual = Input(shape=(comprimento,), dtype='int64', name='entrada_atual')\n",
    "    proxima_entrada = Input(shape=(comprimento,), dtype='int64', name='proxima_entrada')\n",
    "    vetorizado = Embedding(output_dim=dim_vetorial, input_dim=vocabulario, input_length=comprimento, mask_zero=True)(entrada_atual)\n",
    "    contexto_recorrente = LSTM(dim_vetorial, input_shape=(None, dim_vetorial), return_sequences=True)(vetorizado)\n",
    "    estimativa = EstimativaConstrastivaRuidosa(dimensao_entrada=dim_vetorial, comprimento=comprimento, vocabulario=vocabulario,\n",
    "                ruidos=ruidos, distribuicao_ruidos=distribuicao_ruidos)([contexto_recorrente, proxima_entrada])\n",
    "    return Model(inputs=[entrada_atual, proxima_entrada], outputs=estimativa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Average train sequence length: 238\n",
      "Average test sequence length: 230\n",
      "Adding 2-gram features\n",
      "Average train sequence length: 476\n",
      "Average test sequence length: 428\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.5862 - acc: 0.7874 - val_loss: 0.4372 - val_acc: 0.8548\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 34s 1ms/step - loss: 0.2877 - acc: 0.9276 - val_loss: 0.3030 - val_acc: 0.8904\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 34s 1ms/step - loss: 0.1432 - acc: 0.9696 - val_loss: 0.2628 - val_acc: 0.8998\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 34s 1ms/step - loss: 0.0775 - acc: 0.9871 - val_loss: 0.2441 - val_acc: 0.9030\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 34s 1ms/step - loss: 0.0437 - acc: 0.9950 - val_loss: 0.2376 - val_acc: 0.9040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa688dd99e8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#This example demonstrates the use of fasttext for text classification\n",
    "Based on Joulin et al's paper:\n",
    "[Bags of Tricks for Efficient Text Classification\n",
    "](https://arxiv.org/abs/1607.01759)\n",
    "Results on IMDB datasets with uni and bi-gram embeddings:\n",
    "Embedding|Accuracy, 5 epochs|Speed (s/epoch)|Hardware\n",
    ":--------|-----------------:|----:|:-------\n",
    "Uni-gram |            0.8813|    8|i7 CPU\n",
    "Bi-gram  |            0.9056|    2|GTx 980M GPU\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 2\n",
    "max_features = 20000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "# we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "# of all words in the document\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'numpy' from '/usr/lib64/python3.6/site-packages/numpy/__init__.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n",
      "Processing text dataset\n",
      "Found 19997 texts.\n",
      "Found 174074 unique tokens.\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 20)\n",
      "Preparing embedding matrix.\n",
      "Training model.\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/10\n",
      "15998/15998 [==============================] - 5s 340us/step - loss: 2.4037 - acc: 0.2187 - val_loss: 1.8067 - val_acc: 0.3768\n",
      "Epoch 2/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 1.5519 - acc: 0.4621 - val_loss: 1.4603 - val_acc: 0.5064\n",
      "Epoch 3/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 1.1902 - acc: 0.5953 - val_loss: 1.1682 - val_acc: 0.5969\n",
      "Epoch 4/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.9690 - acc: 0.6748 - val_loss: 1.0070 - val_acc: 0.6624\n",
      "Epoch 5/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.8137 - acc: 0.7250 - val_loss: 0.9541 - val_acc: 0.6784\n",
      "Epoch 6/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.6990 - acc: 0.7604 - val_loss: 1.0010 - val_acc: 0.6557\n",
      "Epoch 7/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.5930 - acc: 0.7985 - val_loss: 0.9482 - val_acc: 0.6899\n",
      "Epoch 8/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.5045 - acc: 0.8270 - val_loss: 0.8498 - val_acc: 0.7367\n",
      "Epoch 9/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.4280 - acc: 0.8572 - val_loss: 0.8620 - val_acc: 0.7339\n",
      "Epoch 10/10\n",
      "15998/15998 [==============================] - 4s 276us/step - loss: 0.3622 - acc: 0.8782 - val_loss: 0.9350 - val_acc: 0.7302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7699e01080>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This script loads pre-trained word embeddings (GloVe embeddings)\n",
    "into a frozen Keras Embedding layer, and uses it to\n",
    "train a text classification model on the 20 Newsgroup dataset\n",
    "(classification of newsgroup messages into 20 different categories).\n",
    "GloVe embedding data can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/)\n",
    "20 Newsgroup data can be found at:\n",
    "http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove_en')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "8982 train sequences\n",
      "2246 test sequences\n",
      "46 classes\n",
      "Vectorizing sequence data...\n",
      "x_train shape: (8982, 1000)\n",
      "x_test shape: (2246, 1000)\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n",
      "Building model...\n",
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 1.5670 - acc: 0.6538 - val_loss: 1.1814 - val_acc: 0.7442\n",
      "Epoch 2/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.8898 - acc: 0.7982 - val_loss: 0.9999 - val_acc: 0.7798\n",
      "Epoch 3/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.6416 - acc: 0.8458 - val_loss: 0.8985 - val_acc: 0.7875\n",
      "Epoch 4/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.4929 - acc: 0.8820 - val_loss: 0.8817 - val_acc: 0.8031\n",
      "Epoch 5/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.3945 - acc: 0.9042 - val_loss: 0.8789 - val_acc: 0.7976\n",
      "Epoch 6/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.3191 - acc: 0.9209 - val_loss: 0.8845 - val_acc: 0.8131\n",
      "Epoch 7/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.2819 - acc: 0.9289 - val_loss: 0.8867 - val_acc: 0.8120\n",
      "Epoch 8/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.2442 - acc: 0.9398 - val_loss: 0.9005 - val_acc: 0.8076\n",
      "Epoch 9/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.2170 - acc: 0.9433 - val_loss: 0.9392 - val_acc: 0.7976\n",
      "Epoch 10/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1934 - acc: 0.9482 - val_loss: 0.9641 - val_acc: 0.7898\n",
      "Epoch 11/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1861 - acc: 0.9469 - val_loss: 0.9742 - val_acc: 0.7998\n",
      "Epoch 12/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1678 - acc: 0.9529 - val_loss: 1.0137 - val_acc: 0.7931\n",
      "Epoch 13/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1673 - acc: 0.9526 - val_loss: 1.0068 - val_acc: 0.7831\n",
      "Epoch 14/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1584 - acc: 0.9551 - val_loss: 1.0321 - val_acc: 0.7909\n",
      "Epoch 15/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1542 - acc: 0.9547 - val_loss: 1.0219 - val_acc: 0.7909\n",
      "Epoch 16/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1503 - acc: 0.9545 - val_loss: 1.0426 - val_acc: 0.7853\n",
      "Epoch 17/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1451 - acc: 0.9568 - val_loss: 1.0406 - val_acc: 0.7831\n",
      "Epoch 18/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1369 - acc: 0.9588 - val_loss: 1.0810 - val_acc: 0.7887\n",
      "Epoch 19/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1308 - acc: 0.9620 - val_loss: 1.1056 - val_acc: 0.7853\n",
      "Epoch 20/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1256 - acc: 0.9594 - val_loss: 1.0935 - val_acc: 0.7820\n",
      "Epoch 21/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1311 - acc: 0.9592 - val_loss: 1.1259 - val_acc: 0.7875\n",
      "Epoch 22/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1273 - acc: 0.9602 - val_loss: 1.1245 - val_acc: 0.7931\n",
      "Epoch 23/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1320 - acc: 0.9604 - val_loss: 1.1163 - val_acc: 0.7920\n",
      "Epoch 24/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1326 - acc: 0.9577 - val_loss: 1.0951 - val_acc: 0.7998\n",
      "Epoch 25/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1266 - acc: 0.9587 - val_loss: 1.1417 - val_acc: 0.7853\n",
      "Epoch 26/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1225 - acc: 0.9592 - val_loss: 1.1342 - val_acc: 0.7942\n",
      "Epoch 27/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1226 - acc: 0.9604 - val_loss: 1.1586 - val_acc: 0.7887\n",
      "Epoch 28/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1187 - acc: 0.9616 - val_loss: 1.1299 - val_acc: 0.7987\n",
      "Epoch 29/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1217 - acc: 0.9604 - val_loss: 1.1817 - val_acc: 0.7875\n",
      "Epoch 30/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1275 - acc: 0.9587 - val_loss: 1.1431 - val_acc: 0.7998\n",
      "Epoch 31/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1167 - acc: 0.9620 - val_loss: 1.1619 - val_acc: 0.7976\n",
      "Epoch 32/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1115 - acc: 0.9626 - val_loss: 1.1774 - val_acc: 0.7909\n",
      "Epoch 33/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1256 - acc: 0.9584 - val_loss: 1.1786 - val_acc: 0.7898\n",
      "Epoch 34/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1200 - acc: 0.9609 - val_loss: 1.1845 - val_acc: 0.7887\n",
      "Epoch 35/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1198 - acc: 0.9619 - val_loss: 1.2025 - val_acc: 0.7998\n",
      "Epoch 36/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1187 - acc: 0.9621 - val_loss: 1.2237 - val_acc: 0.7731\n",
      "Epoch 37/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1199 - acc: 0.9607 - val_loss: 1.2167 - val_acc: 0.7909\n",
      "Epoch 38/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1150 - acc: 0.9620 - val_loss: 1.1866 - val_acc: 0.7998\n",
      "Epoch 39/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1161 - acc: 0.9607 - val_loss: 1.2133 - val_acc: 0.7898\n",
      "Epoch 40/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1220 - acc: 0.9595 - val_loss: 1.2217 - val_acc: 0.7853\n",
      "Epoch 41/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1141 - acc: 0.9631 - val_loss: 1.2411 - val_acc: 0.7887\n",
      "Epoch 42/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1113 - acc: 0.9610 - val_loss: 1.2606 - val_acc: 0.7853\n",
      "Epoch 43/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1102 - acc: 0.9628 - val_loss: 1.2209 - val_acc: 0.7875\n",
      "Epoch 44/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1075 - acc: 0.9629 - val_loss: 1.2654 - val_acc: 0.7853\n",
      "Epoch 45/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1139 - acc: 0.9614 - val_loss: 1.2527 - val_acc: 0.7875\n",
      "Epoch 46/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1122 - acc: 0.9608 - val_loss: 1.2485 - val_acc: 0.7820\n",
      "Epoch 47/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1092 - acc: 0.9630 - val_loss: 1.2679 - val_acc: 0.7864\n",
      "Epoch 48/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1096 - acc: 0.9623 - val_loss: 1.2907 - val_acc: 0.7887\n",
      "Epoch 49/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1086 - acc: 0.9624 - val_loss: 1.2607 - val_acc: 0.7775\n",
      "Epoch 50/50\n",
      "8083/8083 [==============================] - 0s 24us/step - loss: 0.1048 - acc: 0.9634 - val_loss: 1.2892 - val_acc: 0.7831\n",
      "2246/2246 [==============================] - 0s 12us/step\n",
      "Test score: 1.2998778688408705\n",
      "Test accuracy: 0.7831700801424755\n"
     ]
    }
   ],
   "source": [
    "'''Trains and evaluate a simple MLP\n",
    "on the Reuters newswire topic classification task.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 1000\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n",
      "Vectorization...\n",
      "Training Data:\n",
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "Validation Data:\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n",
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 4, 12)             1548      \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 102us/step - loss: 1.8918 - acc: 0.3203 - val_loss: 1.7945 - val_acc: 0.3375\n",
      "Q 489+65  T 554  \u001b[91m☒\u001b[0m Q 685+99  T 784  \u001b[91m☒\u001b[0m Q 443+430 T 873  \u001b[91m☒\u001b[0m Q 81+934  T 1015 \u001b[91m☒\u001b[0m Q 423+374 T 797  \u001b[91m☒\u001b[0m Q 98+60   T 158  \u001b[91m☒\u001b[0m Q 526+89  T 615  \u001b[91m☒\u001b[0m Q 746+0   T 746  \u001b[91m☒\u001b[0m Q 10+444  T 454  \u001b[91m☒\u001b[0m Q 755+13  T 768  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 1.7337 - acc: 0.3593 - val_loss: 1.6606 - val_acc: 0.3797\n",
      "Q 200+78  T 278  \u001b[91m☒\u001b[0m Q 341+254 T 595  \u001b[91m☒\u001b[0m Q 22+99   T 121  \u001b[91m☒\u001b[0m Q 147+88  T 235  \u001b[91m☒\u001b[0m Q 891+246 T 1137 \u001b[91m☒\u001b[0m Q 484+570 T 1054 \u001b[91m☒\u001b[0m Q 1+453   T 454  \u001b[91m☒\u001b[0m Q 58+481  T 539  \u001b[91m☒\u001b[0m Q 553+60  T 613  \u001b[91m☒\u001b[0m Q 9+148   T 157  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 1.5852 - acc: 0.4074 - val_loss: 1.4965 - val_acc: 0.4399\n",
      "Q 959+13  T 972  \u001b[91m☒\u001b[0m Q 850+81  T 931  \u001b[91m☒\u001b[0m Q 817+17  T 834  \u001b[91m☒\u001b[0m Q 32+37   T 69   \u001b[91m☒\u001b[0m Q 7+564   T 571  \u001b[91m☒\u001b[0m Q 917+306 T 1223 \u001b[91m☒\u001b[0m Q 927+533 T 1460 \u001b[91m☒\u001b[0m Q 81+68   T 149  \u001b[91m☒\u001b[0m Q 893+945 T 1838 \u001b[91m☒\u001b[0m Q 877+87  T 964  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 1.3974 - acc: 0.4776 - val_loss: 1.3116 - val_acc: 0.5147\n",
      "Q 207+99  T 306  \u001b[91m☒\u001b[0m Q 66+8    T 74   \u001b[91m☒\u001b[0m Q 71+274  T 345  \u001b[91m☒\u001b[0m Q 10+736  T 746  \u001b[91m☒\u001b[0m Q 5+385   T 390  \u001b[91m☒\u001b[0m Q 401+708 T 1109 \u001b[91m☒\u001b[0m Q 600+709 T 1309 \u001b[91m☒\u001b[0m Q 458+85  T 543  \u001b[91m☒\u001b[0m Q 187+3   T 190  \u001b[91m☒\u001b[0m Q 82+870  T 952  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 1.2297 - acc: 0.5472 - val_loss: 1.1732 - val_acc: 0.5686\n",
      "Q 978+75  T 1053 \u001b[91m☒\u001b[0m Q 208+65  T 273  \u001b[91m☒\u001b[0m Q 699+670 T 1369 \u001b[91m☒\u001b[0m Q 99+182  T 281  \u001b[91m☒\u001b[0m Q 42+379  T 421  \u001b[91m☒\u001b[0m Q 622+66  T 688  \u001b[91m☒\u001b[0m Q 70+987  T 1057 \u001b[91m☒\u001b[0m Q 603+610 T 1213 \u001b[91m☒\u001b[0m Q 820+937 T 1757 \u001b[91m☒\u001b[0m Q 393+36  T 429  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 1.0848 - acc: 0.6054 - val_loss: 1.0308 - val_acc: 0.6301\n",
      "Q 153+367 T 520  \u001b[91m☒\u001b[0m Q 40+514  T 554  \u001b[91m☒\u001b[0m Q 50+35   T 85   \u001b[91m☒\u001b[0m Q 224+297 T 521  \u001b[91m☒\u001b[0m Q 850+411 T 1261 \u001b[91m☒\u001b[0m Q 56+13   T 69   \u001b[91m☒\u001b[0m Q 352+141 T 493  \u001b[91m☒\u001b[0m Q 79+25   T 104  \u001b[91m☒\u001b[0m Q 420+80  T 500  \u001b[91m☒\u001b[0m Q 895+468 T 1363 \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.9858 - acc: 0.6466 - val_loss: 0.9636 - val_acc: 0.6456\n",
      "Q 931+36  T 967  \u001b[92m☑\u001b[0m Q 665+755 T 1420 \u001b[91m☒\u001b[0m Q 806+4   T 810  \u001b[91m☒\u001b[0m Q 56+21   T 77   \u001b[91m☒\u001b[0m Q 592+718 T 1310 \u001b[91m☒\u001b[0m Q 118+914 T 1032 \u001b[91m☒\u001b[0m Q 214+8   T 222  \u001b[92m☑\u001b[0m Q 687+8   T 695  \u001b[91m☒\u001b[0m Q 149+18  T 167  \u001b[91m☒\u001b[0m Q 469+605 T 1074 \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.9069 - acc: 0.6766 - val_loss: 0.8838 - val_acc: 0.6861\n",
      "Q 180+425 T 605  \u001b[91m☒\u001b[0m Q 368+306 T 674  \u001b[91m☒\u001b[0m Q 348+21  T 369  \u001b[91m☒\u001b[0m Q 320+88  T 408  \u001b[91m☒\u001b[0m Q 3+401   T 404  \u001b[91m☒\u001b[0m Q 35+946  T 981  \u001b[91m☒\u001b[0m Q 414+820 T 1234 \u001b[91m☒\u001b[0m Q 0+891   T 891  \u001b[91m☒\u001b[0m Q 68+59   T 127  \u001b[91m☒\u001b[0m Q 21+323  T 344  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.8333 - acc: 0.7034 - val_loss: 0.8017 - val_acc: 0.7172\n",
      "Q 2+419   T 421  \u001b[92m☑\u001b[0m Q 108+238 T 346  \u001b[91m☒\u001b[0m Q 627+668 T 1295 \u001b[91m☒\u001b[0m Q 649+27  T 676  \u001b[91m☒\u001b[0m Q 99+411  T 510  \u001b[91m☒\u001b[0m Q 586+73  T 659  \u001b[91m☒\u001b[0m Q 638+65  T 703  \u001b[91m☒\u001b[0m Q 393+39  T 432  \u001b[91m☒\u001b[0m Q 221+642 T 863  \u001b[91m☒\u001b[0m Q 605+394 T 999  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.7649 - acc: 0.7300 - val_loss: 0.7401 - val_acc: 0.7435\n",
      "Q 96+968  T 1064 \u001b[91m☒\u001b[0m Q 96+100  T 196  \u001b[91m☒\u001b[0m Q 97+31   T 128  \u001b[92m☑\u001b[0m Q 96+80   T 176  \u001b[91m☒\u001b[0m Q 874+1   T 875  \u001b[92m☑\u001b[0m Q 356+171 T 527  \u001b[91m☒\u001b[0m Q 448+0   T 448  \u001b[92m☑\u001b[0m Q 457+586 T 1043 \u001b[91m☒\u001b[0m Q 939+911 T 1850 \u001b[91m☒\u001b[0m Q 369+61  T 430  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.7075 - acc: 0.7520 - val_loss: 0.7053 - val_acc: 0.7475\n",
      "Q 513+48  T 561  \u001b[92m☑\u001b[0m Q 29+70   T 99   \u001b[91m☒\u001b[0m Q 373+3   T 376  \u001b[92m☑\u001b[0m Q 88+69   T 157  \u001b[91m☒\u001b[0m Q 266+13  T 279  \u001b[91m☒\u001b[0m Q 28+94   T 122  \u001b[91m☒\u001b[0m Q 291+61  T 352  \u001b[92m☑\u001b[0m Q 842+21  T 863  \u001b[91m☒\u001b[0m Q 994+5   T 999  \u001b[91m☒\u001b[0m Q 349+21  T 370  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.6411 - acc: 0.7765 - val_loss: 0.5972 - val_acc: 0.7941\n",
      "Q 130+89  T 219  \u001b[92m☑\u001b[0m Q 9+298   T 307  \u001b[92m☑\u001b[0m Q 35+675  T 710  \u001b[92m☑\u001b[0m Q 77+796  T 873  \u001b[92m☑\u001b[0m Q 779+416 T 1195 \u001b[91m☒\u001b[0m Q 907+1   T 908  \u001b[92m☑\u001b[0m Q 779+416 T 1195 \u001b[91m☒\u001b[0m Q 34+96   T 130  \u001b[91m☒\u001b[0m Q 461+301 T 762  \u001b[91m☒\u001b[0m Q 531+3   T 534  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.5110 - acc: 0.8221 - val_loss: 0.4356 - val_acc: 0.8536\n",
      "Q 304+1   T 305  \u001b[91m☒\u001b[0m Q 172+301 T 473  \u001b[91m☒\u001b[0m Q 19+199  T 218  \u001b[91m☒\u001b[0m Q 583+26  T 609  \u001b[92m☑\u001b[0m Q 235+339 T 574  \u001b[91m☒\u001b[0m Q 6+142   T 148  \u001b[91m☒\u001b[0m Q 193+127 T 320  \u001b[91m☒\u001b[0m Q 29+293  T 322  \u001b[91m☒\u001b[0m Q 902+153 T 1055 \u001b[91m☒\u001b[0m Q 14+388  T 402  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.3637 - acc: 0.8866 - val_loss: 0.3101 - val_acc: 0.9121\n",
      "Q 75+315  T 390  \u001b[92m☑\u001b[0m Q 2+114   T 116  \u001b[92m☑\u001b[0m Q 750+244 T 994  \u001b[92m☑\u001b[0m Q 53+135  T 188  \u001b[92m☑\u001b[0m Q 39+590  T 629  \u001b[92m☑\u001b[0m Q 193+85  T 278  \u001b[92m☑\u001b[0m Q 712+87  T 799  \u001b[92m☑\u001b[0m Q 370+228 T 598  \u001b[92m☑\u001b[0m Q 453+37  T 490  \u001b[92m☑\u001b[0m Q 761+7   T 768  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.2601 - acc: 0.9329 - val_loss: 0.2259 - val_acc: 0.9476\n",
      "Q 31+209  T 240  \u001b[92m☑\u001b[0m Q 800+952 T 1752 \u001b[91m☒\u001b[0m Q 795+24  T 819  \u001b[92m☑\u001b[0m Q 674+668 T 1342 \u001b[92m☑\u001b[0m Q 63+85   T 148  \u001b[92m☑\u001b[0m Q 834+409 T 1243 \u001b[92m☑\u001b[0m Q 292+700 T 992  \u001b[92m☑\u001b[0m Q 8+629   T 637  \u001b[92m☑\u001b[0m Q 764+282 T 1046 \u001b[92m☑\u001b[0m Q 85+187  T 272  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.1878 - acc: 0.9603 - val_loss: 0.1780 - val_acc: 0.9577\n",
      "Q 773+511 T 1284 \u001b[92m☑\u001b[0m Q 336+527 T 863  \u001b[92m☑\u001b[0m Q 755+30  T 785  \u001b[92m☑\u001b[0m Q 32+724  T 756  \u001b[92m☑\u001b[0m Q 773+4   T 777  \u001b[92m☑\u001b[0m Q 2+42    T 44   \u001b[92m☑\u001b[0m Q 998+21  T 1019 \u001b[92m☑\u001b[0m Q 698+676 T 1374 \u001b[91m☒\u001b[0m Q 238+80  T 318  \u001b[92m☑\u001b[0m Q 73+60   T 133  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.1460 - acc: 0.9711 - val_loss: 0.1520 - val_acc: 0.9634\n",
      "Q 60+442  T 502  \u001b[92m☑\u001b[0m Q 50+563  T 613  \u001b[92m☑\u001b[0m Q 707+782 T 1489 \u001b[92m☑\u001b[0m Q 30+233  T 263  \u001b[92m☑\u001b[0m Q 48+385  T 433  \u001b[92m☑\u001b[0m Q 243+880 T 1123 \u001b[92m☑\u001b[0m Q 85+16   T 101  \u001b[92m☑\u001b[0m Q 575+76  T 651  \u001b[92m☑\u001b[0m Q 265+0   T 265  \u001b[92m☑\u001b[0m Q 46+464  T 510  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.1072 - acc: 0.9824 - val_loss: 0.1039 - val_acc: 0.9810\n",
      "Q 792+810 T 1602 \u001b[92m☑\u001b[0m Q 81+200  T 281  \u001b[92m☑\u001b[0m Q 978+66  T 1044 \u001b[92m☑\u001b[0m Q 3+983   T 986  \u001b[92m☑\u001b[0m Q 624+76  T 700  \u001b[91m☒\u001b[0m Q 91+354  T 445  \u001b[92m☑\u001b[0m Q 833+353 T 1186 \u001b[92m☑\u001b[0m Q 19+189  T 208  \u001b[92m☑\u001b[0m Q 8+750   T 758  \u001b[92m☑\u001b[0m Q 336+827 T 1163 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0884 - acc: 0.9851 - val_loss: 0.0857 - val_acc: 0.9847\n",
      "Q 483+0   T 483  \u001b[92m☑\u001b[0m Q 51+568  T 619  \u001b[92m☑\u001b[0m Q 94+86   T 180  \u001b[92m☑\u001b[0m Q 2+439   T 441  \u001b[92m☑\u001b[0m Q 64+73   T 137  \u001b[92m☑\u001b[0m Q 81+934  T 1015 \u001b[92m☑\u001b[0m Q 370+228 T 598  \u001b[92m☑\u001b[0m Q 195+339 T 534  \u001b[92m☑\u001b[0m Q 496+94  T 590  \u001b[92m☑\u001b[0m Q 92+40   T 132  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 20\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0833 - acc: 0.9836 - val_loss: 0.0663 - val_acc: 0.9899\n",
      "Q 608+3   T 611  \u001b[92m☑\u001b[0m Q 841+16  T 857  \u001b[92m☑\u001b[0m Q 92+709  T 801  \u001b[92m☑\u001b[0m Q 49+34   T 83   \u001b[92m☑\u001b[0m Q 628+5   T 633  \u001b[92m☑\u001b[0m Q 840+969 T 1809 \u001b[92m☑\u001b[0m Q 641+73  T 714  \u001b[92m☑\u001b[0m Q 714+38  T 752  \u001b[92m☑\u001b[0m Q 90+328  T 418  \u001b[92m☑\u001b[0m Q 52+956  T 1008 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 21\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0637 - acc: 0.9895 - val_loss: 0.0574 - val_acc: 0.9911\n",
      "Q 63+178  T 241  \u001b[92m☑\u001b[0m Q 447+559 T 1006 \u001b[92m☑\u001b[0m Q 13+512  T 525  \u001b[92m☑\u001b[0m Q 872+336 T 1208 \u001b[92m☑\u001b[0m Q 12+475  T 487  \u001b[92m☑\u001b[0m Q 297+86  T 383  \u001b[92m☑\u001b[0m Q 56+535  T 591  \u001b[92m☑\u001b[0m Q 268+43  T 311  \u001b[92m☑\u001b[0m Q 95+859  T 954  \u001b[92m☑\u001b[0m Q 9+776   T 785  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 22\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0474 - acc: 0.9937 - val_loss: 0.0598 - val_acc: 0.9881\n",
      "Q 97+743  T 840  \u001b[91m☒\u001b[0m Q 512+488 T 1000 \u001b[91m☒\u001b[0m Q 91+4    T 95   \u001b[92m☑\u001b[0m Q 521+8   T 529  \u001b[92m☑\u001b[0m Q 9+859   T 868  \u001b[92m☑\u001b[0m Q 2+381   T 383  \u001b[92m☑\u001b[0m Q 569+17  T 586  \u001b[92m☑\u001b[0m Q 909+247 T 1156 \u001b[92m☑\u001b[0m Q 35+452  T 487  \u001b[92m☑\u001b[0m Q 368+306 T 674  \u001b[91m☒\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 23\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0621 - acc: 0.9859 - val_loss: 0.0471 - val_acc: 0.9916\n",
      "Q 3+553   T 556  \u001b[92m☑\u001b[0m Q 292+515 T 807  \u001b[91m☒\u001b[0m Q 56+384  T 440  \u001b[92m☑\u001b[0m Q 215+546 T 761  \u001b[92m☑\u001b[0m Q 568+60  T 628  \u001b[92m☑\u001b[0m Q 545+46  T 591  \u001b[92m☑\u001b[0m Q 9+570   T 579  \u001b[92m☑\u001b[0m Q 113+227 T 340  \u001b[92m☑\u001b[0m Q 536+774 T 1310 \u001b[92m☑\u001b[0m Q 836+546 T 1382 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 24\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0305 - acc: 0.9973 - val_loss: 0.0330 - val_acc: 0.9952\n",
      "Q 963+54  T 1017 \u001b[92m☑\u001b[0m Q 73+944  T 1017 \u001b[92m☑\u001b[0m Q 312+82  T 394  \u001b[92m☑\u001b[0m Q 798+805 T 1603 \u001b[92m☑\u001b[0m Q 43+532  T 575  \u001b[92m☑\u001b[0m Q 97+800  T 897  \u001b[92m☑\u001b[0m Q 848+28  T 876  \u001b[92m☑\u001b[0m Q 421+20  T 441  \u001b[92m☑\u001b[0m Q 750+59  T 809  \u001b[92m☑\u001b[0m Q 1+970   T 971  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 25\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0256 - acc: 0.9978 - val_loss: 0.0312 - val_acc: 0.9948\n",
      "Q 164+19  T 183  \u001b[92m☑\u001b[0m Q 89+200  T 289  \u001b[92m☑\u001b[0m Q 35+540  T 575  \u001b[92m☑\u001b[0m Q 459+324 T 783  \u001b[92m☑\u001b[0m Q 423+62  T 485  \u001b[92m☑\u001b[0m Q 89+888  T 977  \u001b[92m☑\u001b[0m Q 90+94   T 184  \u001b[92m☑\u001b[0m Q 982+892 T 1874 \u001b[92m☑\u001b[0m Q 34+355  T 389  \u001b[92m☑\u001b[0m Q 448+0   T 448  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 26\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0476 - acc: 0.9882 - val_loss: 0.0333 - val_acc: 0.9946\n",
      "Q 482+1   T 483  \u001b[92m☑\u001b[0m Q 699+6   T 705  \u001b[92m☑\u001b[0m Q 478+16  T 494  \u001b[92m☑\u001b[0m Q 489+65  T 554  \u001b[92m☑\u001b[0m Q 6+997   T 1003 \u001b[92m☑\u001b[0m Q 18+35   T 53   \u001b[92m☑\u001b[0m Q 74+275  T 349  \u001b[92m☑\u001b[0m Q 541+924 T 1465 \u001b[92m☑\u001b[0m Q 919+555 T 1474 \u001b[92m☑\u001b[0m Q 950+712 T 1662 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 27\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0198 - acc: 0.9984 - val_loss: 0.0215 - val_acc: 0.9974\n",
      "Q 27+293  T 320  \u001b[92m☑\u001b[0m Q 625+48  T 673  \u001b[92m☑\u001b[0m Q 833+5   T 838  \u001b[92m☑\u001b[0m Q 758+733 T 1491 \u001b[92m☑\u001b[0m Q 79+84   T 163  \u001b[92m☑\u001b[0m Q 441+691 T 1132 \u001b[92m☑\u001b[0m Q 92+173  T 265  \u001b[92m☑\u001b[0m Q 558+95  T 653  \u001b[92m☑\u001b[0m Q 764+57  T 821  \u001b[92m☑\u001b[0m Q 121+29  T 150  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 28\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0164 - acc: 0.9988 - val_loss: 0.0209 - val_acc: 0.9969\n",
      "Q 414+85  T 499  \u001b[92m☑\u001b[0m Q 237+202 T 439  \u001b[92m☑\u001b[0m Q 94+498  T 592  \u001b[92m☑\u001b[0m Q 451+0   T 451  \u001b[92m☑\u001b[0m Q 56+29   T 85   \u001b[92m☑\u001b[0m Q 27+55   T 82   \u001b[92m☑\u001b[0m Q 6+59    T 65   \u001b[92m☑\u001b[0m Q 350+496 T 846  \u001b[92m☑\u001b[0m Q 382+29  T 411  \u001b[92m☑\u001b[0m Q 926+89  T 1015 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 29\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0298 - acc: 0.9939 - val_loss: 0.2711 - val_acc: 0.9139\n",
      "Q 9+850   T 859  \u001b[92m☑\u001b[0m Q 10+769  T 779  \u001b[92m☑\u001b[0m Q 382+17  T 399  \u001b[92m☑\u001b[0m Q 79+478  T 557  \u001b[91m☒\u001b[0m Q 79+930  T 1009 \u001b[92m☑\u001b[0m Q 45+883  T 928  \u001b[92m☑\u001b[0m Q 0+63    T 63   \u001b[91m☒\u001b[0m Q 858+44  T 902  \u001b[92m☑\u001b[0m Q 158+494 T 652  \u001b[91m☒\u001b[0m Q 556+70  T 626  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 30\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0495 - acc: 0.9872 - val_loss: 0.0205 - val_acc: 0.9967\n",
      "Q 973+3   T 976  \u001b[92m☑\u001b[0m Q 841+44  T 885  \u001b[92m☑\u001b[0m Q 386+508 T 894  \u001b[92m☑\u001b[0m Q 384+73  T 457  \u001b[92m☑\u001b[0m Q 8+844   T 852  \u001b[92m☑\u001b[0m Q 955+64  T 1019 \u001b[92m☑\u001b[0m Q 25+289  T 314  \u001b[92m☑\u001b[0m Q 201+945 T 1146 \u001b[92m☑\u001b[0m Q 144+295 T 439  \u001b[92m☑\u001b[0m Q 19+32   T 51   \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 31\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0114 - acc: 0.9994 - val_loss: 0.0142 - val_acc: 0.9982\n",
      "Q 399+662 T 1061 \u001b[92m☑\u001b[0m Q 2+802   T 804  \u001b[92m☑\u001b[0m Q 732+55  T 787  \u001b[92m☑\u001b[0m Q 87+757  T 844  \u001b[92m☑\u001b[0m Q 45+906  T 951  \u001b[92m☑\u001b[0m Q 926+6   T 932  \u001b[92m☑\u001b[0m Q 82+967  T 1049 \u001b[92m☑\u001b[0m Q 19+474  T 493  \u001b[92m☑\u001b[0m Q 98+741  T 839  \u001b[92m☑\u001b[0m Q 22+891  T 913  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 32\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0095 - acc: 0.9995 - val_loss: 0.0133 - val_acc: 0.9980\n",
      "Q 621+54  T 675  \u001b[92m☑\u001b[0m Q 251+8   T 259  \u001b[92m☑\u001b[0m Q 116+187 T 303  \u001b[92m☑\u001b[0m Q 72+44   T 116  \u001b[92m☑\u001b[0m Q 952+42  T 994  \u001b[92m☑\u001b[0m Q 765+9   T 774  \u001b[92m☑\u001b[0m Q 101+4   T 105  \u001b[92m☑\u001b[0m Q 49+603  T 652  \u001b[92m☑\u001b[0m Q 88+783  T 871  \u001b[92m☑\u001b[0m Q 606+67  T 673  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 33\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0101 - acc: 0.9991 - val_loss: 0.0487 - val_acc: 0.9843\n",
      "Q 29+299  T 328  \u001b[92m☑\u001b[0m Q 3+401   T 404  \u001b[92m☑\u001b[0m Q 267+904 T 1171 \u001b[92m☑\u001b[0m Q 39+876  T 915  \u001b[92m☑\u001b[0m Q 279+280 T 559  \u001b[92m☑\u001b[0m Q 20+226  T 246  \u001b[92m☑\u001b[0m Q 926+6   T 932  \u001b[92m☑\u001b[0m Q 15+793  T 808  \u001b[92m☑\u001b[0m Q 280+97  T 377  \u001b[92m☑\u001b[0m Q 95+240  T 335  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 34\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0617 - acc: 0.9810 - val_loss: 0.0158 - val_acc: 0.9972\n",
      "Q 83+277  T 360  \u001b[92m☑\u001b[0m Q 314+3   T 317  \u001b[92m☑\u001b[0m Q 227+70  T 297  \u001b[92m☑\u001b[0m Q 113+81  T 194  \u001b[92m☑\u001b[0m Q 617+514 T 1131 \u001b[92m☑\u001b[0m Q 656+72  T 728  \u001b[92m☑\u001b[0m Q 918+88  T 1006 \u001b[92m☑\u001b[0m Q 872+53  T 925  \u001b[92m☑\u001b[0m Q 41+549  T 590  \u001b[92m☑\u001b[0m Q 854+753 T 1607 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 35\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0083 - acc: 0.9995 - val_loss: 0.0106 - val_acc: 0.9985\n",
      "Q 50+563  T 613  \u001b[92m☑\u001b[0m Q 97+743  T 840  \u001b[92m☑\u001b[0m Q 630+22  T 652  \u001b[92m☑\u001b[0m Q 2+99    T 101  \u001b[92m☑\u001b[0m Q 672+422 T 1094 \u001b[92m☑\u001b[0m Q 65+784  T 849  \u001b[92m☑\u001b[0m Q 730+47  T 777  \u001b[92m☑\u001b[0m Q 42+582  T 624  \u001b[92m☑\u001b[0m Q 97+31   T 128  \u001b[92m☑\u001b[0m Q 877+898 T 1775 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 36\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0066 - acc: 0.9998 - val_loss: 0.0093 - val_acc: 0.9988\n",
      "Q 532+5   T 537  \u001b[92m☑\u001b[0m Q 310+929 T 1239 \u001b[92m☑\u001b[0m Q 606+8   T 614  \u001b[92m☑\u001b[0m Q 20+405  T 425  \u001b[92m☑\u001b[0m Q 8+881   T 889  \u001b[92m☑\u001b[0m Q 60+199  T 259  \u001b[92m☑\u001b[0m Q 7+175   T 182  \u001b[92m☑\u001b[0m Q 37+403  T 440  \u001b[92m☑\u001b[0m Q 73+357  T 430  \u001b[92m☑\u001b[0m Q 60+495  T 555  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 37\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0061 - acc: 0.9997 - val_loss: 0.0096 - val_acc: 0.9980\n",
      "Q 21+172  T 193  \u001b[92m☑\u001b[0m Q 761+95  T 856  \u001b[92m☑\u001b[0m Q 517+77  T 594  \u001b[92m☑\u001b[0m Q 682+74  T 756  \u001b[92m☑\u001b[0m Q 640+964 T 1604 \u001b[92m☑\u001b[0m Q 794+1   T 795  \u001b[92m☑\u001b[0m Q 750+821 T 1571 \u001b[92m☑\u001b[0m Q 63+165  T 228  \u001b[92m☑\u001b[0m Q 221+46  T 267  \u001b[92m☑\u001b[0m Q 263+98  T 361  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 38\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0411 - acc: 0.9875 - val_loss: 0.0199 - val_acc: 0.9950\n",
      "Q 223+6   T 229  \u001b[92m☑\u001b[0m Q 361+77  T 438  \u001b[92m☑\u001b[0m Q 337+1   T 338  \u001b[92m☑\u001b[0m Q 581+2   T 583  \u001b[92m☑\u001b[0m Q 75+220  T 295  \u001b[92m☑\u001b[0m Q 74+85   T 159  \u001b[92m☑\u001b[0m Q 50+900  T 950  \u001b[92m☑\u001b[0m Q 277+485 T 762  \u001b[92m☑\u001b[0m Q 466+202 T 668  \u001b[92m☑\u001b[0m Q 451+7   T 458  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 39\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0075 - acc: 0.9994 - val_loss: 0.0090 - val_acc: 0.9987\n",
      "Q 775+651 T 1426 \u001b[92m☑\u001b[0m Q 9+859   T 868  \u001b[92m☑\u001b[0m Q 2+972   T 974  \u001b[92m☑\u001b[0m Q 2+964   T 966  \u001b[92m☑\u001b[0m Q 31+496  T 527  \u001b[92m☑\u001b[0m Q 471+822 T 1293 \u001b[92m☑\u001b[0m Q 83+3    T 86   \u001b[92m☑\u001b[0m Q 25+43   T 68   \u001b[92m☑\u001b[0m Q 507+29  T 536  \u001b[92m☑\u001b[0m Q 28+512  T 540  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 40\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0048 - acc: 0.9998 - val_loss: 0.0073 - val_acc: 0.9990\n",
      "Q 7+230   T 237  \u001b[92m☑\u001b[0m Q 390+538 T 928  \u001b[92m☑\u001b[0m Q 526+89  T 615  \u001b[92m☑\u001b[0m Q 43+835  T 878  \u001b[92m☑\u001b[0m Q 54+167  T 221  \u001b[92m☑\u001b[0m Q 404+881 T 1285 \u001b[92m☑\u001b[0m Q 25+289  T 314  \u001b[92m☑\u001b[0m Q 271+588 T 859  \u001b[92m☑\u001b[0m Q 302+774 T 1076 \u001b[92m☑\u001b[0m Q 80+519  T 599  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 41\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0042 - acc: 0.9998 - val_loss: 0.0071 - val_acc: 0.9988\n",
      "Q 145+27  T 172  \u001b[92m☑\u001b[0m Q 210+656 T 866  \u001b[92m☑\u001b[0m Q 971+87  T 1058 \u001b[92m☑\u001b[0m Q 66+97   T 163  \u001b[92m☑\u001b[0m Q 81+51   T 132  \u001b[92m☑\u001b[0m Q 158+64  T 222  \u001b[92m☑\u001b[0m Q 765+53  T 818  \u001b[92m☑\u001b[0m Q 62+94   T 156  \u001b[92m☑\u001b[0m Q 212+910 T 1122 \u001b[92m☑\u001b[0m Q 937+709 T 1646 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 42\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0041 - acc: 0.9998 - val_loss: 0.0069 - val_acc: 0.9989\n",
      "Q 779+8   T 787  \u001b[92m☑\u001b[0m Q 447+559 T 1006 \u001b[92m☑\u001b[0m Q 76+259  T 335  \u001b[92m☑\u001b[0m Q 611+391 T 1002 \u001b[92m☑\u001b[0m Q 3+934   T 937  \u001b[92m☑\u001b[0m Q 599+454 T 1053 \u001b[92m☑\u001b[0m Q 281+884 T 1165 \u001b[92m☑\u001b[0m Q 2+976   T 978  \u001b[92m☑\u001b[0m Q 656+36  T 692  \u001b[92m☑\u001b[0m Q 8+690   T 698  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 43\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0038 - acc: 0.9998 - val_loss: 0.0078 - val_acc: 0.9986\n",
      "Q 12+92   T 104  \u001b[92m☑\u001b[0m Q 71+124  T 195  \u001b[92m☑\u001b[0m Q 812+372 T 1184 \u001b[92m☑\u001b[0m Q 551+74  T 625  \u001b[92m☑\u001b[0m Q 28+512  T 540  \u001b[92m☑\u001b[0m Q 112+41  T 153  \u001b[92m☑\u001b[0m Q 835+70  T 905  \u001b[92m☑\u001b[0m Q 92+168  T 260  \u001b[92m☑\u001b[0m Q 434+68  T 502  \u001b[92m☑\u001b[0m Q 955+64  T 1019 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 44\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0490 - acc: 0.9853 - val_loss: 0.0090 - val_acc: 0.9983\n",
      "Q 717+928 T 1645 \u001b[92m☑\u001b[0m Q 357+255 T 612  \u001b[92m☑\u001b[0m Q 274+962 T 1236 \u001b[92m☑\u001b[0m Q 2+802   T 804  \u001b[92m☑\u001b[0m Q 989+296 T 1285 \u001b[92m☑\u001b[0m Q 606+766 T 1372 \u001b[92m☑\u001b[0m Q 142+34  T 176  \u001b[92m☑\u001b[0m Q 119+23  T 142  \u001b[92m☑\u001b[0m Q 989+516 T 1505 \u001b[92m☑\u001b[0m Q 885+85  T 970  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 45\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0045 - acc: 0.9998 - val_loss: 0.0067 - val_acc: 0.9989\n",
      "Q 59+583  T 642  \u001b[92m☑\u001b[0m Q 5+861   T 866  \u001b[92m☑\u001b[0m Q 0+722   T 722  \u001b[92m☑\u001b[0m Q 270+687 T 957  \u001b[92m☑\u001b[0m Q 51+198  T 249  \u001b[92m☑\u001b[0m Q 9+277   T 286  \u001b[92m☑\u001b[0m Q 34+254  T 288  \u001b[92m☑\u001b[0m Q 91+346  T 437  \u001b[92m☑\u001b[0m Q 37+494  T 531  \u001b[92m☑\u001b[0m Q 40+69   T 109  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 46\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0032 - acc: 0.9999 - val_loss: 0.0055 - val_acc: 0.9991\n",
      "Q 760+714 T 1474 \u001b[92m☑\u001b[0m Q 321+21  T 342  \u001b[92m☑\u001b[0m Q 946+14  T 960  \u001b[92m☑\u001b[0m Q 1+829   T 830  \u001b[92m☑\u001b[0m Q 803+630 T 1433 \u001b[92m☑\u001b[0m Q 11+872  T 883  \u001b[92m☑\u001b[0m Q 77+796  T 873  \u001b[92m☑\u001b[0m Q 450+559 T 1009 \u001b[92m☑\u001b[0m Q 779+416 T 1195 \u001b[92m☑\u001b[0m Q 95+984  T 1079 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 47\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0029 - acc: 0.9999 - val_loss: 0.0073 - val_acc: 0.9981\n",
      "Q 228+10  T 238  \u001b[92m☑\u001b[0m Q 892+845 T 1737 \u001b[92m☑\u001b[0m Q 359+91  T 450  \u001b[92m☑\u001b[0m Q 27+105  T 132  \u001b[92m☑\u001b[0m Q 53+222  T 275  \u001b[92m☑\u001b[0m Q 812+74  T 886  \u001b[92m☑\u001b[0m Q 7+908   T 915  \u001b[92m☑\u001b[0m Q 643+498 T 1141 \u001b[92m☑\u001b[0m Q 649+13  T 662  \u001b[92m☑\u001b[0m Q 243+639 T 882  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 48\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0028 - acc: 0.9998 - val_loss: 0.0050 - val_acc: 0.9991\n",
      "Q 271+940 T 1211 \u001b[92m☑\u001b[0m Q 839+419 T 1258 \u001b[92m☑\u001b[0m Q 528+35  T 563  \u001b[92m☑\u001b[0m Q 378+7   T 385  \u001b[92m☑\u001b[0m Q 47+555  T 602  \u001b[92m☑\u001b[0m Q 414+3   T 417  \u001b[92m☑\u001b[0m Q 2+668   T 670  \u001b[92m☑\u001b[0m Q 4+227   T 231  \u001b[92m☑\u001b[0m Q 171+23  T 194  \u001b[92m☑\u001b[0m Q 700+533 T 1233 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 49\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0520 - acc: 0.9844 - val_loss: 0.0217 - val_acc: 0.9941\n",
      "Q 814+990 T 1804 \u001b[92m☑\u001b[0m Q 778+18  T 796  \u001b[92m☑\u001b[0m Q 818+2   T 820  \u001b[92m☑\u001b[0m Q 98+52   T 150  \u001b[92m☑\u001b[0m Q 5+168   T 173  \u001b[92m☑\u001b[0m Q 923+779 T 1702 \u001b[92m☑\u001b[0m Q 68+868  T 936  \u001b[92m☑\u001b[0m Q 97+693  T 790  \u001b[92m☑\u001b[0m Q 12+392  T 404  \u001b[92m☑\u001b[0m Q 5+700   T 705  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 50\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0060 - acc: 0.9992 - val_loss: 0.0058 - val_acc: 0.9992\n",
      "Q 43+919  T 962  \u001b[92m☑\u001b[0m Q 942+85  T 1027 \u001b[92m☑\u001b[0m Q 67+826  T 893  \u001b[92m☑\u001b[0m Q 92+51   T 143  \u001b[92m☑\u001b[0m Q 637+577 T 1214 \u001b[92m☑\u001b[0m Q 325+8   T 333  \u001b[92m☑\u001b[0m Q 6+197   T 203  \u001b[92m☑\u001b[0m Q 199+215 T 414  \u001b[92m☑\u001b[0m Q 163+504 T 667  \u001b[92m☑\u001b[0m Q 307+5   T 312  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 51\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0027 - acc: 0.9999 - val_loss: 0.0048 - val_acc: 0.9994\n",
      "Q 6+142   T 148  \u001b[92m☑\u001b[0m Q 860+91  T 951  \u001b[92m☑\u001b[0m Q 358+814 T 1172 \u001b[92m☑\u001b[0m Q 211+31  T 242  \u001b[92m☑\u001b[0m Q 739+82  T 821  \u001b[92m☑\u001b[0m Q 841+753 T 1594 \u001b[92m☑\u001b[0m Q 36+184  T 220  \u001b[92m☑\u001b[0m Q 49+1    T 50   \u001b[92m☑\u001b[0m Q 77+644  T 721  \u001b[92m☑\u001b[0m Q 335+0   T 335  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 52\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0024 - acc: 0.9999 - val_loss: 0.0042 - val_acc: 0.9994\n",
      "Q 841+73  T 914  \u001b[92m☑\u001b[0m Q 360+95  T 455  \u001b[92m☑\u001b[0m Q 294+52  T 346  \u001b[92m☑\u001b[0m Q 559+695 T 1254 \u001b[92m☑\u001b[0m Q 99+36   T 135  \u001b[92m☑\u001b[0m Q 24+273  T 297  \u001b[92m☑\u001b[0m Q 128+987 T 1115 \u001b[92m☑\u001b[0m Q 146+5   T 151  \u001b[92m☑\u001b[0m Q 325+70  T 395  \u001b[92m☑\u001b[0m Q 200+736 T 936  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 53\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0021 - acc: 0.9999 - val_loss: 0.0043 - val_acc: 0.9993\n",
      "Q 34+355  T 389  \u001b[92m☑\u001b[0m Q 961+177 T 1138 \u001b[92m☑\u001b[0m Q 43+892  T 935  \u001b[92m☑\u001b[0m Q 906+94  T 1000 \u001b[92m☑\u001b[0m Q 918+3   T 921  \u001b[92m☑\u001b[0m Q 811+447 T 1258 \u001b[92m☑\u001b[0m Q 494+899 T 1393 \u001b[92m☑\u001b[0m Q 143+945 T 1088 \u001b[92m☑\u001b[0m Q 92+268  T 360  \u001b[92m☑\u001b[0m Q 580+20  T 600  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 54\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0122 - acc: 0.9967 - val_loss: 0.2125 - val_acc: 0.9336\n",
      "Q 52+16   T 68   \u001b[92m☑\u001b[0m Q 66+8    T 74   \u001b[92m☑\u001b[0m Q 5+453   T 458  \u001b[92m☑\u001b[0m Q 985+0   T 985  \u001b[92m☑\u001b[0m Q 398+923 T 1321 \u001b[92m☑\u001b[0m Q 994+5   T 999  \u001b[92m☑\u001b[0m Q 828+876 T 1704 \u001b[92m☑\u001b[0m Q 806+4   T 810  \u001b[92m☑\u001b[0m Q 268+43  T 311  \u001b[92m☑\u001b[0m Q 73+357  T 430  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 55\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0418 - acc: 0.9877 - val_loss: 0.0058 - val_acc: 0.9990\n",
      "Q 158+64  T 222  \u001b[92m☑\u001b[0m Q 73+258  T 331  \u001b[92m☑\u001b[0m Q 106+817 T 923  \u001b[92m☑\u001b[0m Q 473+468 T 941  \u001b[92m☑\u001b[0m Q 514+136 T 650  \u001b[92m☑\u001b[0m Q 868+70  T 938  \u001b[92m☑\u001b[0m Q 4+761   T 765  \u001b[92m☑\u001b[0m Q 2+998   T 1000 \u001b[92m☑\u001b[0m Q 870+33  T 903  \u001b[92m☑\u001b[0m Q 56+29   T 85   \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 56\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0026 - acc: 0.9999 - val_loss: 0.0042 - val_acc: 0.9992\n",
      "Q 87+62   T 149  \u001b[92m☑\u001b[0m Q 66+62   T 128  \u001b[92m☑\u001b[0m Q 335+0   T 335  \u001b[92m☑\u001b[0m Q 177+13  T 190  \u001b[92m☑\u001b[0m Q 1+441   T 442  \u001b[92m☑\u001b[0m Q 478+14  T 492  \u001b[92m☑\u001b[0m Q 87+287  T 374  \u001b[92m☑\u001b[0m Q 631+4   T 635  \u001b[92m☑\u001b[0m Q 9+202   T 211  \u001b[92m☑\u001b[0m Q 387+820 T 1207 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 57\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 0.9994\n",
      "Q 266+637 T 903  \u001b[92m☑\u001b[0m Q 286+95  T 381  \u001b[92m☑\u001b[0m Q 93+266  T 359  \u001b[92m☑\u001b[0m Q 243+32  T 275  \u001b[92m☑\u001b[0m Q 529+51  T 580  \u001b[92m☑\u001b[0m Q 686+4   T 690  \u001b[92m☑\u001b[0m Q 892+366 T 1258 \u001b[92m☑\u001b[0m Q 17+728  T 745  \u001b[92m☑\u001b[0m Q 30+801  T 831  \u001b[92m☑\u001b[0m Q 96+289  T 385  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 58\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0018 - acc: 0.9999 - val_loss: 0.0035 - val_acc: 0.9994\n",
      "Q 332+7   T 339  \u001b[92m☑\u001b[0m Q 597+375 T 972  \u001b[92m☑\u001b[0m Q 16+8    T 24   \u001b[92m☑\u001b[0m Q 685+361 T 1046 \u001b[92m☑\u001b[0m Q 410+2   T 412  \u001b[92m☑\u001b[0m Q 352+409 T 761  \u001b[92m☑\u001b[0m Q 976+272 T 1248 \u001b[92m☑\u001b[0m Q 315+5   T 320  \u001b[92m☑\u001b[0m Q 15+15   T 30   \u001b[92m☑\u001b[0m Q 630+99  T 729  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 59\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0016 - acc: 0.9999 - val_loss: 0.0040 - val_acc: 0.9992\n",
      "Q 81+287  T 368  \u001b[92m☑\u001b[0m Q 79+853  T 932  \u001b[92m☑\u001b[0m Q 510+13  T 523  \u001b[92m☑\u001b[0m Q 31+674  T 705  \u001b[92m☑\u001b[0m Q 338+177 T 515  \u001b[92m☑\u001b[0m Q 119+74  T 193  \u001b[92m☑\u001b[0m Q 109+446 T 555  \u001b[92m☑\u001b[0m Q 52+114  T 166  \u001b[92m☑\u001b[0m Q 250+270 T 520  \u001b[92m☑\u001b[0m Q 38+590  T 628  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 60\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0016 - acc: 0.9999 - val_loss: 0.0040 - val_acc: 0.9991\n",
      "Q 5+453   T 458  \u001b[92m☑\u001b[0m Q 88+5    T 93   \u001b[92m☑\u001b[0m Q 693+649 T 1342 \u001b[92m☑\u001b[0m Q 711+345 T 1056 \u001b[92m☑\u001b[0m Q 349+588 T 937  \u001b[92m☑\u001b[0m Q 893+221 T 1114 \u001b[92m☑\u001b[0m Q 773+511 T 1284 \u001b[92m☑\u001b[0m Q 90+901  T 991  \u001b[92m☑\u001b[0m Q 595+608 T 1203 \u001b[92m☑\u001b[0m Q 746+48  T 794  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 61\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0449 - acc: 0.9860 - val_loss: 0.0068 - val_acc: 0.9987\n",
      "Q 906+53  T 959  \u001b[92m☑\u001b[0m Q 734+780 T 1514 \u001b[92m☑\u001b[0m Q 48+559  T 607  \u001b[92m☑\u001b[0m Q 679+3   T 682  \u001b[92m☑\u001b[0m Q 578+537 T 1115 \u001b[92m☑\u001b[0m Q 34+35   T 69   \u001b[92m☑\u001b[0m Q 87+62   T 149  \u001b[92m☑\u001b[0m Q 982+892 T 1874 \u001b[92m☑\u001b[0m Q 908+26  T 934  \u001b[92m☑\u001b[0m Q 540+27  T 567  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 62\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0027 - acc: 0.9999 - val_loss: 0.0039 - val_acc: 0.9997\n",
      "Q 6+416   T 422  \u001b[92m☑\u001b[0m Q 243+880 T 1123 \u001b[92m☑\u001b[0m Q 280+97  T 377  \u001b[92m☑\u001b[0m Q 959+9   T 968  \u001b[92m☑\u001b[0m Q 46+49   T 95   \u001b[92m☑\u001b[0m Q 6+145   T 151  \u001b[92m☑\u001b[0m Q 19+645  T 664  \u001b[92m☑\u001b[0m Q 22+15   T 37   \u001b[92m☑\u001b[0m Q 585+973 T 1558 \u001b[92m☑\u001b[0m Q 56+515  T 571  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 63\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 0.9997\n",
      "Q 27+1    T 28   \u001b[92m☑\u001b[0m Q 616+766 T 1382 \u001b[92m☑\u001b[0m Q 621+75  T 696  \u001b[92m☑\u001b[0m Q 78+408  T 486  \u001b[92m☑\u001b[0m Q 94+186  T 280  \u001b[92m☑\u001b[0m Q 708+304 T 1012 \u001b[92m☑\u001b[0m Q 616+3   T 619  \u001b[92m☑\u001b[0m Q 891+8   T 899  \u001b[92m☑\u001b[0m Q 406+4   T 410  \u001b[92m☑\u001b[0m Q 58+629  T 687  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 64\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9997\n",
      "Q 74+871  T 945  \u001b[92m☑\u001b[0m Q 630+22  T 652  \u001b[92m☑\u001b[0m Q 234+60  T 294  \u001b[92m☑\u001b[0m Q 24+38   T 62   \u001b[92m☑\u001b[0m Q 73+6    T 79   \u001b[92m☑\u001b[0m Q 708+0   T 708  \u001b[92m☑\u001b[0m Q 495+79  T 574  \u001b[92m☑\u001b[0m Q 33+952  T 985  \u001b[92m☑\u001b[0m Q 556+70  T 626  \u001b[92m☑\u001b[0m Q 317+289 T 606  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 65\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.0032 - val_acc: 0.9995\n",
      "Q 429+70  T 499  \u001b[92m☑\u001b[0m Q 405+16  T 421  \u001b[92m☑\u001b[0m Q 215+303 T 518  \u001b[92m☑\u001b[0m Q 68+541  T 609  \u001b[92m☑\u001b[0m Q 674+64  T 738  \u001b[92m☑\u001b[0m Q 87+885  T 972  \u001b[92m☑\u001b[0m Q 3+934   T 937  \u001b[92m☑\u001b[0m Q 337+76  T 413  \u001b[92m☑\u001b[0m Q 242+158 T 400  \u001b[92m☑\u001b[0m Q 236+457 T 693  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 66\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0266 - acc: 0.9920 - val_loss: 0.0939 - val_acc: 0.9669\n",
      "Q 509+4   T 513  \u001b[92m☑\u001b[0m Q 525+680 T 1205 \u001b[92m☑\u001b[0m Q 498+235 T 733  \u001b[92m☑\u001b[0m Q 112+41  T 153  \u001b[92m☑\u001b[0m Q 563+60  T 623  \u001b[92m☑\u001b[0m Q 936+4   T 940  \u001b[92m☑\u001b[0m Q 606+7   T 613  \u001b[92m☑\u001b[0m Q 674+64  T 738  \u001b[92m☑\u001b[0m Q 357+22  T 379  \u001b[91m☒\u001b[0m Q 977+837 T 1814 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 67\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0148 - acc: 0.9959 - val_loss: 0.0052 - val_acc: 0.9991\n",
      "Q 83+346  T 429  \u001b[92m☑\u001b[0m Q 786+378 T 1164 \u001b[92m☑\u001b[0m Q 506+53  T 559  \u001b[92m☑\u001b[0m Q 293+504 T 797  \u001b[92m☑\u001b[0m Q 6+641   T 647  \u001b[92m☑\u001b[0m Q 764+57  T 821  \u001b[92m☑\u001b[0m Q 529+55  T 584  \u001b[92m☑\u001b[0m Q 3+866   T 869  \u001b[92m☑\u001b[0m Q 8+185   T 193  \u001b[92m☑\u001b[0m Q 71+109  T 180  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 68\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 0.9994\n",
      "Q 774+807 T 1581 \u001b[92m☑\u001b[0m Q 8+394   T 402  \u001b[92m☑\u001b[0m Q 29+505  T 534  \u001b[92m☑\u001b[0m Q 995+847 T 1842 \u001b[92m☑\u001b[0m Q 652+354 T 1006 \u001b[92m☑\u001b[0m Q 48+612  T 660  \u001b[92m☑\u001b[0m Q 706+184 T 890  \u001b[92m☑\u001b[0m Q 901+63  T 964  \u001b[92m☑\u001b[0m Q 752+933 T 1685 \u001b[92m☑\u001b[0m Q 830+538 T 1368 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 69\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9994\n",
      "Q 85+378  T 463  \u001b[92m☑\u001b[0m Q 534+83  T 617  \u001b[92m☑\u001b[0m Q 4+600   T 604  \u001b[92m☑\u001b[0m Q 84+428  T 512  \u001b[92m☑\u001b[0m Q 458+936 T 1394 \u001b[92m☑\u001b[0m Q 842+6   T 848  \u001b[92m☑\u001b[0m Q 233+7   T 240  \u001b[92m☑\u001b[0m Q 851+623 T 1474 \u001b[92m☑\u001b[0m Q 76+259  T 335  \u001b[92m☑\u001b[0m Q 569+6   T 575  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 70\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 0.9996\n",
      "Q 85+780  T 865  \u001b[92m☑\u001b[0m Q 928+7   T 935  \u001b[92m☑\u001b[0m Q 841+73  T 914  \u001b[92m☑\u001b[0m Q 20+24   T 44   \u001b[92m☑\u001b[0m Q 42+91   T 133  \u001b[92m☑\u001b[0m Q 607+552 T 1159 \u001b[92m☑\u001b[0m Q 639+71  T 710  \u001b[92m☑\u001b[0m Q 639+6   T 645  \u001b[92m☑\u001b[0m Q 80+687  T 767  \u001b[92m☑\u001b[0m Q 476+396 T 872  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 71\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.0037 - val_acc: 0.9993\n",
      "Q 75+147  T 222  \u001b[92m☑\u001b[0m Q 367+56  T 423  \u001b[92m☑\u001b[0m Q 646+6   T 652  \u001b[92m☑\u001b[0m Q 30+974  T 1004 \u001b[92m☑\u001b[0m Q 229+815 T 1044 \u001b[92m☑\u001b[0m Q 677+448 T 1125 \u001b[92m☑\u001b[0m Q 300+49  T 349  \u001b[92m☑\u001b[0m Q 510+55  T 565  \u001b[92m☑\u001b[0m Q 75+330  T 405  \u001b[92m☑\u001b[0m Q 227+547 T 774  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 72\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0404 - acc: 0.9886 - val_loss: 0.0053 - val_acc: 0.9991\n",
      "Q 456+933 T 1389 \u001b[92m☑\u001b[0m Q 91+243  T 334  \u001b[92m☑\u001b[0m Q 49+194  T 243  \u001b[92m☑\u001b[0m Q 0+606   T 606  \u001b[92m☑\u001b[0m Q 0+393   T 393  \u001b[92m☑\u001b[0m Q 672+81  T 753  \u001b[92m☑\u001b[0m Q 56+17   T 73   \u001b[92m☑\u001b[0m Q 681+1   T 682  \u001b[92m☑\u001b[0m Q 668+89  T 757  \u001b[92m☑\u001b[0m Q 1+473   T 474  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 73\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 0.9997\n",
      "Q 144+26  T 170  \u001b[92m☑\u001b[0m Q 23+275  T 298  \u001b[92m☑\u001b[0m Q 325+163 T 488  \u001b[92m☑\u001b[0m Q 690+91  T 781  \u001b[92m☑\u001b[0m Q 48+947  T 995  \u001b[92m☑\u001b[0m Q 5+619   T 624  \u001b[92m☑\u001b[0m Q 690+603 T 1293 \u001b[92m☑\u001b[0m Q 9+47    T 56   \u001b[92m☑\u001b[0m Q 28+969  T 997  \u001b[92m☑\u001b[0m Q 483+32  T 515  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 74\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 0.9997\n",
      "Q 610+6   T 616  \u001b[92m☑\u001b[0m Q 173+84  T 257  \u001b[92m☑\u001b[0m Q 466+202 T 668  \u001b[92m☑\u001b[0m Q 570+1   T 571  \u001b[92m☑\u001b[0m Q 690+545 T 1235 \u001b[92m☑\u001b[0m Q 49+292  T 341  \u001b[92m☑\u001b[0m Q 97+618  T 715  \u001b[92m☑\u001b[0m Q 5+252   T 257  \u001b[92m☑\u001b[0m Q 19+915  T 934  \u001b[92m☑\u001b[0m Q 79+40   T 119  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 75\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 0.9996\n",
      "Q 866+6   T 872  \u001b[92m☑\u001b[0m Q 736+246 T 982  \u001b[92m☑\u001b[0m Q 350+515 T 865  \u001b[92m☑\u001b[0m Q 91+187  T 278  \u001b[92m☑\u001b[0m Q 2+42    T 44   \u001b[92m☑\u001b[0m Q 536+758 T 1294 \u001b[92m☑\u001b[0m Q 895+468 T 1363 \u001b[92m☑\u001b[0m Q 524+6   T 530  \u001b[92m☑\u001b[0m Q 516+75  T 591  \u001b[92m☑\u001b[0m Q 608+3   T 611  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 76\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 0.9996\n",
      "Q 560+44  T 604  \u001b[92m☑\u001b[0m Q 989+516 T 1505 \u001b[92m☑\u001b[0m Q 963+54  T 1017 \u001b[92m☑\u001b[0m Q 547+3   T 550  \u001b[92m☑\u001b[0m Q 174+816 T 990  \u001b[92m☑\u001b[0m Q 94+56   T 150  \u001b[92m☑\u001b[0m Q 353+68  T 421  \u001b[92m☑\u001b[0m Q 768+744 T 1512 \u001b[92m☑\u001b[0m Q 310+929 T 1239 \u001b[92m☑\u001b[0m Q 869+272 T 1141 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 77\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 102us/step - loss: 9.5912e-04 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9991\n",
      "Q 406+92  T 498  \u001b[92m☑\u001b[0m Q 283+8   T 291  \u001b[92m☑\u001b[0m Q 847+23  T 870  \u001b[92m☑\u001b[0m Q 512+7   T 519  \u001b[92m☑\u001b[0m Q 32+52   T 84   \u001b[92m☑\u001b[0m Q 352+371 T 723  \u001b[92m☑\u001b[0m Q 6+577   T 583  \u001b[92m☑\u001b[0m Q 5+861   T 866  \u001b[92m☑\u001b[0m Q 502+24  T 526  \u001b[92m☑\u001b[0m Q 584+66  T 650  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 78\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 8.4884e-04 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 0.9993\n",
      "Q 0+367   T 367  \u001b[92m☑\u001b[0m Q 42+406  T 448  \u001b[92m☑\u001b[0m Q 599+481 T 1080 \u001b[92m☑\u001b[0m Q 750+244 T 994  \u001b[92m☑\u001b[0m Q 327+48  T 375  \u001b[92m☑\u001b[0m Q 57+40   T 97   \u001b[92m☑\u001b[0m Q 755+30  T 785  \u001b[92m☑\u001b[0m Q 287+797 T 1084 \u001b[92m☑\u001b[0m Q 995+96  T 1091 \u001b[92m☑\u001b[0m Q 68+369  T 437  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 79\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0408 - acc: 0.9875 - val_loss: 0.0273 - val_acc: 0.9907\n",
      "Q 72+86   T 158  \u001b[92m☑\u001b[0m Q 25+289  T 314  \u001b[92m☑\u001b[0m Q 19+910  T 929  \u001b[92m☑\u001b[0m Q 66+71   T 137  \u001b[92m☑\u001b[0m Q 72+53   T 125  \u001b[92m☑\u001b[0m Q 315+828 T 1143 \u001b[92m☑\u001b[0m Q 117+748 T 865  \u001b[92m☑\u001b[0m Q 381+10  T 391  \u001b[92m☑\u001b[0m Q 743+702 T 1445 \u001b[92m☑\u001b[0m Q 426+351 T 777  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 80\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0043 - acc: 0.9993 - val_loss: 0.0036 - val_acc: 0.9994\n",
      "Q 652+354 T 1006 \u001b[92m☑\u001b[0m Q 80+403  T 483  \u001b[92m☑\u001b[0m Q 878+541 T 1419 \u001b[92m☑\u001b[0m Q 76+532  T 608  \u001b[92m☑\u001b[0m Q 739+36  T 775  \u001b[92m☑\u001b[0m Q 761+95  T 856  \u001b[92m☑\u001b[0m Q 74+409  T 483  \u001b[92m☑\u001b[0m Q 284+980 T 1264 \u001b[92m☑\u001b[0m Q 3+94    T 97   \u001b[92m☑\u001b[0m Q 476+11  T 487  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 81\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 0.9995\n",
      "Q 626+37  T 663  \u001b[92m☑\u001b[0m Q 47+367  T 414  \u001b[92m☑\u001b[0m Q 670+529 T 1199 \u001b[92m☑\u001b[0m Q 627+68  T 695  \u001b[92m☑\u001b[0m Q 83+80   T 163  \u001b[92m☑\u001b[0m Q 30+912  T 942  \u001b[92m☑\u001b[0m Q 40+774  T 814  \u001b[92m☑\u001b[0m Q 61+403  T 464  \u001b[92m☑\u001b[0m Q 859+1   T 860  \u001b[92m☑\u001b[0m Q 191+97  T 288  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 82\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9995\n",
      "Q 880+54  T 934  \u001b[92m☑\u001b[0m Q 433+85  T 518  \u001b[92m☑\u001b[0m Q 988+72  T 1060 \u001b[92m☑\u001b[0m Q 946+316 T 1262 \u001b[92m☑\u001b[0m Q 50+893  T 943  \u001b[92m☑\u001b[0m Q 290+629 T 919  \u001b[92m☑\u001b[0m Q 47+665  T 712  \u001b[92m☑\u001b[0m Q 39+557  T 596  \u001b[92m☑\u001b[0m Q 491+3   T 494  \u001b[92m☑\u001b[0m Q 2+381   T 383  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 83\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 8.6651e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 0.9997\n",
      "Q 7+604   T 611  \u001b[92m☑\u001b[0m Q 80+18   T 98   \u001b[92m☑\u001b[0m Q 187+567 T 754  \u001b[92m☑\u001b[0m Q 484+1   T 485  \u001b[92m☑\u001b[0m Q 763+298 T 1061 \u001b[92m☑\u001b[0m Q 130+89  T 219  \u001b[92m☑\u001b[0m Q 898+197 T 1095 \u001b[92m☑\u001b[0m Q 0+326   T 326  \u001b[92m☑\u001b[0m Q 28+328  T 356  \u001b[92m☑\u001b[0m Q 547+3   T 550  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 84\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 8.1137e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9994\n",
      "Q 702+979 T 1681 \u001b[92m☑\u001b[0m Q 632+19  T 651  \u001b[92m☑\u001b[0m Q 30+390  T 420  \u001b[92m☑\u001b[0m Q 820+300 T 1120 \u001b[92m☑\u001b[0m Q 257+75  T 332  \u001b[92m☑\u001b[0m Q 549+92  T 641  \u001b[92m☑\u001b[0m Q 29+70   T 99   \u001b[92m☑\u001b[0m Q 841+42  T 883  \u001b[92m☑\u001b[0m Q 82+870  T 952  \u001b[92m☑\u001b[0m Q 911+38  T 949  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 85\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 7.1844e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9996\n",
      "Q 35+946  T 981  \u001b[92m☑\u001b[0m Q 47+845  T 892  \u001b[92m☑\u001b[0m Q 948+87  T 1035 \u001b[92m☑\u001b[0m Q 62+646  T 708  \u001b[92m☑\u001b[0m Q 405+54  T 459  \u001b[92m☑\u001b[0m Q 572+634 T 1206 \u001b[92m☑\u001b[0m Q 8+166   T 174  \u001b[92m☑\u001b[0m Q 45+56   T 101  \u001b[92m☑\u001b[0m Q 494+899 T 1393 \u001b[92m☑\u001b[0m Q 32+52   T 84   \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 86\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0082 - val_acc: 0.9972\n",
      "Q 227+914 T 1141 \u001b[92m☑\u001b[0m Q 639+6   T 645  \u001b[92m☑\u001b[0m Q 358+191 T 549  \u001b[92m☑\u001b[0m Q 287+797 T 1084 \u001b[92m☑\u001b[0m Q 621+54  T 675  \u001b[92m☑\u001b[0m Q 841+44  T 885  \u001b[92m☑\u001b[0m Q 253+136 T 389  \u001b[92m☑\u001b[0m Q 73+669  T 742  \u001b[92m☑\u001b[0m Q 48+947  T 995  \u001b[92m☑\u001b[0m Q 512+9   T 521  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 87\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0452 - acc: 0.9864 - val_loss: 0.0060 - val_acc: 0.9987\n",
      "Q 432+88  T 520  \u001b[92m☑\u001b[0m Q 337+0   T 337  \u001b[92m☑\u001b[0m Q 292+24  T 316  \u001b[92m☑\u001b[0m Q 393+17  T 410  \u001b[92m☑\u001b[0m Q 635+540 T 1175 \u001b[92m☑\u001b[0m Q 553+740 T 1293 \u001b[92m☑\u001b[0m Q 16+36   T 52   \u001b[92m☑\u001b[0m Q 67+150  T 217  \u001b[92m☑\u001b[0m Q 49+99   T 148  \u001b[92m☑\u001b[0m Q 303+795 T 1098 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 88\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0016 - acc: 0.9999 - val_loss: 0.0028 - val_acc: 0.9995\n",
      "Q 60+17   T 77   \u001b[92m☑\u001b[0m Q 24+227  T 251  \u001b[92m☑\u001b[0m Q 850+81  T 931  \u001b[92m☑\u001b[0m Q 127+545 T 672  \u001b[92m☑\u001b[0m Q 56+282  T 338  \u001b[92m☑\u001b[0m Q 65+662  T 727  \u001b[92m☑\u001b[0m Q 26+725  T 751  \u001b[92m☑\u001b[0m Q 549+3   T 552  \u001b[92m☑\u001b[0m Q 963+179 T 1142 \u001b[92m☑\u001b[0m Q 875+962 T 1837 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 89\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 9.6438e-04 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9996\n",
      "Q 270+687 T 957  \u001b[92m☑\u001b[0m Q 67+456  T 523  \u001b[92m☑\u001b[0m Q 552+318 T 870  \u001b[92m☑\u001b[0m Q 161+7   T 168  \u001b[92m☑\u001b[0m Q 74+409  T 483  \u001b[92m☑\u001b[0m Q 61+78   T 139  \u001b[92m☑\u001b[0m Q 73+60   T 133  \u001b[92m☑\u001b[0m Q 73+260  T 333  \u001b[92m☑\u001b[0m Q 48+185  T 233  \u001b[92m☑\u001b[0m Q 690+91  T 781  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 90\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 7.8806e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9996\n",
      "Q 773+511 T 1284 \u001b[92m☑\u001b[0m Q 690+3   T 693  \u001b[92m☑\u001b[0m Q 52+60   T 112  \u001b[92m☑\u001b[0m Q 109+53  T 162  \u001b[92m☑\u001b[0m Q 832+81  T 913  \u001b[92m☑\u001b[0m Q 10+720  T 730  \u001b[92m☑\u001b[0m Q 96+968  T 1064 \u001b[92m☑\u001b[0m Q 1+440   T 441  \u001b[92m☑\u001b[0m Q 904+968 T 1872 \u001b[92m☑\u001b[0m Q 536+186 T 722  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 91\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 6.9665e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9994\n",
      "Q 9+725   T 734  \u001b[92m☑\u001b[0m Q 402+300 T 702  \u001b[92m☑\u001b[0m Q 166+87  T 253  \u001b[92m☑\u001b[0m Q 78+14   T 92   \u001b[92m☑\u001b[0m Q 227+914 T 1141 \u001b[92m☑\u001b[0m Q 673+48  T 721  \u001b[92m☑\u001b[0m Q 247+77  T 324  \u001b[92m☑\u001b[0m Q 7+118   T 125  \u001b[92m☑\u001b[0m Q 215+6   T 221  \u001b[92m☑\u001b[0m Q 96+948  T 1044 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 92\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 6.2176e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9995\n",
      "Q 250+299 T 549  \u001b[92m☑\u001b[0m Q 333+273 T 606  \u001b[92m☑\u001b[0m Q 8+4     T 12   \u001b[92m☑\u001b[0m Q 223+835 T 1058 \u001b[92m☑\u001b[0m Q 710+217 T 927  \u001b[92m☑\u001b[0m Q 435+7   T 442  \u001b[92m☑\u001b[0m Q 598+718 T 1316 \u001b[92m☑\u001b[0m Q 9+184   T 193  \u001b[92m☑\u001b[0m Q 36+7    T 43   \u001b[92m☑\u001b[0m Q 19+922  T 941  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 93\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 5s 101us/step - loss: 5.8933e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9995\n",
      "Q 1+518   T 519  \u001b[92m☑\u001b[0m Q 874+342 T 1216 \u001b[92m☑\u001b[0m Q 774+807 T 1581 \u001b[92m☑\u001b[0m Q 5+568   T 573  \u001b[92m☑\u001b[0m Q 88+65   T 153  \u001b[92m☑\u001b[0m Q 803+14  T 817  \u001b[92m☑\u001b[0m Q 622+814 T 1436 \u001b[92m☑\u001b[0m Q 227+603 T 830  \u001b[92m☑\u001b[0m Q 695+50  T 745  \u001b[92m☑\u001b[0m Q 880+54  T 934  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 94\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0130 - acc: 0.9964 - val_loss: 0.1569 - val_acc: 0.9524\n",
      "Q 59+463  T 522  \u001b[92m☑\u001b[0m Q 103+74  T 177  \u001b[92m☑\u001b[0m Q 526+398 T 924  \u001b[92m☑\u001b[0m Q 418+23  T 441  \u001b[92m☑\u001b[0m Q 275+11  T 286  \u001b[92m☑\u001b[0m Q 952+17  T 969  \u001b[92m☑\u001b[0m Q 82+112  T 194  \u001b[92m☑\u001b[0m Q 853+621 T 1474 \u001b[91m☒\u001b[0m Q 229+815 T 1044 \u001b[92m☑\u001b[0m Q 218+511 T 729  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 95\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0231 - acc: 0.9932 - val_loss: 0.0040 - val_acc: 0.9994\n",
      "Q 3+933   T 936  \u001b[92m☑\u001b[0m Q 498+235 T 733  \u001b[92m☑\u001b[0m Q 596+61  T 657  \u001b[92m☑\u001b[0m Q 19+476  T 495  \u001b[92m☑\u001b[0m Q 537+0   T 537  \u001b[92m☑\u001b[0m Q 2+690   T 692  \u001b[92m☑\u001b[0m Q 33+387  T 420  \u001b[92m☑\u001b[0m Q 272+211 T 483  \u001b[92m☑\u001b[0m Q 59+42   T 101  \u001b[92m☑\u001b[0m Q 5+523   T 528  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 96\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9996\n",
      "Q 491+3   T 494  \u001b[92m☑\u001b[0m Q 2+762   T 764  \u001b[92m☑\u001b[0m Q 10+829  T 839  \u001b[92m☑\u001b[0m Q 389+996 T 1385 \u001b[92m☑\u001b[0m Q 899+400 T 1299 \u001b[92m☑\u001b[0m Q 4+198   T 202  \u001b[92m☑\u001b[0m Q 8+724   T 732  \u001b[92m☑\u001b[0m Q 17+528  T 545  \u001b[92m☑\u001b[0m Q 65+153  T 218  \u001b[92m☑\u001b[0m Q 699+6   T 705  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 97\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 7.4669e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9996\n",
      "Q 234+12  T 246  \u001b[92m☑\u001b[0m Q 456+933 T 1389 \u001b[92m☑\u001b[0m Q 28+436  T 464  \u001b[92m☑\u001b[0m Q 66+985  T 1051 \u001b[92m☑\u001b[0m Q 842+6   T 848  \u001b[92m☑\u001b[0m Q 393+497 T 890  \u001b[92m☑\u001b[0m Q 789+16  T 805  \u001b[92m☑\u001b[0m Q 906+1   T 907  \u001b[92m☑\u001b[0m Q 290+629 T 919  \u001b[92m☑\u001b[0m Q 177+85  T 262  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 98\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 6.3131e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9996\n",
      "Q 38+400  T 438  \u001b[92m☑\u001b[0m Q 517+89  T 606  \u001b[92m☑\u001b[0m Q 231+11  T 242  \u001b[92m☑\u001b[0m Q 907+1   T 908  \u001b[92m☑\u001b[0m Q 72+55   T 127  \u001b[92m☑\u001b[0m Q 624+36  T 660  \u001b[92m☑\u001b[0m Q 278+473 T 751  \u001b[92m☑\u001b[0m Q 306+8   T 314  \u001b[92m☑\u001b[0m Q 803+630 T 1433 \u001b[92m☑\u001b[0m Q 80+846  T 926  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 99\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 5.6383e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9996\n",
      "Q 420+851 T 1271 \u001b[92m☑\u001b[0m Q 5+57    T 62   \u001b[92m☑\u001b[0m Q 637+5   T 642  \u001b[92m☑\u001b[0m Q 619+8   T 627  \u001b[92m☑\u001b[0m Q 585+497 T 1082 \u001b[92m☑\u001b[0m Q 27+24   T 51   \u001b[92m☑\u001b[0m Q 626+37  T 663  \u001b[92m☑\u001b[0m Q 121+29  T 150  \u001b[92m☑\u001b[0m Q 905+97  T 1002 \u001b[92m☑\u001b[0m Q 297+86  T 383  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 100\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 6.7258e-04 - acc: 0.9999 - val_loss: 0.0024 - val_acc: 0.9996\n",
      "Q 3+532   T 535  \u001b[92m☑\u001b[0m Q 278+473 T 751  \u001b[92m☑\u001b[0m Q 144+26  T 170  \u001b[92m☑\u001b[0m Q 394+63  T 457  \u001b[92m☑\u001b[0m Q 28+910  T 938  \u001b[92m☑\u001b[0m Q 695+50  T 745  \u001b[92m☑\u001b[0m Q 31+483  T 514  \u001b[92m☑\u001b[0m Q 2+972   T 974  \u001b[92m☑\u001b[0m Q 22+678  T 700  \u001b[92m☑\u001b[0m Q 98+137  T 235  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 101\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0194 - acc: 0.9940 - val_loss: 0.0466 - val_acc: 0.9830\n",
      "Q 383+355 T 738  \u001b[92m☑\u001b[0m Q 670+529 T 1199 \u001b[92m☑\u001b[0m Q 339+289 T 628  \u001b[92m☑\u001b[0m Q 13+929  T 942  \u001b[92m☑\u001b[0m Q 547+18  T 565  \u001b[92m☑\u001b[0m Q 396+81  T 477  \u001b[92m☑\u001b[0m Q 795+97  T 892  \u001b[92m☑\u001b[0m Q 78+408  T 486  \u001b[92m☑\u001b[0m Q 58+46   T 104  \u001b[92m☑\u001b[0m Q 10+872  T 882  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 102\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0196 - acc: 0.9945 - val_loss: 0.0037 - val_acc: 0.9993\n",
      "Q 235+50  T 285  \u001b[92m☑\u001b[0m Q 85+512  T 597  \u001b[92m☑\u001b[0m Q 954+631 T 1585 \u001b[92m☑\u001b[0m Q 3+933   T 936  \u001b[92m☑\u001b[0m Q 891+668 T 1559 \u001b[92m☑\u001b[0m Q 770+322 T 1092 \u001b[92m☑\u001b[0m Q 2+633   T 635  \u001b[92m☑\u001b[0m Q 328+71  T 399  \u001b[92m☑\u001b[0m Q 74+644  T 718  \u001b[92m☑\u001b[0m Q 117+46  T 163  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 103\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9996\n",
      "Q 86+429  T 515  \u001b[92m☑\u001b[0m Q 99+182  T 281  \u001b[92m☑\u001b[0m Q 892+366 T 1258 \u001b[92m☑\u001b[0m Q 605+394 T 999  \u001b[92m☑\u001b[0m Q 238+93  T 331  \u001b[92m☑\u001b[0m Q 396+81  T 477  \u001b[92m☑\u001b[0m Q 6+835   T 841  \u001b[92m☑\u001b[0m Q 61+78   T 139  \u001b[92m☑\u001b[0m Q 740+699 T 1439 \u001b[92m☑\u001b[0m Q 63+178  T 241  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 104\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 6.8956e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9996\n",
      "Q 43+45   T 88   \u001b[92m☑\u001b[0m Q 83+618  T 701  \u001b[92m☑\u001b[0m Q 544+15  T 559  \u001b[92m☑\u001b[0m Q 296+241 T 537  \u001b[92m☑\u001b[0m Q 526+72  T 598  \u001b[92m☑\u001b[0m Q 67+656  T 723  \u001b[92m☑\u001b[0m Q 104+15  T 119  \u001b[92m☑\u001b[0m Q 604+260 T 864  \u001b[92m☑\u001b[0m Q 507+867 T 1374 \u001b[92m☑\u001b[0m Q 183+61  T 244  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 105\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 5.7517e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9997\n",
      "Q 926+8   T 934  \u001b[92m☑\u001b[0m Q 901+44  T 945  \u001b[92m☑\u001b[0m Q 26+6    T 32   \u001b[92m☑\u001b[0m Q 73+707  T 780  \u001b[92m☑\u001b[0m Q 53+22   T 75   \u001b[92m☑\u001b[0m Q 41+407  T 448  \u001b[92m☑\u001b[0m Q 11+431  T 442  \u001b[92m☑\u001b[0m Q 94+821  T 915  \u001b[92m☑\u001b[0m Q 877+206 T 1083 \u001b[92m☑\u001b[0m Q 280+97  T 377  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 106\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 5.1156e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9996\n",
      "Q 40+85   T 125  \u001b[92m☑\u001b[0m Q 297+742 T 1039 \u001b[92m☑\u001b[0m Q 561+69  T 630  \u001b[92m☑\u001b[0m Q 367+58  T 425  \u001b[92m☑\u001b[0m Q 851+92  T 943  \u001b[92m☑\u001b[0m Q 63+794  T 857  \u001b[92m☑\u001b[0m Q 200+736 T 936  \u001b[92m☑\u001b[0m Q 606+67  T 673  \u001b[92m☑\u001b[0m Q 192+516 T 708  \u001b[92m☑\u001b[0m Q 563+25  T 588  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 107\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 4.6194e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Q 973+814 T 1787 \u001b[92m☑\u001b[0m Q 877+21  T 898  \u001b[92m☑\u001b[0m Q 260+213 T 473  \u001b[92m☑\u001b[0m Q 846+73  T 919  \u001b[92m☑\u001b[0m Q 351+718 T 1069 \u001b[92m☑\u001b[0m Q 70+986  T 1056 \u001b[92m☑\u001b[0m Q 34+86   T 120  \u001b[92m☑\u001b[0m Q 559+85  T 644  \u001b[92m☑\u001b[0m Q 85+605  T 690  \u001b[92m☑\u001b[0m Q 48+545  T 593  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 108\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 4.1488e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9997\n",
      "Q 83+199  T 282  \u001b[92m☑\u001b[0m Q 98+76   T 174  \u001b[92m☑\u001b[0m Q 369+476 T 845  \u001b[92m☑\u001b[0m Q 841+20  T 861  \u001b[92m☑\u001b[0m Q 1+189   T 190  \u001b[92m☑\u001b[0m Q 305+4   T 309  \u001b[92m☑\u001b[0m Q 281+884 T 1165 \u001b[92m☑\u001b[0m Q 369+61  T 430  \u001b[92m☑\u001b[0m Q 80+775  T 855  \u001b[92m☑\u001b[0m Q 441+768 T 1209 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 109\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 3.9230e-04 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 0.9994\n",
      "Q 30+170  T 200  \u001b[92m☑\u001b[0m Q 478+14  T 492  \u001b[92m☑\u001b[0m Q 665+755 T 1420 \u001b[92m☑\u001b[0m Q 672+557 T 1229 \u001b[92m☑\u001b[0m Q 57+882  T 939  \u001b[92m☑\u001b[0m Q 73+646  T 719  \u001b[92m☑\u001b[0m Q 693+649 T 1342 \u001b[92m☑\u001b[0m Q 273+63  T 336  \u001b[92m☑\u001b[0m Q 942+85  T 1027 \u001b[92m☑\u001b[0m Q 80+775  T 855  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 110\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0287 - acc: 0.9915 - val_loss: 0.0039 - val_acc: 0.9993\n",
      "Q 238+69  T 307  \u001b[92m☑\u001b[0m Q 38+704  T 742  \u001b[92m☑\u001b[0m Q 86+785  T 871  \u001b[92m☑\u001b[0m Q 441+691 T 1132 \u001b[92m☑\u001b[0m Q 904+387 T 1291 \u001b[92m☑\u001b[0m Q 3+306   T 309  \u001b[92m☑\u001b[0m Q 907+1   T 908  \u001b[92m☑\u001b[0m Q 629+741 T 1370 \u001b[92m☑\u001b[0m Q 974+41  T 1015 \u001b[92m☑\u001b[0m Q 866+6   T 872  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 111\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 0.9994\n",
      "Q 667+88  T 755  \u001b[92m☑\u001b[0m Q 873+93  T 966  \u001b[92m☑\u001b[0m Q 52+404  T 456  \u001b[92m☑\u001b[0m Q 996+75  T 1071 \u001b[92m☑\u001b[0m Q 358+814 T 1172 \u001b[92m☑\u001b[0m Q 36+40   T 76   \u001b[92m☑\u001b[0m Q 11+19   T 30   \u001b[92m☑\u001b[0m Q 888+718 T 1606 \u001b[92m☑\u001b[0m Q 2+122   T 124  \u001b[92m☑\u001b[0m Q 91+821  T 912  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 112\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 6.4008e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9996\n",
      "Q 24+744  T 768  \u001b[92m☑\u001b[0m Q 541+9   T 550  \u001b[92m☑\u001b[0m Q 73+944  T 1017 \u001b[92m☑\u001b[0m Q 88+446  T 534  \u001b[92m☑\u001b[0m Q 796+599 T 1395 \u001b[92m☑\u001b[0m Q 60+442  T 502  \u001b[92m☑\u001b[0m Q 778+137 T 915  \u001b[92m☑\u001b[0m Q 556+70  T 626  \u001b[92m☑\u001b[0m Q 91+858  T 949  \u001b[92m☑\u001b[0m Q 861+487 T 1348 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 113\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 5.0590e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9995\n",
      "Q 12+92   T 104  \u001b[92m☑\u001b[0m Q 573+696 T 1269 \u001b[92m☑\u001b[0m Q 189+74  T 263  \u001b[92m☑\u001b[0m Q 37+64   T 101  \u001b[92m☑\u001b[0m Q 335+199 T 534  \u001b[92m☑\u001b[0m Q 94+911  T 1005 \u001b[92m☑\u001b[0m Q 3+115   T 118  \u001b[92m☑\u001b[0m Q 88+15   T 103  \u001b[92m☑\u001b[0m Q 788+350 T 1138 \u001b[92m☑\u001b[0m Q 617+514 T 1131 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 114\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 4.4814e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9994\n",
      "Q 785+13  T 798  \u001b[92m☑\u001b[0m Q 94+56   T 150  \u001b[92m☑\u001b[0m Q 665+41  T 706  \u001b[92m☑\u001b[0m Q 317+750 T 1067 \u001b[92m☑\u001b[0m Q 828+488 T 1316 \u001b[92m☑\u001b[0m Q 884+55  T 939  \u001b[92m☑\u001b[0m Q 28+75   T 103  \u001b[92m☑\u001b[0m Q 4+372   T 376  \u001b[92m☑\u001b[0m Q 23+357  T 380  \u001b[92m☑\u001b[0m Q 367+58  T 425  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 115\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 3.9156e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9995\n",
      "Q 817+17  T 834  \u001b[92m☑\u001b[0m Q 812+167 T 979  \u001b[92m☑\u001b[0m Q 786+52  T 838  \u001b[92m☑\u001b[0m Q 54+167  T 221  \u001b[92m☑\u001b[0m Q 250+177 T 427  \u001b[92m☑\u001b[0m Q 43+234  T 277  \u001b[92m☑\u001b[0m Q 710+37  T 747  \u001b[92m☑\u001b[0m Q 96+80   T 176  \u001b[92m☑\u001b[0m Q 879+536 T 1415 \u001b[92m☑\u001b[0m Q 99+478  T 577  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 116\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 3.4414e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9995\n",
      "Q 215+983 T 1198 \u001b[91m☒\u001b[0m Q 17+415  T 432  \u001b[92m☑\u001b[0m Q 37+65   T 102  \u001b[92m☑\u001b[0m Q 19+693  T 712  \u001b[92m☑\u001b[0m Q 73+884  T 957  \u001b[92m☑\u001b[0m Q 429+465 T 894  \u001b[92m☑\u001b[0m Q 455+30  T 485  \u001b[92m☑\u001b[0m Q 809+767 T 1576 \u001b[92m☑\u001b[0m Q 3+36    T 39   \u001b[92m☑\u001b[0m Q 11+10   T 21   \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 117\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 3.0804e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Q 29+590  T 619  \u001b[92m☑\u001b[0m Q 768+94  T 862  \u001b[92m☑\u001b[0m Q 137+680 T 817  \u001b[92m☑\u001b[0m Q 98+484  T 582  \u001b[92m☑\u001b[0m Q 128+987 T 1115 \u001b[92m☑\u001b[0m Q 9+800   T 809  \u001b[92m☑\u001b[0m Q 152+913 T 1065 \u001b[92m☑\u001b[0m Q 710+74  T 784  \u001b[92m☑\u001b[0m Q 66+817  T 883  \u001b[92m☑\u001b[0m Q 96+853  T 949  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 118\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 2.8470e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9996\n",
      "Q 68+776  T 844  \u001b[92m☑\u001b[0m Q 72+974  T 1046 \u001b[92m☑\u001b[0m Q 96+853  T 949  \u001b[92m☑\u001b[0m Q 542+40  T 582  \u001b[92m☑\u001b[0m Q 956+3   T 959  \u001b[92m☑\u001b[0m Q 43+532  T 575  \u001b[92m☑\u001b[0m Q 280+97  T 377  \u001b[92m☑\u001b[0m Q 75+147  T 222  \u001b[92m☑\u001b[0m Q 1+518   T 519  \u001b[92m☑\u001b[0m Q 906+2   T 908  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 119\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0378 - acc: 0.9887 - val_loss: 0.0123 - val_acc: 0.9965\n",
      "Q 28+35   T 63   \u001b[92m☑\u001b[0m Q 142+34  T 176  \u001b[92m☑\u001b[0m Q 88+178  T 266  \u001b[92m☑\u001b[0m Q 414+85  T 499  \u001b[92m☑\u001b[0m Q 768+744 T 1512 \u001b[92m☑\u001b[0m Q 76+257  T 333  \u001b[92m☑\u001b[0m Q 42+91   T 133  \u001b[92m☑\u001b[0m Q 58+481  T 539  \u001b[92m☑\u001b[0m Q 959+13  T 972  \u001b[92m☑\u001b[0m Q 296+432 T 728  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 120\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0028 - acc: 0.9995 - val_loss: 0.0063 - val_acc: 0.9983\n",
      "Q 5+385   T 390  \u001b[92m☑\u001b[0m Q 975+498 T 1473 \u001b[91m☒\u001b[0m Q 714+38  T 752  \u001b[92m☑\u001b[0m Q 11+19   T 30   \u001b[92m☑\u001b[0m Q 52+25   T 77   \u001b[92m☑\u001b[0m Q 705+3   T 708  \u001b[92m☑\u001b[0m Q 91+493  T 584  \u001b[92m☑\u001b[0m Q 429+51  T 480  \u001b[92m☑\u001b[0m Q 848+894 T 1742 \u001b[92m☑\u001b[0m Q 4+600   T 604  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 121\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.0024 - val_acc: 0.9994\n",
      "Q 749+56  T 805  \u001b[92m☑\u001b[0m Q 22+849  T 871  \u001b[92m☑\u001b[0m Q 451+0   T 451  \u001b[92m☑\u001b[0m Q 765+70  T 835  \u001b[92m☑\u001b[0m Q 74+270  T 344  \u001b[92m☑\u001b[0m Q 468+2   T 470  \u001b[92m☑\u001b[0m Q 999+63  T 1062 \u001b[92m☑\u001b[0m Q 725+121 T 846  \u001b[92m☑\u001b[0m Q 264+68  T 332  \u001b[92m☑\u001b[0m Q 1+288   T 289  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 122\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 5.1667e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9994\n",
      "Q 798+805 T 1603 \u001b[92m☑\u001b[0m Q 274+52  T 326  \u001b[92m☑\u001b[0m Q 964+14  T 978  \u001b[92m☑\u001b[0m Q 271+301 T 572  \u001b[92m☑\u001b[0m Q 362+899 T 1261 \u001b[92m☑\u001b[0m Q 0+158   T 158  \u001b[92m☑\u001b[0m Q 606+7   T 613  \u001b[92m☑\u001b[0m Q 18+94   T 112  \u001b[92m☑\u001b[0m Q 38+704  T 742  \u001b[92m☑\u001b[0m Q 8+398   T 406  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 123\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 101us/step - loss: 4.2753e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9996\n",
      "Q 994+91  T 1085 \u001b[92m☑\u001b[0m Q 82+149  T 231  \u001b[92m☑\u001b[0m Q 9+105   T 114  \u001b[92m☑\u001b[0m Q 5+276   T 281  \u001b[92m☑\u001b[0m Q 629+741 T 1370 \u001b[92m☑\u001b[0m Q 57+98   T 155  \u001b[92m☑\u001b[0m Q 50+81   T 131  \u001b[92m☑\u001b[0m Q 13+9    T 22   \u001b[92m☑\u001b[0m Q 6+48    T 54   \u001b[92m☑\u001b[0m Q 37+363  T 400  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 124\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 5s 101us/step - loss: 3.7567e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9997\n",
      "Q 463+497 T 960  \u001b[92m☑\u001b[0m Q 620+5   T 625  \u001b[92m☑\u001b[0m Q 804+0   T 804  \u001b[92m☑\u001b[0m Q 73+353  T 426  \u001b[92m☑\u001b[0m Q 63+43   T 106  \u001b[92m☑\u001b[0m Q 86+155  T 241  \u001b[92m☑\u001b[0m Q 27+1    T 28   \u001b[92m☑\u001b[0m Q 978+27  T 1005 \u001b[92m☑\u001b[0m Q 400+597 T 997  \u001b[92m☑\u001b[0m Q 85+605  T 690  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 125\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 3.3243e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9997\n",
      "Q 9+800   T 809  \u001b[92m☑\u001b[0m Q 772+860 T 1632 \u001b[92m☑\u001b[0m Q 123+87  T 210  \u001b[92m☑\u001b[0m Q 91+605  T 696  \u001b[92m☑\u001b[0m Q 14+10   T 24   \u001b[92m☑\u001b[0m Q 25+960  T 985  \u001b[92m☑\u001b[0m Q 410+37  T 447  \u001b[92m☑\u001b[0m Q 88+85   T 173  \u001b[92m☑\u001b[0m Q 435+34  T 469  \u001b[92m☑\u001b[0m Q 961+828 T 1789 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 126\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 4.9742e-04 - acc: 0.9999 - val_loss: 0.0023 - val_acc: 0.9994\n",
      "Q 952+2   T 954  \u001b[92m☑\u001b[0m Q 423+491 T 914  \u001b[92m☑\u001b[0m Q 127+153 T 280  \u001b[92m☑\u001b[0m Q 79+18   T 97   \u001b[92m☑\u001b[0m Q 46+911  T 957  \u001b[92m☑\u001b[0m Q 716+915 T 1631 \u001b[92m☑\u001b[0m Q 43+732  T 775  \u001b[92m☑\u001b[0m Q 869+778 T 1647 \u001b[92m☑\u001b[0m Q 93+266  T 359  \u001b[92m☑\u001b[0m Q 485+7   T 492  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 127\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 6.2542e-04 - acc: 0.9999 - val_loss: 0.0038 - val_acc: 0.9990\n",
      "Q 869+272 T 1141 \u001b[92m☑\u001b[0m Q 130+42  T 172  \u001b[92m☑\u001b[0m Q 556+45  T 601  \u001b[92m☑\u001b[0m Q 68+733  T 801  \u001b[92m☑\u001b[0m Q 918+778 T 1696 \u001b[92m☑\u001b[0m Q 23+123  T 146  \u001b[92m☑\u001b[0m Q 638+311 T 949  \u001b[92m☑\u001b[0m Q 173+174 T 347  \u001b[92m☑\u001b[0m Q 40+218  T 258  \u001b[92m☑\u001b[0m Q 2+122   T 124  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 128\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0293 - acc: 0.9918 - val_loss: 0.0051 - val_acc: 0.9987\n",
      "Q 974+7   T 981  \u001b[92m☑\u001b[0m Q 79+465  T 544  \u001b[92m☑\u001b[0m Q 721+317 T 1038 \u001b[92m☑\u001b[0m Q 82+295  T 377  \u001b[92m☑\u001b[0m Q 144+921 T 1065 \u001b[92m☑\u001b[0m Q 392+138 T 530  \u001b[92m☑\u001b[0m Q 35+634  T 669  \u001b[92m☑\u001b[0m Q 60+236  T 296  \u001b[92m☑\u001b[0m Q 551+9   T 560  \u001b[92m☑\u001b[0m Q 958+56  T 1014 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 129\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 8.4501e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 0.9995\n",
      "Q 86+70   T 156  \u001b[92m☑\u001b[0m Q 71+109  T 180  \u001b[92m☑\u001b[0m Q 157+977 T 1134 \u001b[92m☑\u001b[0m Q 4+688   T 692  \u001b[92m☑\u001b[0m Q 57+98   T 155  \u001b[92m☑\u001b[0m Q 349+588 T 937  \u001b[92m☑\u001b[0m Q 997+33  T 1030 \u001b[92m☑\u001b[0m Q 457+3   T 460  \u001b[92m☑\u001b[0m Q 534+821 T 1355 \u001b[92m☑\u001b[0m Q 49+975  T 1024 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 130\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 4.9240e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9994\n",
      "Q 469+789 T 1258 \u001b[92m☑\u001b[0m Q 623+7   T 630  \u001b[92m☑\u001b[0m Q 830+48  T 878  \u001b[92m☑\u001b[0m Q 8+993   T 1001 \u001b[92m☑\u001b[0m Q 679+407 T 1086 \u001b[92m☑\u001b[0m Q 470+8   T 478  \u001b[92m☑\u001b[0m Q 5+39    T 44   \u001b[92m☑\u001b[0m Q 423+491 T 914  \u001b[92m☑\u001b[0m Q 601+120 T 721  \u001b[92m☑\u001b[0m Q 99+55   T 154  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 131\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 4.0165e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9995\n",
      "Q 497+783 T 1280 \u001b[92m☑\u001b[0m Q 91+605  T 696  \u001b[92m☑\u001b[0m Q 433+703 T 1136 \u001b[92m☑\u001b[0m Q 21+26   T 47   \u001b[92m☑\u001b[0m Q 594+87  T 681  \u001b[92m☑\u001b[0m Q 843+779 T 1622 \u001b[92m☑\u001b[0m Q 19+189  T 208  \u001b[92m☑\u001b[0m Q 851+4   T 855  \u001b[92m☑\u001b[0m Q 549+44  T 593  \u001b[92m☑\u001b[0m Q 110+425 T 535  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 132\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 3.4588e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9995\n",
      "Q 12+995  T 1007 \u001b[92m☑\u001b[0m Q 627+89  T 716  \u001b[92m☑\u001b[0m Q 298+99  T 397  \u001b[91m☒\u001b[0m Q 551+74  T 625  \u001b[92m☑\u001b[0m Q 726+672 T 1398 \u001b[92m☑\u001b[0m Q 837+492 T 1329 \u001b[92m☑\u001b[0m Q 858+44  T 902  \u001b[92m☑\u001b[0m Q 57+707  T 764  \u001b[92m☑\u001b[0m Q 126+649 T 775  \u001b[92m☑\u001b[0m Q 972+72  T 1044 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 133\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 3.0550e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Q 632+0   T 632  \u001b[92m☑\u001b[0m Q 84+11   T 95   \u001b[92m☑\u001b[0m Q 411+89  T 500  \u001b[92m☑\u001b[0m Q 117+46  T 163  \u001b[92m☑\u001b[0m Q 692+307 T 999  \u001b[92m☑\u001b[0m Q 196+0   T 196  \u001b[92m☑\u001b[0m Q 700+533 T 1233 \u001b[92m☑\u001b[0m Q 825+3   T 828  \u001b[92m☑\u001b[0m Q 737+67  T 804  \u001b[92m☑\u001b[0m Q 104+15  T 119  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 134\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.7152e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "Q 460+902 T 1362 \u001b[92m☑\u001b[0m Q 355+32  T 387  \u001b[92m☑\u001b[0m Q 99+946  T 1045 \u001b[92m☑\u001b[0m Q 481+33  T 514  \u001b[92m☑\u001b[0m Q 74+91   T 165  \u001b[92m☑\u001b[0m Q 581+2   T 583  \u001b[92m☑\u001b[0m Q 623+7   T 630  \u001b[92m☑\u001b[0m Q 112+325 T 437  \u001b[92m☑\u001b[0m Q 594+2   T 596  \u001b[92m☑\u001b[0m Q 310+685 T 995  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 135\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.4669e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9996\n",
      "Q 3+125   T 128  \u001b[92m☑\u001b[0m Q 58+491  T 549  \u001b[92m☑\u001b[0m Q 98+484  T 582  \u001b[92m☑\u001b[0m Q 596+483 T 1079 \u001b[92m☑\u001b[0m Q 755+30  T 785  \u001b[92m☑\u001b[0m Q 462+5   T 467  \u001b[92m☑\u001b[0m Q 334+929 T 1263 \u001b[92m☑\u001b[0m Q 451+7   T 458  \u001b[92m☑\u001b[0m Q 45+11   T 56   \u001b[92m☑\u001b[0m Q 12+475  T 487  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 136\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.2055e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Q 575+86  T 661  \u001b[92m☑\u001b[0m Q 402+300 T 702  \u001b[92m☑\u001b[0m Q 82+20   T 102  \u001b[92m☑\u001b[0m Q 9+166   T 175  \u001b[92m☑\u001b[0m Q 669+94  T 763  \u001b[92m☑\u001b[0m Q 7+753   T 760  \u001b[92m☑\u001b[0m Q 25+825  T 850  \u001b[92m☑\u001b[0m Q 9+47    T 56   \u001b[92m☑\u001b[0m Q 211+2   T 213  \u001b[92m☑\u001b[0m Q 208+65  T 273  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 137\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.0157e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9997\n",
      "Q 390+7   T 397  \u001b[92m☑\u001b[0m Q 67+488  T 555  \u001b[92m☑\u001b[0m Q 411+89  T 500  \u001b[92m☑\u001b[0m Q 295+8   T 303  \u001b[92m☑\u001b[0m Q 750+136 T 886  \u001b[92m☑\u001b[0m Q 420+417 T 837  \u001b[92m☑\u001b[0m Q 17+216  T 233  \u001b[92m☑\u001b[0m Q 7+655   T 662  \u001b[92m☑\u001b[0m Q 667+495 T 1162 \u001b[92m☑\u001b[0m Q 3+306   T 309  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 138\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0317 - acc: 0.9914 - val_loss: 0.0231 - val_acc: 0.9933\n",
      "Q 2+394   T 396  \u001b[92m☑\u001b[0m Q 770+26  T 796  \u001b[92m☑\u001b[0m Q 6+587   T 593  \u001b[92m☑\u001b[0m Q 67+972  T 1039 \u001b[92m☑\u001b[0m Q 227+586 T 813  \u001b[92m☑\u001b[0m Q 961+2   T 963  \u001b[92m☑\u001b[0m Q 924+45  T 969  \u001b[92m☑\u001b[0m Q 493+59  T 552  \u001b[92m☑\u001b[0m Q 5+796   T 801  \u001b[92m☑\u001b[0m Q 70+407  T 477  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 139\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.0022 - val_acc: 0.9996\n",
      "Q 330+717 T 1047 \u001b[92m☑\u001b[0m Q 258+13  T 271  \u001b[92m☑\u001b[0m Q 656+72  T 728  \u001b[92m☑\u001b[0m Q 367+56  T 423  \u001b[92m☑\u001b[0m Q 82+911  T 993  \u001b[92m☑\u001b[0m Q 732+55  T 787  \u001b[92m☑\u001b[0m Q 278+479 T 757  \u001b[92m☑\u001b[0m Q 352+167 T 519  \u001b[92m☑\u001b[0m Q 758+271 T 1029 \u001b[92m☑\u001b[0m Q 141+26  T 167  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 140\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 5s 100us/step - loss: 5.7683e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9996\n",
      "Q 275+6   T 281  \u001b[92m☑\u001b[0m Q 72+55   T 127  \u001b[92m☑\u001b[0m Q 453+44  T 497  \u001b[92m☑\u001b[0m Q 12+392  T 404  \u001b[92m☑\u001b[0m Q 30+302  T 332  \u001b[92m☑\u001b[0m Q 350+496 T 846  \u001b[92m☑\u001b[0m Q 588+367 T 955  \u001b[92m☑\u001b[0m Q 81+308  T 389  \u001b[92m☑\u001b[0m Q 300+58  T 358  \u001b[92m☑\u001b[0m Q 669+409 T 1078 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 141\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 6.4655e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Q 53+22   T 75   \u001b[92m☑\u001b[0m Q 752+261 T 1013 \u001b[92m☑\u001b[0m Q 679+3   T 682  \u001b[92m☑\u001b[0m Q 650+998 T 1648 \u001b[92m☑\u001b[0m Q 679+192 T 871  \u001b[92m☑\u001b[0m Q 863+601 T 1464 \u001b[92m☑\u001b[0m Q 3+94    T 97   \u001b[92m☑\u001b[0m Q 964+110 T 1074 \u001b[92m☑\u001b[0m Q 171+23  T 194  \u001b[92m☑\u001b[0m Q 107+50  T 157  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 142\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 3.5080e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9997\n",
      "Q 442+66  T 508  \u001b[92m☑\u001b[0m Q 215+6   T 221  \u001b[92m☑\u001b[0m Q 779+50  T 829  \u001b[92m☑\u001b[0m Q 585+973 T 1558 \u001b[92m☑\u001b[0m Q 340+924 T 1264 \u001b[92m☑\u001b[0m Q 906+1   T 907  \u001b[92m☑\u001b[0m Q 48+23   T 71   \u001b[92m☑\u001b[0m Q 3+36    T 39   \u001b[92m☑\u001b[0m Q 742+311 T 1053 \u001b[92m☑\u001b[0m Q 375+515 T 890  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 143\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.9650e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Q 892+228 T 1120 \u001b[92m☑\u001b[0m Q 902+7   T 909  \u001b[92m☑\u001b[0m Q 640+23  T 663  \u001b[92m☑\u001b[0m Q 68+988  T 1056 \u001b[92m☑\u001b[0m Q 11+957  T 968  \u001b[92m☑\u001b[0m Q 615+741 T 1356 \u001b[92m☑\u001b[0m Q 691+822 T 1513 \u001b[92m☑\u001b[0m Q 89+410  T 499  \u001b[92m☑\u001b[0m Q 665+41  T 706  \u001b[92m☑\u001b[0m Q 603+610 T 1213 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 144\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.6126e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9997\n",
      "Q 178+70  T 248  \u001b[92m☑\u001b[0m Q 55+342  T 397  \u001b[92m☑\u001b[0m Q 598+718 T 1316 \u001b[92m☑\u001b[0m Q 3+75    T 78   \u001b[92m☑\u001b[0m Q 572+634 T 1206 \u001b[92m☑\u001b[0m Q 89+576  T 665  \u001b[92m☑\u001b[0m Q 71+298  T 369  \u001b[92m☑\u001b[0m Q 379+233 T 612  \u001b[92m☑\u001b[0m Q 80+52   T 132  \u001b[92m☑\u001b[0m Q 9+811   T 820  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 145\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.3367e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Q 75+959  T 1034 \u001b[92m☑\u001b[0m Q 83+77   T 160  \u001b[92m☑\u001b[0m Q 146+67  T 213  \u001b[92m☑\u001b[0m Q 52+567  T 619  \u001b[92m☑\u001b[0m Q 214+57  T 271  \u001b[92m☑\u001b[0m Q 40+85   T 125  \u001b[92m☑\u001b[0m Q 66+8    T 74   \u001b[92m☑\u001b[0m Q 29+14   T 43   \u001b[92m☑\u001b[0m Q 42+406  T 448  \u001b[92m☑\u001b[0m Q 926+8   T 934  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 146\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.1029e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9997\n",
      "Q 86+35   T 121  \u001b[92m☑\u001b[0m Q 184+81  T 265  \u001b[92m☑\u001b[0m Q 50+199  T 249  \u001b[92m☑\u001b[0m Q 93+78   T 171  \u001b[92m☑\u001b[0m Q 30+990  T 1020 \u001b[92m☑\u001b[0m Q 89+12   T 101  \u001b[92m☑\u001b[0m Q 9+277   T 286  \u001b[92m☑\u001b[0m Q 672+557 T 1229 \u001b[92m☑\u001b[0m Q 575+71  T 646  \u001b[92m☑\u001b[0m Q 66+714  T 780  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 147\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.9082e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n",
      "Q 203+91  T 294  \u001b[92m☑\u001b[0m Q 423+491 T 914  \u001b[92m☑\u001b[0m Q 621+556 T 1177 \u001b[92m☑\u001b[0m Q 13+315  T 328  \u001b[92m☑\u001b[0m Q 95+164  T 259  \u001b[92m☑\u001b[0m Q 1+288   T 289  \u001b[92m☑\u001b[0m Q 96+926  T 1022 \u001b[92m☑\u001b[0m Q 954+631 T 1585 \u001b[92m☑\u001b[0m Q 75+147  T 222  \u001b[92m☑\u001b[0m Q 16+597  T 613  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 148\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.7249e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9998\n",
      "Q 35+84   T 119  \u001b[92m☑\u001b[0m Q 83+558  T 641  \u001b[92m☑\u001b[0m Q 222+42  T 264  \u001b[92m☑\u001b[0m Q 566+394 T 960  \u001b[92m☑\u001b[0m Q 187+3   T 190  \u001b[92m☑\u001b[0m Q 162+98  T 260  \u001b[92m☑\u001b[0m Q 75+42   T 117  \u001b[92m☑\u001b[0m Q 57+707  T 764  \u001b[92m☑\u001b[0m Q 715+798 T 1513 \u001b[92m☑\u001b[0m Q 842+6   T 848  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 149\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.6317e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9997\n",
      "Q 911+5   T 916  \u001b[92m☑\u001b[0m Q 6+641   T 647  \u001b[92m☑\u001b[0m Q 741+3   T 744  \u001b[92m☑\u001b[0m Q 96+152  T 248  \u001b[92m☑\u001b[0m Q 853+621 T 1474 \u001b[92m☑\u001b[0m Q 292+24  T 316  \u001b[92m☑\u001b[0m Q 304+352 T 656  \u001b[92m☑\u001b[0m Q 25+637  T 662  \u001b[92m☑\u001b[0m Q 363+633 T 996  \u001b[92m☑\u001b[0m Q 29+286  T 315  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 150\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0295 - acc: 0.9920 - val_loss: 0.0048 - val_acc: 0.9990\n",
      "Q 39+222  T 261  \u001b[92m☑\u001b[0m Q 208+443 T 651  \u001b[92m☑\u001b[0m Q 999+63  T 1062 \u001b[92m☑\u001b[0m Q 682+74  T 756  \u001b[92m☑\u001b[0m Q 605+80  T 685  \u001b[92m☑\u001b[0m Q 468+2   T 470  \u001b[92m☑\u001b[0m Q 426+351 T 777  \u001b[92m☑\u001b[0m Q 87+328  T 415  \u001b[92m☑\u001b[0m Q 462+5   T 467  \u001b[92m☑\u001b[0m Q 321+10  T 331  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 151\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0024 - val_acc: 0.9993\n",
      "Q 188+783 T 971  \u001b[92m☑\u001b[0m Q 68+990  T 1058 \u001b[92m☑\u001b[0m Q 802+75  T 877  \u001b[92m☑\u001b[0m Q 94+821  T 915  \u001b[92m☑\u001b[0m Q 210+854 T 1064 \u001b[92m☑\u001b[0m Q 553+740 T 1293 \u001b[92m☑\u001b[0m Q 3+326   T 329  \u001b[92m☑\u001b[0m Q 448+0   T 448  \u001b[92m☑\u001b[0m Q 65+51   T 116  \u001b[92m☑\u001b[0m Q 59+578  T 637  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 152\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 4.1194e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9994\n",
      "Q 632+0   T 632  \u001b[92m☑\u001b[0m Q 50+900  T 950  \u001b[92m☑\u001b[0m Q 633+38  T 671  \u001b[92m☑\u001b[0m Q 54+629  T 683  \u001b[92m☑\u001b[0m Q 871+46  T 917  \u001b[92m☑\u001b[0m Q 266+637 T 903  \u001b[92m☑\u001b[0m Q 22+18   T 40   \u001b[92m☑\u001b[0m Q 764+117 T 881  \u001b[92m☑\u001b[0m Q 39+557  T 596  \u001b[92m☑\u001b[0m Q 75+975  T 1050 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 153\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 3.1279e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9995\n",
      "Q 753+348 T 1101 \u001b[92m☑\u001b[0m Q 73+325  T 398  \u001b[92m☑\u001b[0m Q 9+672   T 681  \u001b[92m☑\u001b[0m Q 256+68  T 324  \u001b[92m☑\u001b[0m Q 72+974  T 1046 \u001b[92m☑\u001b[0m Q 501+47  T 548  \u001b[92m☑\u001b[0m Q 994+9   T 1003 \u001b[92m☑\u001b[0m Q 216+46  T 262  \u001b[92m☑\u001b[0m Q 6+113   T 119  \u001b[92m☑\u001b[0m Q 906+486 T 1392 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 154\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.6716e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Q 847+358 T 1205 \u001b[92m☑\u001b[0m Q 95+565  T 660  \u001b[92m☑\u001b[0m Q 336+827 T 1163 \u001b[92m☑\u001b[0m Q 986+74  T 1060 \u001b[92m☑\u001b[0m Q 832+81  T 913  \u001b[92m☑\u001b[0m Q 12+535  T 547  \u001b[92m☑\u001b[0m Q 164+0   T 164  \u001b[92m☑\u001b[0m Q 94+911  T 1005 \u001b[92m☑\u001b[0m Q 38+834  T 872  \u001b[92m☑\u001b[0m Q 736+99  T 835  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 155\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.3419e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9996\n",
      "Q 73+669  T 742  \u001b[92m☑\u001b[0m Q 30+769  T 799  \u001b[92m☑\u001b[0m Q 91+36   T 127  \u001b[92m☑\u001b[0m Q 17+189  T 206  \u001b[92m☑\u001b[0m Q 846+79  T 925  \u001b[92m☑\u001b[0m Q 842+60  T 902  \u001b[92m☑\u001b[0m Q 151+35  T 186  \u001b[92m☑\u001b[0m Q 152+913 T 1065 \u001b[92m☑\u001b[0m Q 35+25   T 60   \u001b[92m☑\u001b[0m Q 77+29   T 106  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 156\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.0895e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9996\n",
      "Q 837+664 T 1501 \u001b[92m☑\u001b[0m Q 6+150   T 156  \u001b[92m☑\u001b[0m Q 90+901  T 991  \u001b[92m☑\u001b[0m Q 2+424   T 426  \u001b[92m☑\u001b[0m Q 113+351 T 464  \u001b[92m☑\u001b[0m Q 6+521   T 527  \u001b[92m☑\u001b[0m Q 178+80  T 258  \u001b[92m☑\u001b[0m Q 87+35   T 122  \u001b[92m☑\u001b[0m Q 290+37  T 327  \u001b[92m☑\u001b[0m Q 3+983   T 986  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 157\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.8735e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "Q 583+62  T 645  \u001b[92m☑\u001b[0m Q 269+860 T 1129 \u001b[92m☑\u001b[0m Q 690+91  T 781  \u001b[92m☑\u001b[0m Q 8+712   T 720  \u001b[92m☑\u001b[0m Q 72+923  T 995  \u001b[92m☑\u001b[0m Q 78+874  T 952  \u001b[92m☑\u001b[0m Q 976+652 T 1628 \u001b[92m☑\u001b[0m Q 221+46  T 267  \u001b[92m☑\u001b[0m Q 888+5   T 893  \u001b[92m☑\u001b[0m Q 99+946  T 1045 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 158\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.7021e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Q 868+654 T 1522 \u001b[92m☑\u001b[0m Q 9+418   T 427  \u001b[92m☑\u001b[0m Q 70+197  T 267  \u001b[92m☑\u001b[0m Q 270+390 T 660  \u001b[92m☑\u001b[0m Q 789+16  T 805  \u001b[92m☑\u001b[0m Q 148+33  T 181  \u001b[92m☑\u001b[0m Q 11+49   T 60   \u001b[92m☑\u001b[0m Q 66+714  T 780  \u001b[92m☑\u001b[0m Q 9+811   T 820  \u001b[92m☑\u001b[0m Q 714+38  T 752  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 159\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0305 - acc: 0.9915 - val_loss: 0.0292 - val_acc: 0.9900\n",
      "Q 441+98  T 539  \u001b[92m☑\u001b[0m Q 63+618  T 681  \u001b[92m☑\u001b[0m Q 549+44  T 593  \u001b[92m☑\u001b[0m Q 429+70  T 499  \u001b[92m☑\u001b[0m Q 686+753 T 1439 \u001b[92m☑\u001b[0m Q 450+733 T 1183 \u001b[92m☑\u001b[0m Q 296+241 T 537  \u001b[92m☑\u001b[0m Q 39+227  T 266  \u001b[92m☑\u001b[0m Q 42+91   T 133  \u001b[92m☑\u001b[0m Q 22+891  T 913  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 160\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 0.0052 - acc: 0.9987 - val_loss: 0.0048 - val_acc: 0.9984\n",
      "Q 201+66  T 267  \u001b[92m☑\u001b[0m Q 306+78  T 384  \u001b[92m☑\u001b[0m Q 306+78  T 384  \u001b[92m☑\u001b[0m Q 54+412  T 466  \u001b[92m☑\u001b[0m Q 853+395 T 1248 \u001b[92m☑\u001b[0m Q 459+55  T 514  \u001b[92m☑\u001b[0m Q 13+98   T 111  \u001b[92m☑\u001b[0m Q 94+472  T 566  \u001b[92m☑\u001b[0m Q 521+434 T 955  \u001b[92m☑\u001b[0m Q 85+175  T 260  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 161\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 6.2316e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9997\n",
      "Q 230+590 T 820  \u001b[92m☑\u001b[0m Q 316+382 T 698  \u001b[92m☑\u001b[0m Q 2+394   T 396  \u001b[92m☑\u001b[0m Q 603+848 T 1451 \u001b[92m☑\u001b[0m Q 597+375 T 972  \u001b[92m☑\u001b[0m Q 76+141  T 217  \u001b[92m☑\u001b[0m Q 13+98   T 111  \u001b[92m☑\u001b[0m Q 50+35   T 85   \u001b[92m☑\u001b[0m Q 473+468 T 941  \u001b[92m☑\u001b[0m Q 879+211 T 1090 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 162\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 3.7668e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9997\n",
      "Q 40+514  T 554  \u001b[92m☑\u001b[0m Q 116+187 T 303  \u001b[92m☑\u001b[0m Q 169+891 T 1060 \u001b[92m☑\u001b[0m Q 491+3   T 494  \u001b[92m☑\u001b[0m Q 21+323  T 344  \u001b[92m☑\u001b[0m Q 451+52  T 503  \u001b[92m☑\u001b[0m Q 57+98   T 155  \u001b[92m☑\u001b[0m Q 967+7   T 974  \u001b[92m☑\u001b[0m Q 799+539 T 1338 \u001b[92m☑\u001b[0m Q 117+923 T 1040 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 163\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 3.0262e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9997\n",
      "Q 21+837  T 858  \u001b[92m☑\u001b[0m Q 557+0   T 557  \u001b[92m☑\u001b[0m Q 130+89  T 219  \u001b[92m☑\u001b[0m Q 72+842  T 914  \u001b[92m☑\u001b[0m Q 5+333   T 338  \u001b[92m☑\u001b[0m Q 94+827  T 921  \u001b[92m☑\u001b[0m Q 640+23  T 663  \u001b[92m☑\u001b[0m Q 760+157 T 917  \u001b[92m☑\u001b[0m Q 177+3   T 180  \u001b[92m☑\u001b[0m Q 1+627   T 628  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 164\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 2.5776e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9997\n",
      "Q 900+393 T 1293 \u001b[92m☑\u001b[0m Q 841+98  T 939  \u001b[92m☑\u001b[0m Q 61+272  T 333  \u001b[92m☑\u001b[0m Q 668+89  T 757  \u001b[92m☑\u001b[0m Q 685+99  T 784  \u001b[92m☑\u001b[0m Q 9+329   T 338  \u001b[92m☑\u001b[0m Q 88+238  T 326  \u001b[92m☑\u001b[0m Q 661+1   T 662  \u001b[92m☑\u001b[0m Q 708+304 T 1012 \u001b[92m☑\u001b[0m Q 73+201  T 274  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 165\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.2600e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Q 673+5   T 678  \u001b[92m☑\u001b[0m Q 923+235 T 1158 \u001b[92m☑\u001b[0m Q 916+632 T 1548 \u001b[92m☑\u001b[0m Q 746+48  T 794  \u001b[92m☑\u001b[0m Q 4+420   T 424  \u001b[92m☑\u001b[0m Q 12+439  T 451  \u001b[92m☑\u001b[0m Q 830+538 T 1368 \u001b[92m☑\u001b[0m Q 78+14   T 92   \u001b[92m☑\u001b[0m Q 53+92   T 145  \u001b[92m☑\u001b[0m Q 993+894 T 1887 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 166\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.0151e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Q 447+331 T 778  \u001b[92m☑\u001b[0m Q 92+51   T 143  \u001b[92m☑\u001b[0m Q 388+74  T 462  \u001b[92m☑\u001b[0m Q 307+2   T 309  \u001b[92m☑\u001b[0m Q 123+71  T 194  \u001b[92m☑\u001b[0m Q 257+75  T 332  \u001b[92m☑\u001b[0m Q 885+838 T 1723 \u001b[92m☑\u001b[0m Q 259+60  T 319  \u001b[92m☑\u001b[0m Q 77+871  T 948  \u001b[92m☑\u001b[0m Q 860+435 T 1295 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 167\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.7869e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9997\n",
      "Q 23+275  T 298  \u001b[92m☑\u001b[0m Q 75+824  T 899  \u001b[92m☑\u001b[0m Q 227+72  T 299  \u001b[92m☑\u001b[0m Q 460+77  T 537  \u001b[92m☑\u001b[0m Q 67+174  T 241  \u001b[92m☑\u001b[0m Q 972+52  T 1024 \u001b[92m☑\u001b[0m Q 75+42   T 117  \u001b[92m☑\u001b[0m Q 39+680  T 719  \u001b[92m☑\u001b[0m Q 42+582  T 624  \u001b[92m☑\u001b[0m Q 578+91  T 669  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 168\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.6104e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n",
      "Q 960+69  T 1029 \u001b[92m☑\u001b[0m Q 686+3   T 689  \u001b[92m☑\u001b[0m Q 113+81  T 194  \u001b[92m☑\u001b[0m Q 877+11  T 888  \u001b[92m☑\u001b[0m Q 207+90  T 297  \u001b[92m☑\u001b[0m Q 13+495  T 508  \u001b[92m☑\u001b[0m Q 272+211 T 483  \u001b[92m☑\u001b[0m Q 60+178  T 238  \u001b[92m☑\u001b[0m Q 971+20  T 991  \u001b[92m☑\u001b[0m Q 75+727  T 802  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 169\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 1.5625e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9995\n",
      "Q 484+17  T 501  \u001b[92m☑\u001b[0m Q 80+52   T 132  \u001b[92m☑\u001b[0m Q 905+98  T 1003 \u001b[92m☑\u001b[0m Q 90+83   T 173  \u001b[92m☑\u001b[0m Q 28+35   T 63   \u001b[92m☑\u001b[0m Q 549+84  T 633  \u001b[92m☑\u001b[0m Q 409+69  T 478  \u001b[92m☑\u001b[0m Q 181+43  T 224  \u001b[92m☑\u001b[0m Q 24+84   T 108  \u001b[92m☑\u001b[0m Q 686+361 T 1047 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 170\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0295 - acc: 0.9912 - val_loss: 0.0032 - val_acc: 0.9991\n",
      "Q 856+36  T 892  \u001b[92m☑\u001b[0m Q 9+277   T 286  \u001b[92m☑\u001b[0m Q 481+54  T 535  \u001b[92m☑\u001b[0m Q 597+375 T 972  \u001b[92m☑\u001b[0m Q 48+545  T 593  \u001b[92m☑\u001b[0m Q 405+879 T 1284 \u001b[92m☑\u001b[0m Q 154+551 T 705  \u001b[92m☑\u001b[0m Q 7+99    T 106  \u001b[92m☑\u001b[0m Q 74+602  T 676  \u001b[92m☑\u001b[0m Q 352+98  T 450  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 171\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 9.2295e-04 - acc: 0.9999 - val_loss: 0.0020 - val_acc: 0.9996\n",
      "Q 64+607  T 671  \u001b[92m☑\u001b[0m Q 457+8   T 465  \u001b[92m☑\u001b[0m Q 75+727  T 802  \u001b[92m☑\u001b[0m Q 418+23  T 441  \u001b[92m☑\u001b[0m Q 891+33  T 924  \u001b[92m☑\u001b[0m Q 71+97   T 168  \u001b[92m☑\u001b[0m Q 842+4   T 846  \u001b[92m☑\u001b[0m Q 497+970 T 1467 \u001b[92m☑\u001b[0m Q 396+758 T 1154 \u001b[92m☑\u001b[0m Q 46+72   T 118  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 172\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 4s 100us/step - loss: 4.1313e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9996\n",
      "Q 32+237  T 269  \u001b[92m☑\u001b[0m Q 34+254  T 288  \u001b[92m☑\u001b[0m Q 15+35   T 50   \u001b[92m☑\u001b[0m Q 859+814 T 1673 \u001b[92m☑\u001b[0m Q 24+620  T 644  \u001b[92m☑\u001b[0m Q 782+79  T 861  \u001b[92m☑\u001b[0m Q 183+61  T 244  \u001b[92m☑\u001b[0m Q 488+23  T 511  \u001b[92m☑\u001b[0m Q 679+713 T 1392 \u001b[92m☑\u001b[0m Q 520+224 T 744  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 173\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 3.0820e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9997\n",
      "Q 564+696 T 1260 \u001b[92m☑\u001b[0m Q 12+475  T 487  \u001b[92m☑\u001b[0m Q 802+3   T 805  \u001b[92m☑\u001b[0m Q 528+116 T 644  \u001b[92m☑\u001b[0m Q 35+342  T 377  \u001b[92m☑\u001b[0m Q 75+330  T 405  \u001b[92m☑\u001b[0m Q 509+36  T 545  \u001b[92m☑\u001b[0m Q 624+76  T 700  \u001b[92m☑\u001b[0m Q 90+669  T 759  \u001b[92m☑\u001b[0m Q 4+346   T 350  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 174\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.5728e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Q 30+641  T 671  \u001b[92m☑\u001b[0m Q 861+3   T 864  \u001b[92m☑\u001b[0m Q 560+688 T 1248 \u001b[92m☑\u001b[0m Q 331+14  T 345  \u001b[92m☑\u001b[0m Q 712+49  T 761  \u001b[92m☑\u001b[0m Q 737+77  T 814  \u001b[92m☑\u001b[0m Q 994+5   T 999  \u001b[92m☑\u001b[0m Q 456+6   T 462  \u001b[92m☑\u001b[0m Q 71+938  T 1009 \u001b[92m☑\u001b[0m Q 819+144 T 963  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 175\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.2247e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Q 29+590  T 619  \u001b[92m☑\u001b[0m Q 288+0   T 288  \u001b[92m☑\u001b[0m Q 43+892  T 935  \u001b[92m☑\u001b[0m Q 497+233 T 730  \u001b[92m☑\u001b[0m Q 488+46  T 534  \u001b[92m☑\u001b[0m Q 73+40   T 113  \u001b[92m☑\u001b[0m Q 78+0    T 78   \u001b[92m☑\u001b[0m Q 101+61  T 162  \u001b[92m☑\u001b[0m Q 340+80  T 420  \u001b[92m☑\u001b[0m Q 79+25   T 104  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 176\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.9613e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9997\n",
      "Q 7+642   T 649  \u001b[92m☑\u001b[0m Q 664+3   T 667  \u001b[92m☑\u001b[0m Q 59+46   T 105  \u001b[92m☑\u001b[0m Q 797+830 T 1627 \u001b[92m☑\u001b[0m Q 105+446 T 551  \u001b[92m☑\u001b[0m Q 28+946  T 974  \u001b[92m☑\u001b[0m Q 9+725   T 734  \u001b[92m☑\u001b[0m Q 490+273 T 763  \u001b[92m☑\u001b[0m Q 502+17  T 519  \u001b[92m☑\u001b[0m Q 68+493  T 561  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 177\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.7453e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9997\n",
      "Q 154+421 T 575  \u001b[92m☑\u001b[0m Q 836+201 T 1037 \u001b[92m☑\u001b[0m Q 37+409  T 446  \u001b[92m☑\u001b[0m Q 66+365  T 431  \u001b[92m☑\u001b[0m Q 921+311 T 1232 \u001b[92m☑\u001b[0m Q 421+51  T 472  \u001b[92m☑\u001b[0m Q 31+209  T 240  \u001b[92m☑\u001b[0m Q 23+357  T 380  \u001b[92m☑\u001b[0m Q 93+441  T 534  \u001b[92m☑\u001b[0m Q 828+94  T 922  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 178\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 1.5586e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9997\n",
      "Q 61+272  T 333  \u001b[92m☑\u001b[0m Q 229+63  T 292  \u001b[92m☑\u001b[0m Q 38+967  T 1005 \u001b[92m☑\u001b[0m Q 671+416 T 1087 \u001b[92m☑\u001b[0m Q 88+178  T 266  \u001b[92m☑\u001b[0m Q 131+334 T 465  \u001b[92m☑\u001b[0m Q 50+352  T 402  \u001b[92m☑\u001b[0m Q 5+333   T 338  \u001b[92m☑\u001b[0m Q 851+5   T 856  \u001b[92m☑\u001b[0m Q 70+773  T 843  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 179\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.4058e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9997\n",
      "Q 223+648 T 871  \u001b[92m☑\u001b[0m Q 871+753 T 1624 \u001b[92m☑\u001b[0m Q 37+94   T 131  \u001b[92m☑\u001b[0m Q 257+75  T 332  \u001b[92m☑\u001b[0m Q 2+972   T 974  \u001b[92m☑\u001b[0m Q 5+872   T 877  \u001b[92m☑\u001b[0m Q 4+464   T 468  \u001b[92m☑\u001b[0m Q 523+883 T 1406 \u001b[92m☑\u001b[0m Q 401+708 T 1109 \u001b[92m☑\u001b[0m Q 961+886 T 1847 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 180\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.2806e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9997\n",
      "Q 368+91  T 459  \u001b[92m☑\u001b[0m Q 516+1   T 517  \u001b[92m☑\u001b[0m Q 528+116 T 644  \u001b[92m☑\u001b[0m Q 649+27  T 676  \u001b[92m☑\u001b[0m Q 661+701 T 1362 \u001b[92m☑\u001b[0m Q 1+807   T 808  \u001b[92m☑\u001b[0m Q 189+74  T 263  \u001b[92m☑\u001b[0m Q 6+192   T 198  \u001b[92m☑\u001b[0m Q 250+299 T 549  \u001b[92m☑\u001b[0m Q 243+90  T 333  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 181\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 1.1623e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9997\n",
      "Q 390+538 T 928  \u001b[92m☑\u001b[0m Q 12+535  T 547  \u001b[92m☑\u001b[0m Q 48+759  T 807  \u001b[92m☑\u001b[0m Q 69+593  T 662  \u001b[92m☑\u001b[0m Q 284+980 T 1264 \u001b[92m☑\u001b[0m Q 21+286  T 307  \u001b[92m☑\u001b[0m Q 95+221  T 316  \u001b[92m☑\u001b[0m Q 80+36   T 116  \u001b[92m☑\u001b[0m Q 82+659  T 741  \u001b[92m☑\u001b[0m Q 201+945 T 1146 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 182\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 5s 100us/step - loss: 0.0196 - acc: 0.9951 - val_loss: 0.0924 - val_acc: 0.9712\n",
      "Q 707+782 T 1489 \u001b[92m☑\u001b[0m Q 47+367  T 414  \u001b[92m☑\u001b[0m Q 1+189   T 190  \u001b[92m☑\u001b[0m Q 430+13  T 443  \u001b[92m☑\u001b[0m Q 214+57  T 271  \u001b[92m☑\u001b[0m Q 3+866   T 869  \u001b[92m☑\u001b[0m Q 673+48  T 721  \u001b[92m☑\u001b[0m Q 32+904  T 936  \u001b[92m☑\u001b[0m Q 321+21  T 342  \u001b[92m☑\u001b[0m Q 66+87   T 153  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 183\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 0.0133 - acc: 0.9963 - val_loss: 0.0032 - val_acc: 0.9992\n",
      "Q 46+911  T 957  \u001b[92m☑\u001b[0m Q 402+300 T 702  \u001b[92m☑\u001b[0m Q 818+8   T 826  \u001b[92m☑\u001b[0m Q 616+56  T 672  \u001b[92m☑\u001b[0m Q 666+793 T 1459 \u001b[92m☑\u001b[0m Q 12+409  T 421  \u001b[92m☑\u001b[0m Q 501+14  T 515  \u001b[92m☑\u001b[0m Q 76+505  T 581  \u001b[92m☑\u001b[0m Q 113+81  T 194  \u001b[92m☑\u001b[0m Q 12+310  T 322  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 184\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 5.1385e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9995\n",
      "Q 379+635 T 1014 \u001b[92m☑\u001b[0m Q 82+149  T 231  \u001b[92m☑\u001b[0m Q 933+42  T 975  \u001b[92m☑\u001b[0m Q 257+95  T 352  \u001b[92m☑\u001b[0m Q 0+326   T 326  \u001b[92m☑\u001b[0m Q 41+25   T 66   \u001b[92m☑\u001b[0m Q 511+7   T 518  \u001b[92m☑\u001b[0m Q 769+213 T 982  \u001b[92m☑\u001b[0m Q 49+871  T 920  \u001b[92m☑\u001b[0m Q 905+0   T 905  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 185\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 3.0489e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9996\n",
      "Q 305+1   T 306  \u001b[92m☑\u001b[0m Q 64+982  T 1046 \u001b[92m☑\u001b[0m Q 599+496 T 1095 \u001b[92m☑\u001b[0m Q 1+730   T 731  \u001b[92m☑\u001b[0m Q 923+779 T 1702 \u001b[92m☑\u001b[0m Q 715+695 T 1410 \u001b[92m☑\u001b[0m Q 5+252   T 257  \u001b[92m☑\u001b[0m Q 223+45  T 268  \u001b[92m☑\u001b[0m Q 895+294 T 1189 \u001b[92m☑\u001b[0m Q 79+956  T 1035 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 186\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 2.4762e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Q 677+822 T 1499 \u001b[92m☑\u001b[0m Q 717+680 T 1397 \u001b[92m☑\u001b[0m Q 0+756   T 756  \u001b[92m☑\u001b[0m Q 820+14  T 834  \u001b[92m☑\u001b[0m Q 312+82  T 394  \u001b[92m☑\u001b[0m Q 69+295  T 364  \u001b[92m☑\u001b[0m Q 350+50  T 400  \u001b[92m☑\u001b[0m Q 396+704 T 1100 \u001b[92m☑\u001b[0m Q 72+873  T 945  \u001b[92m☑\u001b[0m Q 467+3   T 470  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 187\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 2.1170e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9996\n",
      "Q 924+7   T 931  \u001b[92m☑\u001b[0m Q 8+70    T 78   \u001b[92m☑\u001b[0m Q 78+874  T 952  \u001b[92m☑\u001b[0m Q 66+97   T 163  \u001b[92m☑\u001b[0m Q 94+498  T 592  \u001b[92m☑\u001b[0m Q 22+69   T 91   \u001b[92m☑\u001b[0m Q 46+308  T 354  \u001b[92m☑\u001b[0m Q 40+544  T 584  \u001b[92m☑\u001b[0m Q 475+42  T 517  \u001b[92m☑\u001b[0m Q 699+50  T 749  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 188\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.8462e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9996\n",
      "Q 717+42  T 759  \u001b[92m☑\u001b[0m Q 76+49   T 125  \u001b[92m☑\u001b[0m Q 0+606   T 606  \u001b[92m☑\u001b[0m Q 79+934  T 1013 \u001b[92m☑\u001b[0m Q 6+625   T 631  \u001b[92m☑\u001b[0m Q 83+0    T 83   \u001b[92m☑\u001b[0m Q 63+57   T 120  \u001b[92m☑\u001b[0m Q 405+7   T 412  \u001b[92m☑\u001b[0m Q 42+280  T 322  \u001b[92m☑\u001b[0m Q 12+541  T 553  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 189\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 1.6379e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Q 59+512  T 571  \u001b[92m☑\u001b[0m Q 72+82   T 154  \u001b[92m☑\u001b[0m Q 21+388  T 409  \u001b[92m☑\u001b[0m Q 16+59   T 75   \u001b[92m☑\u001b[0m Q 326+221 T 547  \u001b[92m☑\u001b[0m Q 803+521 T 1324 \u001b[92m☑\u001b[0m Q 55+737  T 792  \u001b[92m☑\u001b[0m Q 2+548   T 550  \u001b[92m☑\u001b[0m Q 66+591  T 657  \u001b[92m☑\u001b[0m Q 330+681 T 1011 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 190\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 1.4627e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Q 884+40  T 924  \u001b[92m☑\u001b[0m Q 45+28   T 73   \u001b[92m☑\u001b[0m Q 57+707  T 764  \u001b[92m☑\u001b[0m Q 936+75  T 1011 \u001b[92m☑\u001b[0m Q 83+618  T 701  \u001b[92m☑\u001b[0m Q 437+2   T 439  \u001b[92m☑\u001b[0m Q 93+92   T 185  \u001b[92m☑\u001b[0m Q 624+6   T 630  \u001b[92m☑\u001b[0m Q 34+776  T 810  \u001b[92m☑\u001b[0m Q 777+787 T 1564 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 191\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 1.3163e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Q 551+91  T 642  \u001b[92m☑\u001b[0m Q 618+15  T 633  \u001b[92m☑\u001b[0m Q 606+67  T 673  \u001b[92m☑\u001b[0m Q 135+93  T 228  \u001b[92m☑\u001b[0m Q 81+934  T 1015 \u001b[92m☑\u001b[0m Q 57+81   T 138  \u001b[92m☑\u001b[0m Q 830+22  T 852  \u001b[92m☑\u001b[0m Q 54+629  T 683  \u001b[92m☑\u001b[0m Q 201+296 T 497  \u001b[92m☑\u001b[0m Q 24+686  T 710  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 192\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 1.1936e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Q 74+275  T 349  \u001b[92m☑\u001b[0m Q 927+617 T 1544 \u001b[92m☑\u001b[0m Q 65+51   T 116  \u001b[92m☑\u001b[0m Q 885+838 T 1723 \u001b[92m☑\u001b[0m Q 108+37  T 145  \u001b[92m☑\u001b[0m Q 637+400 T 1037 \u001b[92m☑\u001b[0m Q 232+115 T 347  \u001b[92m☑\u001b[0m Q 596+77  T 673  \u001b[92m☑\u001b[0m Q 295+8   T 303  \u001b[92m☑\u001b[0m Q 644+92  T 736  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 193\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 1.0813e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9996\n",
      "Q 544+68  T 612  \u001b[92m☑\u001b[0m Q 652+354 T 1006 \u001b[92m☑\u001b[0m Q 483+0   T 483  \u001b[92m☑\u001b[0m Q 320+30  T 350  \u001b[92m☑\u001b[0m Q 52+8    T 60   \u001b[92m☑\u001b[0m Q 139+810 T 949  \u001b[92m☑\u001b[0m Q 960+65  T 1025 \u001b[92m☑\u001b[0m Q 793+62  T 855  \u001b[92m☑\u001b[0m Q 64+79   T 143  \u001b[92m☑\u001b[0m Q 721+317 T 1038 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 194\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 9.8120e-05 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9997\n",
      "Q 682+74  T 756  \u001b[92m☑\u001b[0m Q 786+787 T 1573 \u001b[92m☑\u001b[0m Q 60+281  T 341  \u001b[92m☑\u001b[0m Q 1+829   T 830  \u001b[92m☑\u001b[0m Q 693+17  T 710  \u001b[92m☑\u001b[0m Q 66+97   T 163  \u001b[92m☑\u001b[0m Q 59+816  T 875  \u001b[92m☑\u001b[0m Q 293+504 T 797  \u001b[92m☑\u001b[0m Q 257+95  T 352  \u001b[92m☑\u001b[0m Q 480+107 T 587  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 195\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 0.0242 - acc: 0.9927 - val_loss: 0.0077 - val_acc: 0.9979\n",
      "Q 581+536 T 1117 \u001b[92m☑\u001b[0m Q 62+49   T 111  \u001b[92m☑\u001b[0m Q 304+1   T 305  \u001b[92m☑\u001b[0m Q 378+8   T 386  \u001b[92m☑\u001b[0m Q 157+49  T 206  \u001b[92m☑\u001b[0m Q 804+51  T 855  \u001b[92m☑\u001b[0m Q 377+548 T 925  \u001b[92m☑\u001b[0m Q 926+6   T 932  \u001b[92m☑\u001b[0m Q 76+151  T 227  \u001b[92m☑\u001b[0m Q 382+17  T 399  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 196\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0025 - val_acc: 0.9993\n",
      "Q 85+983  T 1068 \u001b[92m☑\u001b[0m Q 165+3   T 168  \u001b[92m☑\u001b[0m Q 61+762  T 823  \u001b[92m☑\u001b[0m Q 299+583 T 882  \u001b[92m☑\u001b[0m Q 669+913 T 1582 \u001b[92m☑\u001b[0m Q 38+12   T 50   \u001b[92m☑\u001b[0m Q 847+23  T 870  \u001b[92m☑\u001b[0m Q 82+911  T 993  \u001b[92m☑\u001b[0m Q 558+95  T 653  \u001b[92m☑\u001b[0m Q 227+70  T 297  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 197\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 3.5839e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9994\n",
      "Q 718+22  T 740  \u001b[92m☑\u001b[0m Q 936+75  T 1011 \u001b[92m☑\u001b[0m Q 806+4   T 810  \u001b[92m☑\u001b[0m Q 171+23  T 194  \u001b[92m☑\u001b[0m Q 1+730   T 731  \u001b[92m☑\u001b[0m Q 983+5   T 988  \u001b[92m☑\u001b[0m Q 979+143 T 1122 \u001b[92m☑\u001b[0m Q 38+872  T 910  \u001b[92m☑\u001b[0m Q 19+895  T 914  \u001b[92m☑\u001b[0m Q 339+624 T 963  \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 198\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 2.4429e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Q 14+662  T 676  \u001b[92m☑\u001b[0m Q 595+2   T 597  \u001b[92m☑\u001b[0m Q 171+36  T 207  \u001b[92m☑\u001b[0m Q 50+95   T 145  \u001b[92m☑\u001b[0m Q 12+439  T 451  \u001b[92m☑\u001b[0m Q 50+563  T 613  \u001b[92m☑\u001b[0m Q 572+30  T 602  \u001b[92m☑\u001b[0m Q 351+58  T 409  \u001b[92m☑\u001b[0m Q 452+53  T 505  \u001b[92m☑\u001b[0m Q 492+923 T 1415 \u001b[92m☑\u001b[0m \n",
      "--------------------------------------------------\n",
      "Iteration 199\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 4s 99us/step - loss: 2.0393e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "Q 0+719   T 719  \u001b[92m☑\u001b[0m Q 33+228  T 261  \u001b[92m☑\u001b[0m Q 47+119  T 166  \u001b[92m☑\u001b[0m Q 530+981 T 1511 \u001b[92m☑\u001b[0m Q 332+996 T 1328 \u001b[92m☑\u001b[0m Q 650+61  T 711  \u001b[92m☑\u001b[0m Q 422+326 T 748  \u001b[92m☑\u001b[0m Q 386+1   T 387  \u001b[92m☑\u001b[0m Q 37+324  T 361  \u001b[92m☑\u001b[0m Q 761+34  T 795  \u001b[92m☑\u001b[0m 795 \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "# An implementation of sequence to sequence learning for performing addition\n",
    "Input: \"535+61\"\n",
    "Output: \"596\"\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "Input may optionally be reversed, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "Two digits reversed:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "Three digits reversed:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "Four digits reversed:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "Five digits reversed:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n",
    "'''  # noqa\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "\n",
    "\n",
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one-hot integer representation\n",
    "    + Decode the one-hot or integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One-hot encode given string C.\n",
    "        # Arguments\n",
    "            C: string, to be encoded.\n",
    "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"Decode the given vector or 2D array to their character output.\n",
    "        # Arguments\n",
    "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
    "                or a vector of character indices (used with `calc_argmax=False`).\n",
    "            calc_argmax: Whether to find the character index with maximum\n",
    "                probability, defaults to `True`.\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n",
    "\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "REVERSE = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# Try replacing GRU, or SimpleRNN.\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last output of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for iteration in range(1, 200):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val, y_val))\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if REVERSE else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "print(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.LSTM'>, 50, 100, 100\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\n",
      "11747328/11745123 [==============================] - 12s 1us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab = ['.', '?', 'Daniel', 'John', 'Mary', 'Sandra', 'Where', 'apple', 'back', 'bathroom', 'bedroom', 'discarded', 'down', 'dropped', 'football', 'garden', 'got', 'grabbed', 'hallway', 'is', 'journeyed', 'kitchen', 'left', 'milk', 'moved', 'office', 'picked', 'put', 'the', 'there', 'to', 'took', 'travelled', 'up', 'went']\n",
      "x.shape = (1000, 552)\n",
      "xq.shape = (1000, 5)\n",
      "y.shape = (1000, 36)\n",
      "story_maxlen, query_maxlen = 552, 5\n",
      "Build model...\n",
      "Training\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 2.6727 - acc: 0.1989 - val_loss: 1.7922 - val_acc: 0.3000\n",
      "Epoch 2/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.8014 - acc: 0.2242 - val_loss: 1.7732 - val_acc: 0.3000\n",
      "Epoch 3/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7899 - acc: 0.1874 - val_loss: 1.8582 - val_acc: 0.0600\n",
      "Epoch 4/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.8010 - acc: 0.2042 - val_loss: 1.8299 - val_acc: 0.0600\n",
      "Epoch 5/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.8024 - acc: 0.2042 - val_loss: 1.8543 - val_acc: 0.0600\n",
      "Epoch 6/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 1.7915 - acc: 0.2021 - val_loss: 1.7545 - val_acc: 0.2600\n",
      "Epoch 7/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 1.7963 - acc: 0.2021 - val_loss: 1.8071 - val_acc: 0.0600\n",
      "Epoch 8/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 1.7880 - acc: 0.2063 - val_loss: 1.8845 - val_acc: 0.0600\n",
      "Epoch 9/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7936 - acc: 0.2011 - val_loss: 1.8430 - val_acc: 0.0600\n",
      "Epoch 10/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7851 - acc: 0.2200 - val_loss: 1.8020 - val_acc: 0.1600\n",
      "Epoch 11/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 1.7805 - acc: 0.2189 - val_loss: 1.7574 - val_acc: 0.3200\n",
      "Epoch 12/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7715 - acc: 0.2432 - val_loss: 1.7897 - val_acc: 0.2000\n",
      "Epoch 13/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7683 - acc: 0.2358 - val_loss: 1.7844 - val_acc: 0.2000\n",
      "Epoch 14/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7615 - acc: 0.2579 - val_loss: 1.8155 - val_acc: 0.2200\n",
      "Epoch 15/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7518 - acc: 0.2716 - val_loss: 1.7915 - val_acc: 0.1800\n",
      "Epoch 16/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 1.7562 - acc: 0.2611 - val_loss: 1.7278 - val_acc: 0.3000\n",
      "Epoch 17/20\n",
      "950/950 [==============================] - 14s 14ms/step - loss: 1.7311 - acc: 0.2811 - val_loss: 1.7478 - val_acc: 0.2400\n",
      "Epoch 18/20\n",
      "950/950 [==============================] - 14s 15ms/step - loss: 1.7301 - acc: 0.2737 - val_loss: 1.8646 - val_acc: 0.0600\n",
      "Epoch 19/20\n",
      "950/950 [==============================] - 15s 15ms/step - loss: 1.7418 - acc: 0.2737 - val_loss: 1.6838 - val_acc: 0.3800\n",
      "Epoch 20/20\n",
      "950/950 [==============================] - 15s 16ms/step - loss: 1.7312 - acc: 0.2768 - val_loss: 1.7554 - val_acc: 0.2600\n",
      "Evaluation\n",
      "1000/1000 [==============================] - 3s 3ms/step\n",
      "Test loss / test accuracy = 1.7967 / 0.2080\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Trains two recurrent neural networks based upon a story and a question.\n",
    "The resulting merged vector is then queried to answer a range of bAbI tasks.\n",
    "The results are comparable to those for an LSTM model provided in Weston et al.:\n",
    "\"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\"\n",
    "http://arxiv.org/abs/1502.05698\n",
    "Task Number                  | FB LSTM Baseline | Keras QA\n",
    "---                          | ---              | ---\n",
    "QA1 - Single Supporting Fact | 50               | 52.1\n",
    "QA2 - Two Supporting Facts   | 20               | 37.0\n",
    "QA3 - Three Supporting Facts | 20               | 20.5\n",
    "QA4 - Two Arg. Relations     | 61               | 62.9\n",
    "QA5 - Three Arg. Relations   | 70               | 61.9\n",
    "QA6 - yes/No Questions       | 48               | 50.7\n",
    "QA7 - Counting               | 49               | 78.9\n",
    "QA8 - Lists/Sets             | 45               | 77.2\n",
    "QA9 - Simple Negation        | 64               | 64.0\n",
    "QA10 - Indefinite Knowledge  | 44               | 47.7\n",
    "QA11 - Basic Coreference     | 72               | 74.9\n",
    "QA12 - Conjunction           | 74               | 76.4\n",
    "QA13 - Compound Coreference  | 94               | 94.4\n",
    "QA14 - Time Reasoning        | 27               | 34.8\n",
    "QA15 - Basic Deduction       | 21               | 32.4\n",
    "QA16 - Basic Induction       | 23               | 50.6\n",
    "QA17 - Positional Reasoning  | 51               | 49.1\n",
    "QA18 - Size Reasoning        | 52               | 90.8\n",
    "QA19 - Path Finding          | 8                | 9.0\n",
    "QA20 - Agent's Motivations   | 91               | 90.7\n",
    "For the resources related to the bAbI project, refer to:\n",
    "https://research.facebook.com/researchers/1543934539189348\n",
    "### Notes\n",
    "- With default word, sentence, and query vector sizes, the GRU model achieves:\n",
    "  - 52.1% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU)\n",
    "  - 37.0% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU)\n",
    "In comparison, the Facebook paper achieves 50% and 20% for the LSTM baseline.\n",
    "- The task does not traditionally parse the question separately. This likely\n",
    "improves accuracy and is a good example of merging two RNNs.\n",
    "- The word vector embeddings are not shared between the story and question RNNs.\n",
    "- See how the accuracy changes given 10,000 training samples (en-10k) instead\n",
    "of only 1000. 1000 was used in order to be comparable to the original paper.\n",
    "- Experiment with GRU, LSTM, and JZS1-3 as they give subtly different results.\n",
    "- The length and noise (i.e. 'useless' story components) impact the ability of\n",
    "LSTMs / GRUs to provide the correct answer. Given only the supporting facts,\n",
    "these RNNs can achieve 100% accuracy on many tasks. Memory networks and neural\n",
    "networks that use attentional processes can efficiently search through this\n",
    "noise to find the relevant statements, improving performance substantially.\n",
    "This becomes especially obvious on QA2 and QA3, both far longer than QA1.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true,\n",
    "    only the sentences that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file, retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data\n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        y[word_idx[answer]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return (pad_sequences(xs, maxlen=story_maxlen),\n",
    "            pad_sequences(xqs, maxlen=query_maxlen), np.array(ys))\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                    origin='https://s3.amazonaws.com/text-datasets/'\n",
    "                           'babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n",
    "          '.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "\n",
    "# Default QA1 with 1000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n",
    "# QA1 with 10,000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'\n",
    "# QA2 with 1000 samples\n",
    "challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt'\n",
    "# QA2 with 10,000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'\n",
    "with tarfile.open(path) as tar:\n",
    "    train = get_stories(tar.extractfile(challenge.format('train')))\n",
    "    test = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train + test:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "\n",
    "x, xq, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\n",
    "tx, txq, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n",
    "\n",
    "print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xq.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "sentence = layers.Input(shape=(story_maxlen,), dtype='int32')\n",
    "encoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)\n",
    "encoded_sentence = RNN(SENT_HIDDEN_SIZE)(encoded_sentence)\n",
    "\n",
    "question = layers.Input(shape=(query_maxlen,), dtype='int32')\n",
    "encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\n",
    "encoded_question = RNN(QUERY_HIDDEN_SIZE)(encoded_question)\n",
    "\n",
    "merged = layers.concatenate([encoded_sentence, encoded_question])\n",
    "preds = layers.Dense(vocab_size, activation='softmax')(merged)\n",
    "\n",
    "model = Model([sentence, question], preds)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training')\n",
    "model.fit([x, xq], y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05)\n",
    "\n",
    "print('Evaluation')\n",
    "loss, acc = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 5s 207us/step - loss: 0.4117 - acc: 0.7925 - val_loss: 0.2960 - val_acc: 0.8726\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 5s 204us/step - loss: 0.2309 - acc: 0.9072 - val_loss: 0.2960 - val_acc: 0.8762\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 5s 205us/step - loss: 0.1687 - acc: 0.9349 - val_loss: 0.2719 - val_acc: 0.8900\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 5s 203us/step - loss: 0.1195 - acc: 0.9569 - val_loss: 0.3363 - val_acc: 0.8826\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 5s 203us/step - loss: 0.0785 - acc: 0.9716 - val_loss: 0.3456 - val_acc: 0.8856\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 5s 203us/step - loss: 0.0593 - acc: 0.9787 - val_loss: 0.3652 - val_acc: 0.8836\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 5s 204us/step - loss: 0.0470 - acc: 0.9832 - val_loss: 0.4109 - val_acc: 0.8830\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 5s 203us/step - loss: 0.0368 - acc: 0.9870 - val_loss: 0.4949 - val_acc: 0.8764\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 5s 203us/step - loss: 0.0299 - acc: 0.9891 - val_loss: 0.5153 - val_acc: 0.8804\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 5s 203us/step - loss: 0.0316 - acc: 0.9883 - val_loss: 0.5183 - val_acc: 0.8794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f769018c8d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#This example demonstrates the use of Convolution1D for text classification.\n",
    "Gets to 0.89 test accuracy after 2 epochs. </br>\n",
    "90s/epoch on Intel i5 2.4Ghz CPU. </br>\n",
    "10s/epoch on Tesla K40 GPU.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
